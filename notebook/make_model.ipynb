{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "316UqfuqF1L2"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import japanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "# sns.set(font='IPAexGothic')\n",
    "\n",
    "from IPython.display import display, Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kenta_kasugai/competition/nishika_akutagawa/notebook\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negaposi_sentence_split10 = pd.read_csv(\"../input/feature_negaposi_sentence_split10.csv\", index_col=0)\n",
    "negaposi_describe = pd.read_csv(\"../input/feature_negaposi_describe.csv\", index_col=0)\n",
    "word_count100_tfidf = pd.read_csv(\"../input/feature_word_count100_tfidf.csv\", index_col=0)\n",
    "word_count100 = pd.read_csv(\"../input/feature_word_count100.csv\", index_col=0)\n",
    "# word_0102 = pd.read_csv(\"../input/feature_word.csv\", index_col=0)\n",
    "katsuyo0 = pd.read_csv(\"../input/feature_katsuyo0.csv\", index_col=0)\n",
    "katsuyo01 = pd.read_csv(\"../input/feature_katsuyo01.csv\", index_col=0)\n",
    "adjective012 = pd.read_csv(\"../input/feature_adjective012.csv\", index_col=0)\n",
    "adjective01 = pd.read_csv(\"../input/feature_adjective01.csv\", index_col=0)\n",
    "adjective0 = pd.read_csv(\"../input/feature_adjective0.csv\", index_col=0)\n",
    "hyoki = pd.read_csv(\"../input/feature_hyoki.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4396, 10)\n",
      "(4732, 5)\n",
      "(4732, 9502)\n",
      "(4732, 9502)\n",
      "(4732, 58)\n",
      "(4732, 348)\n",
      "(4732, 67)\n",
      "(4732, 50)\n",
      "(4732, 14)\n",
      "(4732, 14)\n"
     ]
    }
   ],
   "source": [
    "print(negaposi_sentence_split10.shape)\n",
    "print(negaposi_describe.shape)\n",
    "print(word_count100_tfidf.shape)\n",
    "print(word_count100.shape)\n",
    "# print(word_0102.shape)\n",
    "print(katsuyo0.shape)\n",
    "print(katsuyo01.shape)\n",
    "print(adjective012.shape)\n",
    "print(adjective01.shape)\n",
    "print(adjective0.shape)\n",
    "print(hyoki.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([negaposi_sentence_split10,\n",
    "                  negaposi_describe,\n",
    "                  word_count100_tfidf,\n",
    "                  word_count100,\n",
    "#                   word_0102,\n",
    "                  katsuyo0,\n",
    "                  katsuyo01,\n",
    "                  adjective012,\n",
    "                  adjective01,\n",
    "                  adjective0,\n",
    "                  hyoki,\n",
    "                  ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negaposi_split_0</th>\n",
       "      <th>negaposi_split_1</th>\n",
       "      <th>negaposi_split_2</th>\n",
       "      <th>negaposi_split_3</th>\n",
       "      <th>negaposi_split_4</th>\n",
       "      <th>negaposi_split_5</th>\n",
       "      <th>negaposi_split_6</th>\n",
       "      <th>negaposi_split_7</th>\n",
       "      <th>negaposi_split_8</th>\n",
       "      <th>negaposi_split_9</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>ああ</th>\n",
       "      <th>あい</th>\n",
       "      <th>あいかわらず</th>\n",
       "      <th>あいさつ</th>\n",
       "      <th>あいだ</th>\n",
       "      <th>あいつ</th>\n",
       "      <th>あいにく</th>\n",
       "      <th>あいまい</th>\n",
       "      <th>あう</th>\n",
       "      <th>あか</th>\n",
       "      <th>あかい</th>\n",
       "      <th>あかり</th>\n",
       "      <th>あがる</th>\n",
       "      <th>あき</th>\n",
       "      <th>あきらか</th>\n",
       "      <th>あきらめ</th>\n",
       "      <th>あきらめる</th>\n",
       "      <th>あきる</th>\n",
       "      <th>あきれる</th>\n",
       "      <th>あく</th>\n",
       "      <th>あくどい</th>\n",
       "      <th>あくび</th>\n",
       "      <th>あくまで</th>\n",
       "      <th>あくまでも</th>\n",
       "      <th>あぐら</th>\n",
       "      <th>あける</th>\n",
       "      <th>あげく</th>\n",
       "      <th>あげる</th>\n",
       "      <th>あこがれ</th>\n",
       "      <th>あこがれる</th>\n",
       "      <th>あご</th>\n",
       "      <th>あさい</th>\n",
       "      <th>あさましい</th>\n",
       "      <th>あざ</th>\n",
       "      <th>あざやか</th>\n",
       "      <th>あし</th>\n",
       "      <th>あした</th>\n",
       "      <th>あす</th>\n",
       "      <th>あすこ</th>\n",
       "      <th>あずかる</th>\n",
       "      <th>あずける</th>\n",
       "      <th>あせる</th>\n",
       "      <th>あそこ</th>\n",
       "      <th>あそぶ</th>\n",
       "      <th>あたかも</th>\n",
       "      <th>あたし</th>\n",
       "      <th>あたたかい</th>\n",
       "      <th>あたら</th>\n",
       "      <th>あたり</th>\n",
       "      <th>あたりまえ</th>\n",
       "      <th>あたり前</th>\n",
       "      <th>あたる</th>\n",
       "      <th>あだな</th>\n",
       "      <th>あちこち</th>\n",
       "      <th>あちら</th>\n",
       "      <th>あっ</th>\n",
       "      <th>あっけ</th>\n",
       "      <th>あっさり</th>\n",
       "      <th>あっし</th>\n",
       "      <th>あっち</th>\n",
       "      <th>あつい</th>\n",
       "      <th>あつかう</th>\n",
       "      <th>あつまる</th>\n",
       "      <th>あつめる</th>\n",
       "      <th>あて</th>\n",
       "      <th>あてる</th>\n",
       "      <th>あと</th>\n",
       "      <th>あな</th>\n",
       "      <th>あながち</th>\n",
       "      <th>あなた</th>\n",
       "      <th>あに</th>\n",
       "      <th>あにい</th>\n",
       "      <th>あの</th>\n",
       "      <th>あはれ</th>\n",
       "      <th>あばれる</th>\n",
       "      <th>あびる</th>\n",
       "      <th>あふる</th>\n",
       "      <th>あふれる</th>\n",
       "      <th>あぶない</th>\n",
       "      <th>あぶる</th>\n",
       "      <th>あべこべ</th>\n",
       "      <th>あま</th>\n",
       "      <th>あまり</th>\n",
       "      <th>あまりに</th>\n",
       "      <th>あまる</th>\n",
       "      <th>...</th>\n",
       "      <th>名詞特殊助動詞語幹</th>\n",
       "      <th>名詞非自立一般</th>\n",
       "      <th>名詞非自立副詞可能</th>\n",
       "      <th>名詞非自立助動詞語幹</th>\n",
       "      <th>名詞非自立形容動詞語幹</th>\n",
       "      <th>形容詞接尾*</th>\n",
       "      <th>形容詞自立*</th>\n",
       "      <th>形容詞非自立*</th>\n",
       "      <th>感動詞**</th>\n",
       "      <th>接続詞**</th>\n",
       "      <th>接頭詞動詞接続*</th>\n",
       "      <th>接頭詞名詞接続*</th>\n",
       "      <th>接頭詞形容詞接続*</th>\n",
       "      <th>接頭詞数接続*</th>\n",
       "      <th>記号アルファベット*</th>\n",
       "      <th>記号一般*</th>\n",
       "      <th>記号句点*</th>\n",
       "      <th>記号括弧閉*</th>\n",
       "      <th>記号括弧開*</th>\n",
       "      <th>記号空白*</th>\n",
       "      <th>記号読点*</th>\n",
       "      <th>連体詞**</th>\n",
       "      <th>BOS/EOS*</th>\n",
       "      <th>その他間投</th>\n",
       "      <th>フィラー*</th>\n",
       "      <th>副詞一般</th>\n",
       "      <th>副詞助詞類接続</th>\n",
       "      <th>助動詞*</th>\n",
       "      <th>助詞並立助詞</th>\n",
       "      <th>助詞係助詞</th>\n",
       "      <th>助詞副助詞</th>\n",
       "      <th>助詞副助詞／並立助詞／終助詞</th>\n",
       "      <th>助詞副詞化</th>\n",
       "      <th>助詞接続助詞</th>\n",
       "      <th>助詞格助詞</th>\n",
       "      <th>助詞特殊</th>\n",
       "      <th>助詞終助詞</th>\n",
       "      <th>助詞連体化</th>\n",
       "      <th>動詞接尾</th>\n",
       "      <th>動詞自立</th>\n",
       "      <th>動詞非自立</th>\n",
       "      <th>名詞サ変接続</th>\n",
       "      <th>名詞ナイ形容詞語幹</th>\n",
       "      <th>名詞一般</th>\n",
       "      <th>名詞代名詞</th>\n",
       "      <th>名詞副詞可能</th>\n",
       "      <th>名詞動詞非自立的</th>\n",
       "      <th>名詞固有名詞</th>\n",
       "      <th>名詞引用文字列</th>\n",
       "      <th>名詞形容動詞語幹</th>\n",
       "      <th>名詞接尾</th>\n",
       "      <th>名詞接続詞的</th>\n",
       "      <th>名詞数</th>\n",
       "      <th>名詞特殊</th>\n",
       "      <th>名詞非自立</th>\n",
       "      <th>形容詞接尾</th>\n",
       "      <th>形容詞自立</th>\n",
       "      <th>形容詞非自立</th>\n",
       "      <th>感動詞*</th>\n",
       "      <th>接続詞*</th>\n",
       "      <th>接頭詞動詞接続</th>\n",
       "      <th>接頭詞名詞接続</th>\n",
       "      <th>接頭詞形容詞接続</th>\n",
       "      <th>接頭詞数接続</th>\n",
       "      <th>記号アルファベット</th>\n",
       "      <th>記号一般</th>\n",
       "      <th>記号句点</th>\n",
       "      <th>記号括弧閉</th>\n",
       "      <th>記号括弧開</th>\n",
       "      <th>記号空白</th>\n",
       "      <th>記号読点</th>\n",
       "      <th>連体詞*</th>\n",
       "      <th>BOS/EOS</th>\n",
       "      <th>その他</th>\n",
       "      <th>フィラー</th>\n",
       "      <th>副詞</th>\n",
       "      <th>助動詞</th>\n",
       "      <th>助詞</th>\n",
       "      <th>動詞</th>\n",
       "      <th>名詞</th>\n",
       "      <th>形容詞</th>\n",
       "      <th>感動詞</th>\n",
       "      <th>接続詞</th>\n",
       "      <th>接頭詞</th>\n",
       "      <th>記号</th>\n",
       "      <th>連体詞</th>\n",
       "      <th>letter_number</th>\n",
       "      <th>kanji</th>\n",
       "      <th>hiragana</th>\n",
       "      <th>katakana</th>\n",
       "      <th>digit</th>\n",
       "      <th>alphabet</th>\n",
       "      <th>old_kanji</th>\n",
       "      <th>old_hiragana</th>\n",
       "      <th>old</th>\n",
       "      <th>lowercase</th>\n",
       "      <th>hagyo</th>\n",
       "      <th>kakko</th>\n",
       "      <th>kagikakko</th>\n",
       "      <th>kutoten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.189030</td>\n",
       "      <td>-0.130570</td>\n",
       "      <td>-0.175781</td>\n",
       "      <td>-0.159920</td>\n",
       "      <td>-0.241814</td>\n",
       "      <td>-0.230414</td>\n",
       "      <td>-0.068126</td>\n",
       "      <td>-0.187654</td>\n",
       "      <td>-0.146884</td>\n",
       "      <td>-0.111569</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-0.152729</td>\n",
       "      <td>0.053904</td>\n",
       "      <td>-0.241814</td>\n",
       "      <td>-0.068126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>117</td>\n",
       "      <td>52</td>\n",
       "      <td>121</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>651.0</td>\n",
       "      <td>0.298003</td>\n",
       "      <td>0.594470</td>\n",
       "      <td>0.036866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>0.016897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012289</td>\n",
       "      <td>0.058372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.112867</td>\n",
       "      <td>-0.149643</td>\n",
       "      <td>-0.163352</td>\n",
       "      <td>-0.187332</td>\n",
       "      <td>-0.184833</td>\n",
       "      <td>-0.069419</td>\n",
       "      <td>-0.144469</td>\n",
       "      <td>-0.173366</td>\n",
       "      <td>-0.115937</td>\n",
       "      <td>-0.085715</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-0.138693</td>\n",
       "      <td>0.054466</td>\n",
       "      <td>-0.250899</td>\n",
       "      <td>-0.037045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>87</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>87</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>69</td>\n",
       "      <td>271</td>\n",
       "      <td>93</td>\n",
       "      <td>283</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>131</td>\n",
       "      <td>18</td>\n",
       "      <td>1363.0</td>\n",
       "      <td>0.319883</td>\n",
       "      <td>0.562729</td>\n",
       "      <td>0.026412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010271</td>\n",
       "      <td>0.078503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.178221</td>\n",
       "      <td>0.108472</td>\n",
       "      <td>-0.259274</td>\n",
       "      <td>0.067481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>87</td>\n",
       "      <td>50</td>\n",
       "      <td>96</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>482.0</td>\n",
       "      <td>0.338174</td>\n",
       "      <td>0.556017</td>\n",
       "      <td>0.051867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022822</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.047718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.208239</td>\n",
       "      <td>-0.178418</td>\n",
       "      <td>-0.181850</td>\n",
       "      <td>-0.188961</td>\n",
       "      <td>-0.181443</td>\n",
       "      <td>-0.170108</td>\n",
       "      <td>-0.146874</td>\n",
       "      <td>-0.193307</td>\n",
       "      <td>-0.158766</td>\n",
       "      <td>-0.204739</td>\n",
       "      <td>114.0</td>\n",
       "      <td>-0.181201</td>\n",
       "      <td>0.067120</td>\n",
       "      <td>-0.333250</td>\n",
       "      <td>0.004930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>114</td>\n",
       "      <td>109</td>\n",
       "      <td>87</td>\n",
       "      <td>41</td>\n",
       "      <td>170</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>28</td>\n",
       "      <td>232</td>\n",
       "      <td>18</td>\n",
       "      <td>112</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>103</td>\n",
       "      <td>373</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>158</td>\n",
       "      <td>8</td>\n",
       "      <td>297</td>\n",
       "      <td>22</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>576</td>\n",
       "      <td>81</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>114</td>\n",
       "      <td>109</td>\n",
       "      <td>87</td>\n",
       "      <td>41</td>\n",
       "      <td>170</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>232</td>\n",
       "      <td>806</td>\n",
       "      <td>327</td>\n",
       "      <td>1038</td>\n",
       "      <td>67</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>613</td>\n",
       "      <td>31</td>\n",
       "      <td>4301.0</td>\n",
       "      <td>0.332946</td>\n",
       "      <td>0.526157</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.007673</td>\n",
       "      <td>0.008370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012555</td>\n",
       "      <td>0.066031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.169961</td>\n",
       "      <td>-0.244158</td>\n",
       "      <td>-0.193420</td>\n",
       "      <td>-0.150912</td>\n",
       "      <td>-0.082719</td>\n",
       "      <td>-0.179402</td>\n",
       "      <td>-0.157739</td>\n",
       "      <td>-0.172234</td>\n",
       "      <td>-0.209102</td>\n",
       "      <td>-0.164302</td>\n",
       "      <td>139.0</td>\n",
       "      <td>-0.169167</td>\n",
       "      <td>0.092031</td>\n",
       "      <td>-0.418720</td>\n",
       "      <td>0.061273</td>\n",
       "      <td>0.015162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>139</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>133</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>14</td>\n",
       "      <td>283</td>\n",
       "      <td>26</td>\n",
       "      <td>122</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>141</td>\n",
       "      <td>383</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>195</td>\n",
       "      <td>19</td>\n",
       "      <td>314</td>\n",
       "      <td>72</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>475</td>\n",
       "      <td>69</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>139</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>133</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>283</td>\n",
       "      <td>902</td>\n",
       "      <td>405</td>\n",
       "      <td>859</td>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "      <td>421</td>\n",
       "      <td>39</td>\n",
       "      <td>4273.0</td>\n",
       "      <td>0.334659</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.025743</td>\n",
       "      <td>0.026445</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>0.063656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.157527</td>\n",
       "      <td>-0.165958</td>\n",
       "      <td>-0.161920</td>\n",
       "      <td>-0.168265</td>\n",
       "      <td>-0.154132</td>\n",
       "      <td>-0.147501</td>\n",
       "      <td>-0.180346</td>\n",
       "      <td>-0.164280</td>\n",
       "      <td>-0.151534</td>\n",
       "      <td>-0.153664</td>\n",
       "      <td>256.0</td>\n",
       "      <td>-0.160886</td>\n",
       "      <td>0.064332</td>\n",
       "      <td>-0.345864</td>\n",
       "      <td>0.244552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.003190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013244</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>0.014503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>257</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>256</td>\n",
       "      <td>501</td>\n",
       "      <td>456</td>\n",
       "      <td>179</td>\n",
       "      <td>732</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>123</td>\n",
       "      <td>135</td>\n",
       "      <td>1086</td>\n",
       "      <td>70</td>\n",
       "      <td>459</td>\n",
       "      <td>88</td>\n",
       "      <td>61</td>\n",
       "      <td>60</td>\n",
       "      <td>504</td>\n",
       "      <td>1080</td>\n",
       "      <td>2</td>\n",
       "      <td>116</td>\n",
       "      <td>448</td>\n",
       "      <td>70</td>\n",
       "      <td>1120</td>\n",
       "      <td>199</td>\n",
       "      <td>250</td>\n",
       "      <td>6</td>\n",
       "      <td>1582</td>\n",
       "      <td>122</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>322</td>\n",
       "      <td>0</td>\n",
       "      <td>346</td>\n",
       "      <td>4</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>256</td>\n",
       "      <td>501</td>\n",
       "      <td>456</td>\n",
       "      <td>179</td>\n",
       "      <td>732</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>258</td>\n",
       "      <td>1086</td>\n",
       "      <td>2888</td>\n",
       "      <td>1389</td>\n",
       "      <td>3412</td>\n",
       "      <td>130</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>105</td>\n",
       "      <td>2664</td>\n",
       "      <td>87</td>\n",
       "      <td>16845.0</td>\n",
       "      <td>0.287029</td>\n",
       "      <td>0.557673</td>\n",
       "      <td>0.022737</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.023212</td>\n",
       "      <td>0.026061</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.028614</td>\n",
       "      <td>0.058652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.151227</td>\n",
       "      <td>-0.137505</td>\n",
       "      <td>-0.194724</td>\n",
       "      <td>-0.120212</td>\n",
       "      <td>-0.153066</td>\n",
       "      <td>-0.132765</td>\n",
       "      <td>-0.117842</td>\n",
       "      <td>-0.156904</td>\n",
       "      <td>-0.151693</td>\n",
       "      <td>-0.201058</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-0.147916</td>\n",
       "      <td>0.050383</td>\n",
       "      <td>-0.240097</td>\n",
       "      <td>-0.057175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.014155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>86</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>172</td>\n",
       "      <td>8</td>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>88</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>88</td>\n",
       "      <td>19</td>\n",
       "      <td>171</td>\n",
       "      <td>40</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>218</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>86</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>172</td>\n",
       "      <td>505</td>\n",
       "      <td>230</td>\n",
       "      <td>522</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>166</td>\n",
       "      <td>25</td>\n",
       "      <td>2722.0</td>\n",
       "      <td>0.273328</td>\n",
       "      <td>0.666789</td>\n",
       "      <td>0.008817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019838</td>\n",
       "      <td>0.023145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.045922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.075585</td>\n",
       "      <td>-0.135701</td>\n",
       "      <td>-0.201506</td>\n",
       "      <td>-0.098010</td>\n",
       "      <td>-0.087101</td>\n",
       "      <td>-0.104366</td>\n",
       "      <td>-0.048110</td>\n",
       "      <td>-0.095735</td>\n",
       "      <td>-0.159340</td>\n",
       "      <td>-0.108527</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-0.110820</td>\n",
       "      <td>0.078236</td>\n",
       "      <td>-0.311974</td>\n",
       "      <td>0.039568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>167</td>\n",
       "      <td>83</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>202</td>\n",
       "      <td>6</td>\n",
       "      <td>89</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>167</td>\n",
       "      <td>83</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>202</td>\n",
       "      <td>388</td>\n",
       "      <td>174</td>\n",
       "      <td>507</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>399</td>\n",
       "      <td>20</td>\n",
       "      <td>2319.0</td>\n",
       "      <td>0.270806</td>\n",
       "      <td>0.567055</td>\n",
       "      <td>0.068564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050022</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.062096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.110723</td>\n",
       "      <td>-0.116878</td>\n",
       "      <td>-0.152853</td>\n",
       "      <td>-0.130457</td>\n",
       "      <td>-0.144178</td>\n",
       "      <td>-0.125764</td>\n",
       "      <td>-0.119872</td>\n",
       "      <td>-0.121663</td>\n",
       "      <td>-0.137912</td>\n",
       "      <td>-0.151600</td>\n",
       "      <td>190.0</td>\n",
       "      <td>-0.131190</td>\n",
       "      <td>0.065456</td>\n",
       "      <td>-0.311596</td>\n",
       "      <td>0.068172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008989</td>\n",
       "      <td>0.008986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>51</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>397</td>\n",
       "      <td>190</td>\n",
       "      <td>385</td>\n",
       "      <td>362</td>\n",
       "      <td>125</td>\n",
       "      <td>429</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>68</td>\n",
       "      <td>36</td>\n",
       "      <td>783</td>\n",
       "      <td>71</td>\n",
       "      <td>372</td>\n",
       "      <td>47</td>\n",
       "      <td>76</td>\n",
       "      <td>31</td>\n",
       "      <td>276</td>\n",
       "      <td>719</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>273</td>\n",
       "      <td>50</td>\n",
       "      <td>839</td>\n",
       "      <td>52</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>1517</td>\n",
       "      <td>112</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>397</td>\n",
       "      <td>190</td>\n",
       "      <td>385</td>\n",
       "      <td>362</td>\n",
       "      <td>125</td>\n",
       "      <td>429</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>104</td>\n",
       "      <td>783</td>\n",
       "      <td>1968</td>\n",
       "      <td>941</td>\n",
       "      <td>2737</td>\n",
       "      <td>103</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>87</td>\n",
       "      <td>1888</td>\n",
       "      <td>75</td>\n",
       "      <td>11568.0</td>\n",
       "      <td>0.295816</td>\n",
       "      <td>0.552472</td>\n",
       "      <td>0.016425</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049101</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.051435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031812</td>\n",
       "      <td>0.053510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.150055</td>\n",
       "      <td>-0.149588</td>\n",
       "      <td>-0.160721</td>\n",
       "      <td>-0.139322</td>\n",
       "      <td>-0.134723</td>\n",
       "      <td>-0.145973</td>\n",
       "      <td>-0.171107</td>\n",
       "      <td>-0.120812</td>\n",
       "      <td>-0.148324</td>\n",
       "      <td>-0.086332</td>\n",
       "      <td>232.0</td>\n",
       "      <td>-0.140937</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>-0.438225</td>\n",
       "      <td>0.193158</td>\n",
       "      <td>0.006842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007861</td>\n",
       "      <td>0.011414</td>\n",
       "      <td>0.017164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>180</td>\n",
       "      <td>55</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>123</td>\n",
       "      <td>232</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>99</td>\n",
       "      <td>309</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>54</td>\n",
       "      <td>653</td>\n",
       "      <td>23</td>\n",
       "      <td>300</td>\n",
       "      <td>52</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>339</td>\n",
       "      <td>713</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>280</td>\n",
       "      <td>26</td>\n",
       "      <td>606</td>\n",
       "      <td>103</td>\n",
       "      <td>179</td>\n",
       "      <td>4</td>\n",
       "      <td>893</td>\n",
       "      <td>136</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>214</td>\n",
       "      <td>7</td>\n",
       "      <td>131</td>\n",
       "      <td>4</td>\n",
       "      <td>264</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>123</td>\n",
       "      <td>232</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>99</td>\n",
       "      <td>309</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>653</td>\n",
       "      <td>1791</td>\n",
       "      <td>735</td>\n",
       "      <td>2064</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "      <td>847</td>\n",
       "      <td>57</td>\n",
       "      <td>9791.0</td>\n",
       "      <td>0.279542</td>\n",
       "      <td>0.582065</td>\n",
       "      <td>0.071903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.019508</td>\n",
       "      <td>0.028904</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.006945</td>\n",
       "      <td>0.055255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.153396</td>\n",
       "      <td>-0.197875</td>\n",
       "      <td>-0.206584</td>\n",
       "      <td>-0.195620</td>\n",
       "      <td>-0.229857</td>\n",
       "      <td>-0.128126</td>\n",
       "      <td>-0.125542</td>\n",
       "      <td>-0.129539</td>\n",
       "      <td>-0.179630</td>\n",
       "      <td>-0.118513</td>\n",
       "      <td>116.0</td>\n",
       "      <td>-0.165060</td>\n",
       "      <td>0.095575</td>\n",
       "      <td>-0.423257</td>\n",
       "      <td>0.073995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>116</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>202</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>22</td>\n",
       "      <td>286</td>\n",
       "      <td>31</td>\n",
       "      <td>176</td>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>122</td>\n",
       "      <td>372</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>158</td>\n",
       "      <td>20</td>\n",
       "      <td>335</td>\n",
       "      <td>58</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>418</td>\n",
       "      <td>41</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>116</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>202</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>286</td>\n",
       "      <td>918</td>\n",
       "      <td>413</td>\n",
       "      <td>872</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>457</td>\n",
       "      <td>48</td>\n",
       "      <td>4828.0</td>\n",
       "      <td>0.339478</td>\n",
       "      <td>0.580157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.028583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.065866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.141464</td>\n",
       "      <td>-0.013136</td>\n",
       "      <td>-0.180173</td>\n",
       "      <td>-0.135678</td>\n",
       "      <td>-0.136750</td>\n",
       "      <td>-0.098015</td>\n",
       "      <td>-0.151664</td>\n",
       "      <td>-0.137001</td>\n",
       "      <td>-0.149854</td>\n",
       "      <td>-0.130725</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-0.125844</td>\n",
       "      <td>0.066257</td>\n",
       "      <td>-0.195397</td>\n",
       "      <td>0.110409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>80</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>116</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>96</td>\n",
       "      <td>276</td>\n",
       "      <td>119</td>\n",
       "      <td>275</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>103</td>\n",
       "      <td>16</td>\n",
       "      <td>1472.0</td>\n",
       "      <td>0.275136</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>0.025136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.047554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.182168</td>\n",
       "      <td>-0.091016</td>\n",
       "      <td>-0.066492</td>\n",
       "      <td>-0.083976</td>\n",
       "      <td>-0.242783</td>\n",
       "      <td>-0.071326</td>\n",
       "      <td>-0.181013</td>\n",
       "      <td>-0.166716</td>\n",
       "      <td>-0.169445</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-0.150682</td>\n",
       "      <td>0.070542</td>\n",
       "      <td>-0.313393</td>\n",
       "      <td>-0.066492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>73</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>59</td>\n",
       "      <td>167</td>\n",
       "      <td>73</td>\n",
       "      <td>172</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>79</td>\n",
       "      <td>17</td>\n",
       "      <td>905.0</td>\n",
       "      <td>0.277348</td>\n",
       "      <td>0.640884</td>\n",
       "      <td>0.006630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>0.067403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.151403</td>\n",
       "      <td>-0.142008</td>\n",
       "      <td>-0.157573</td>\n",
       "      <td>-0.178763</td>\n",
       "      <td>-0.186329</td>\n",
       "      <td>-0.185452</td>\n",
       "      <td>-0.165364</td>\n",
       "      <td>-0.167758</td>\n",
       "      <td>-0.159983</td>\n",
       "      <td>-0.153251</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>-0.164876</td>\n",
       "      <td>0.097569</td>\n",
       "      <td>-0.609971</td>\n",
       "      <td>0.499435</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.019471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>495</td>\n",
       "      <td>235</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>546</td>\n",
       "      <td>15</td>\n",
       "      <td>48</td>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1029</td>\n",
       "      <td>1542</td>\n",
       "      <td>478</td>\n",
       "      <td>389</td>\n",
       "      <td>503</td>\n",
       "      <td>1356</td>\n",
       "      <td>269</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>523</td>\n",
       "      <td>330</td>\n",
       "      <td>2930</td>\n",
       "      <td>123</td>\n",
       "      <td>1064</td>\n",
       "      <td>197</td>\n",
       "      <td>163</td>\n",
       "      <td>227</td>\n",
       "      <td>1542</td>\n",
       "      <td>3455</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>972</td>\n",
       "      <td>151</td>\n",
       "      <td>3334</td>\n",
       "      <td>885</td>\n",
       "      <td>503</td>\n",
       "      <td>14</td>\n",
       "      <td>3609</td>\n",
       "      <td>745</td>\n",
       "      <td>212</td>\n",
       "      <td>5</td>\n",
       "      <td>355</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "      <td>437</td>\n",
       "      <td>1</td>\n",
       "      <td>226</td>\n",
       "      <td>3</td>\n",
       "      <td>874</td>\n",
       "      <td>5</td>\n",
       "      <td>546</td>\n",
       "      <td>15</td>\n",
       "      <td>48</td>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1029</td>\n",
       "      <td>1542</td>\n",
       "      <td>478</td>\n",
       "      <td>389</td>\n",
       "      <td>503</td>\n",
       "      <td>1356</td>\n",
       "      <td>269</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>853</td>\n",
       "      <td>2930</td>\n",
       "      <td>7906</td>\n",
       "      <td>4370</td>\n",
       "      <td>7302</td>\n",
       "      <td>566</td>\n",
       "      <td>48</td>\n",
       "      <td>211</td>\n",
       "      <td>99</td>\n",
       "      <td>5297</td>\n",
       "      <td>269</td>\n",
       "      <td>42552.0</td>\n",
       "      <td>0.285157</td>\n",
       "      <td>0.606951</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.035792</td>\n",
       "      <td>0.025169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015322</td>\n",
       "      <td>0.068105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.147732</td>\n",
       "      <td>-0.137332</td>\n",
       "      <td>-0.111306</td>\n",
       "      <td>-0.149204</td>\n",
       "      <td>-0.140745</td>\n",
       "      <td>-0.144403</td>\n",
       "      <td>-0.180030</td>\n",
       "      <td>-0.207033</td>\n",
       "      <td>-0.164988</td>\n",
       "      <td>-0.205033</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-0.158781</td>\n",
       "      <td>0.078643</td>\n",
       "      <td>-0.414533</td>\n",
       "      <td>-0.031413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.032287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>103</td>\n",
       "      <td>70</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>70</td>\n",
       "      <td>155</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>211</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>61</td>\n",
       "      <td>242</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>101</td>\n",
       "      <td>7</td>\n",
       "      <td>214</td>\n",
       "      <td>16</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>340</td>\n",
       "      <td>48</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>103</td>\n",
       "      <td>70</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>70</td>\n",
       "      <td>155</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>211</td>\n",
       "      <td>526</td>\n",
       "      <td>237</td>\n",
       "      <td>668</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>480</td>\n",
       "      <td>22</td>\n",
       "      <td>3193.0</td>\n",
       "      <td>0.295647</td>\n",
       "      <td>0.473849</td>\n",
       "      <td>0.121829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>0.011275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033511</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.020670</td>\n",
       "      <td>0.070467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.139510</td>\n",
       "      <td>-0.182005</td>\n",
       "      <td>-0.147175</td>\n",
       "      <td>-0.180676</td>\n",
       "      <td>-0.170807</td>\n",
       "      <td>-0.214253</td>\n",
       "      <td>-0.162942</td>\n",
       "      <td>-0.153300</td>\n",
       "      <td>-0.193539</td>\n",
       "      <td>-0.180861</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-0.175878</td>\n",
       "      <td>0.072713</td>\n",
       "      <td>-0.443683</td>\n",
       "      <td>0.015547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010051</td>\n",
       "      <td>0.014942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>86</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>64</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>18</td>\n",
       "      <td>223</td>\n",
       "      <td>9</td>\n",
       "      <td>137</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>85</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>18</td>\n",
       "      <td>238</td>\n",
       "      <td>33</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>344</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>86</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>64</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>223</td>\n",
       "      <td>733</td>\n",
       "      <td>289</td>\n",
       "      <td>724</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>254</td>\n",
       "      <td>36</td>\n",
       "      <td>3913.0</td>\n",
       "      <td>0.286481</td>\n",
       "      <td>0.655763</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.012778</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.004089</td>\n",
       "      <td>0.038334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.049277</td>\n",
       "      <td>-0.065759</td>\n",
       "      <td>-0.077839</td>\n",
       "      <td>-0.086413</td>\n",
       "      <td>-0.091345</td>\n",
       "      <td>-0.059374</td>\n",
       "      <td>-0.081060</td>\n",
       "      <td>-0.085671</td>\n",
       "      <td>-0.066280</td>\n",
       "      <td>-0.058310</td>\n",
       "      <td>123.0</td>\n",
       "      <td>-0.071585</td>\n",
       "      <td>0.041681</td>\n",
       "      <td>-0.204666</td>\n",
       "      <td>0.074664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>123</td>\n",
       "      <td>448</td>\n",
       "      <td>448</td>\n",
       "      <td>31</td>\n",
       "      <td>268</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>407</td>\n",
       "      <td>3</td>\n",
       "      <td>103</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>136</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>363</td>\n",
       "      <td>46</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>531</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>123</td>\n",
       "      <td>448</td>\n",
       "      <td>448</td>\n",
       "      <td>31</td>\n",
       "      <td>268</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>407</td>\n",
       "      <td>671</td>\n",
       "      <td>428</td>\n",
       "      <td>839</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>1390</td>\n",
       "      <td>35</td>\n",
       "      <td>5458.0</td>\n",
       "      <td>0.102968</td>\n",
       "      <td>0.657933</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.023085</td>\n",
       "      <td>0.026383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010993</td>\n",
       "      <td>0.071638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.152688</td>\n",
       "      <td>-0.165167</td>\n",
       "      <td>-0.207856</td>\n",
       "      <td>-0.113340</td>\n",
       "      <td>-0.127995</td>\n",
       "      <td>-0.108571</td>\n",
       "      <td>-0.143030</td>\n",
       "      <td>-0.173354</td>\n",
       "      <td>-0.156123</td>\n",
       "      <td>-0.144980</td>\n",
       "      <td>156.0</td>\n",
       "      <td>-0.151004</td>\n",
       "      <td>0.079988</td>\n",
       "      <td>-0.442094</td>\n",
       "      <td>0.098970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013721</td>\n",
       "      <td>0.015387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>41</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>156</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>73</td>\n",
       "      <td>372</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>45</td>\n",
       "      <td>519</td>\n",
       "      <td>39</td>\n",
       "      <td>209</td>\n",
       "      <td>54</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>217</td>\n",
       "      <td>566</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>179</td>\n",
       "      <td>39</td>\n",
       "      <td>541</td>\n",
       "      <td>95</td>\n",
       "      <td>172</td>\n",
       "      <td>1</td>\n",
       "      <td>511</td>\n",
       "      <td>145</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>156</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>73</td>\n",
       "      <td>372</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>519</td>\n",
       "      <td>1335</td>\n",
       "      <td>675</td>\n",
       "      <td>1346</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>702</td>\n",
       "      <td>70</td>\n",
       "      <td>7572.0</td>\n",
       "      <td>0.260301</td>\n",
       "      <td>0.626519</td>\n",
       "      <td>0.035394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015716</td>\n",
       "      <td>0.027073</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.069731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.166528</td>\n",
       "      <td>-0.161682</td>\n",
       "      <td>-0.150449</td>\n",
       "      <td>-0.146219</td>\n",
       "      <td>-0.163028</td>\n",
       "      <td>-0.127862</td>\n",
       "      <td>-0.164866</td>\n",
       "      <td>-0.166148</td>\n",
       "      <td>-0.155919</td>\n",
       "      <td>-0.175893</td>\n",
       "      <td>684.0</td>\n",
       "      <td>-0.157716</td>\n",
       "      <td>0.117248</td>\n",
       "      <td>-0.852791</td>\n",
       "      <td>0.497453</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.005020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004313</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>323</td>\n",
       "      <td>70</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>157</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>137</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>431</td>\n",
       "      <td>684</td>\n",
       "      <td>88</td>\n",
       "      <td>8</td>\n",
       "      <td>132</td>\n",
       "      <td>830</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>229</td>\n",
       "      <td>148</td>\n",
       "      <td>935</td>\n",
       "      <td>47</td>\n",
       "      <td>537</td>\n",
       "      <td>121</td>\n",
       "      <td>101</td>\n",
       "      <td>78</td>\n",
       "      <td>490</td>\n",
       "      <td>1159</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>322</td>\n",
       "      <td>54</td>\n",
       "      <td>1175</td>\n",
       "      <td>307</td>\n",
       "      <td>290</td>\n",
       "      <td>9</td>\n",
       "      <td>1301</td>\n",
       "      <td>447</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>444</td>\n",
       "      <td>3</td>\n",
       "      <td>157</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>137</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>431</td>\n",
       "      <td>684</td>\n",
       "      <td>88</td>\n",
       "      <td>8</td>\n",
       "      <td>132</td>\n",
       "      <td>830</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>377</td>\n",
       "      <td>935</td>\n",
       "      <td>3031</td>\n",
       "      <td>1536</td>\n",
       "      <td>3304</td>\n",
       "      <td>170</td>\n",
       "      <td>20</td>\n",
       "      <td>137</td>\n",
       "      <td>38</td>\n",
       "      <td>2177</td>\n",
       "      <td>140</td>\n",
       "      <td>17876.0</td>\n",
       "      <td>0.262195</td>\n",
       "      <td>0.614399</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.024894</td>\n",
       "      <td>0.025789</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.084695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.186360</td>\n",
       "      <td>-0.216576</td>\n",
       "      <td>-0.075086</td>\n",
       "      <td>-0.147251</td>\n",
       "      <td>-0.161008</td>\n",
       "      <td>-0.123934</td>\n",
       "      <td>-0.216487</td>\n",
       "      <td>-0.128981</td>\n",
       "      <td>-0.170237</td>\n",
       "      <td>-0.183629</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-0.157586</td>\n",
       "      <td>0.055253</td>\n",
       "      <td>-0.277219</td>\n",
       "      <td>-0.019623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>80</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>16</td>\n",
       "      <td>102</td>\n",
       "      <td>54</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>71</td>\n",
       "      <td>407</td>\n",
       "      <td>172</td>\n",
       "      <td>356</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>23</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>0.293548</td>\n",
       "      <td>0.646237</td>\n",
       "      <td>0.011290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018280</td>\n",
       "      <td>0.025806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.039785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.184261</td>\n",
       "      <td>-0.142509</td>\n",
       "      <td>-0.299094</td>\n",
       "      <td>-0.159134</td>\n",
       "      <td>-0.163160</td>\n",
       "      <td>-0.224339</td>\n",
       "      <td>-0.058496</td>\n",
       "      <td>-0.145703</td>\n",
       "      <td>-0.090445</td>\n",
       "      <td>-0.122038</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-0.159208</td>\n",
       "      <td>0.089172</td>\n",
       "      <td>-0.395959</td>\n",
       "      <td>-0.016645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.046228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>108</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>108</td>\n",
       "      <td>294</td>\n",
       "      <td>145</td>\n",
       "      <td>388</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>168</td>\n",
       "      <td>16</td>\n",
       "      <td>1691.0</td>\n",
       "      <td>0.351271</td>\n",
       "      <td>0.573034</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.057954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.098940</td>\n",
       "      <td>-0.165344</td>\n",
       "      <td>-0.156370</td>\n",
       "      <td>-0.117170</td>\n",
       "      <td>-0.137270</td>\n",
       "      <td>-0.140546</td>\n",
       "      <td>-0.131642</td>\n",
       "      <td>-0.111756</td>\n",
       "      <td>-0.157792</td>\n",
       "      <td>-0.172346</td>\n",
       "      <td>134.0</td>\n",
       "      <td>-0.137740</td>\n",
       "      <td>0.055292</td>\n",
       "      <td>-0.352908</td>\n",
       "      <td>0.043999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>386</td>\n",
       "      <td>134</td>\n",
       "      <td>369</td>\n",
       "      <td>347</td>\n",
       "      <td>75</td>\n",
       "      <td>418</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>38</td>\n",
       "      <td>806</td>\n",
       "      <td>60</td>\n",
       "      <td>341</td>\n",
       "      <td>37</td>\n",
       "      <td>60</td>\n",
       "      <td>21</td>\n",
       "      <td>260</td>\n",
       "      <td>751</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>284</td>\n",
       "      <td>28</td>\n",
       "      <td>679</td>\n",
       "      <td>34</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>1445</td>\n",
       "      <td>89</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>185</td>\n",
       "      <td>5</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>386</td>\n",
       "      <td>134</td>\n",
       "      <td>369</td>\n",
       "      <td>347</td>\n",
       "      <td>75</td>\n",
       "      <td>418</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>88</td>\n",
       "      <td>806</td>\n",
       "      <td>1903</td>\n",
       "      <td>741</td>\n",
       "      <td>2657</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "      <td>38</td>\n",
       "      <td>132</td>\n",
       "      <td>1729</td>\n",
       "      <td>73</td>\n",
       "      <td>11050.0</td>\n",
       "      <td>0.304163</td>\n",
       "      <td>0.551312</td>\n",
       "      <td>0.009683</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044072</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.044887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039819</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.034751</td>\n",
       "      <td>0.049955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.123098</td>\n",
       "      <td>-0.161399</td>\n",
       "      <td>-0.087035</td>\n",
       "      <td>-0.152334</td>\n",
       "      <td>-0.175499</td>\n",
       "      <td>-0.148570</td>\n",
       "      <td>-0.105941</td>\n",
       "      <td>-0.169547</td>\n",
       "      <td>-0.175066</td>\n",
       "      <td>-0.167333</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-0.142327</td>\n",
       "      <td>0.081791</td>\n",
       "      <td>-0.342087</td>\n",
       "      <td>0.194327</td>\n",
       "      <td>0.099095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>401</td>\n",
       "      <td>75</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>174</td>\n",
       "      <td>96</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>169</td>\n",
       "      <td>16</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>132</td>\n",
       "      <td>21</td>\n",
       "      <td>278</td>\n",
       "      <td>13</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>415</td>\n",
       "      <td>69</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>401</td>\n",
       "      <td>75</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>174</td>\n",
       "      <td>96</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>169</td>\n",
       "      <td>612</td>\n",
       "      <td>312</td>\n",
       "      <td>787</td>\n",
       "      <td>57</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>794</td>\n",
       "      <td>35</td>\n",
       "      <td>3478.0</td>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.536515</td>\n",
       "      <td>0.032490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>0.030190</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.049166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.135801</td>\n",
       "      <td>-0.144990</td>\n",
       "      <td>-0.125146</td>\n",
       "      <td>-0.125077</td>\n",
       "      <td>-0.128930</td>\n",
       "      <td>-0.131722</td>\n",
       "      <td>-0.114753</td>\n",
       "      <td>-0.162246</td>\n",
       "      <td>-0.139520</td>\n",
       "      <td>-0.127093</td>\n",
       "      <td>196.0</td>\n",
       "      <td>-0.133564</td>\n",
       "      <td>0.069623</td>\n",
       "      <td>-0.379930</td>\n",
       "      <td>0.123655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>218</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>196</td>\n",
       "      <td>263</td>\n",
       "      <td>236</td>\n",
       "      <td>99</td>\n",
       "      <td>392</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>36</td>\n",
       "      <td>717</td>\n",
       "      <td>80</td>\n",
       "      <td>361</td>\n",
       "      <td>42</td>\n",
       "      <td>48</td>\n",
       "      <td>20</td>\n",
       "      <td>220</td>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>313</td>\n",
       "      <td>45</td>\n",
       "      <td>677</td>\n",
       "      <td>34</td>\n",
       "      <td>149</td>\n",
       "      <td>2</td>\n",
       "      <td>1291</td>\n",
       "      <td>91</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>272</td>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>196</td>\n",
       "      <td>263</td>\n",
       "      <td>236</td>\n",
       "      <td>99</td>\n",
       "      <td>392</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>97</td>\n",
       "      <td>717</td>\n",
       "      <td>1830</td>\n",
       "      <td>756</td>\n",
       "      <td>2631</td>\n",
       "      <td>112</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>70</td>\n",
       "      <td>1536</td>\n",
       "      <td>60</td>\n",
       "      <td>10685.0</td>\n",
       "      <td>0.333271</td>\n",
       "      <td>0.527936</td>\n",
       "      <td>0.016846</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050725</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>0.054282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036032</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.025456</td>\n",
       "      <td>0.055030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.139765</td>\n",
       "      <td>-0.128652</td>\n",
       "      <td>-0.057141</td>\n",
       "      <td>-0.142251</td>\n",
       "      <td>-0.289034</td>\n",
       "      <td>-0.158395</td>\n",
       "      <td>-0.129012</td>\n",
       "      <td>-0.158177</td>\n",
       "      <td>-0.186548</td>\n",
       "      <td>-0.141902</td>\n",
       "      <td>63.0</td>\n",
       "      <td>-0.151812</td>\n",
       "      <td>0.089345</td>\n",
       "      <td>-0.498852</td>\n",
       "      <td>0.184373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>63</td>\n",
       "      <td>92</td>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>118</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>141</td>\n",
       "      <td>7</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>52</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>274</td>\n",
       "      <td>24</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>263</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>63</td>\n",
       "      <td>92</td>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>118</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>141</td>\n",
       "      <td>430</td>\n",
       "      <td>306</td>\n",
       "      <td>485</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>399</td>\n",
       "      <td>15</td>\n",
       "      <td>2648.0</td>\n",
       "      <td>0.245846</td>\n",
       "      <td>0.580816</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025680</td>\n",
       "      <td>0.047583</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.035498</td>\n",
       "      <td>0.068353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.116753</td>\n",
       "      <td>-0.307689</td>\n",
       "      <td>-0.283229</td>\n",
       "      <td>-0.351025</td>\n",
       "      <td>-0.187428</td>\n",
       "      <td>-0.165614</td>\n",
       "      <td>-0.106582</td>\n",
       "      <td>-0.111569</td>\n",
       "      <td>-0.157823</td>\n",
       "      <td>-0.196584</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.188787</td>\n",
       "      <td>0.077924</td>\n",
       "      <td>-0.351025</td>\n",
       "      <td>-0.106582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>140</td>\n",
       "      <td>61</td>\n",
       "      <td>249</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>3</td>\n",
       "      <td>952.0</td>\n",
       "      <td>0.328782</td>\n",
       "      <td>0.476891</td>\n",
       "      <td>0.108193</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.018908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.050420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.232832</td>\n",
       "      <td>-0.275567</td>\n",
       "      <td>-0.185545</td>\n",
       "      <td>-0.271512</td>\n",
       "      <td>-0.237491</td>\n",
       "      <td>-0.251401</td>\n",
       "      <td>-0.230924</td>\n",
       "      <td>-0.246818</td>\n",
       "      <td>-0.225742</td>\n",
       "      <td>-0.203368</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-0.225312</td>\n",
       "      <td>0.059647</td>\n",
       "      <td>-0.325741</td>\n",
       "      <td>-0.074291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>59</td>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>64</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>11</td>\n",
       "      <td>106</td>\n",
       "      <td>33</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>59</td>\n",
       "      <td>326</td>\n",
       "      <td>150</td>\n",
       "      <td>449</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>150</td>\n",
       "      <td>7</td>\n",
       "      <td>1774.0</td>\n",
       "      <td>0.436866</td>\n",
       "      <td>0.450395</td>\n",
       "      <td>0.049042</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.018602</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.115138</td>\n",
       "      <td>-0.133862</td>\n",
       "      <td>-0.114637</td>\n",
       "      <td>-0.128288</td>\n",
       "      <td>-0.139880</td>\n",
       "      <td>-0.158612</td>\n",
       "      <td>-0.155952</td>\n",
       "      <td>-0.139727</td>\n",
       "      <td>-0.166809</td>\n",
       "      <td>-0.140100</td>\n",
       "      <td>244.0</td>\n",
       "      <td>-0.138963</td>\n",
       "      <td>0.070131</td>\n",
       "      <td>-0.439416</td>\n",
       "      <td>0.102146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.007158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007430</td>\n",
       "      <td>0.008604</td>\n",
       "      <td>0.016274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>55</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>431</td>\n",
       "      <td>244</td>\n",
       "      <td>464</td>\n",
       "      <td>434</td>\n",
       "      <td>123</td>\n",
       "      <td>720</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>113</td>\n",
       "      <td>59</td>\n",
       "      <td>1004</td>\n",
       "      <td>81</td>\n",
       "      <td>485</td>\n",
       "      <td>71</td>\n",
       "      <td>64</td>\n",
       "      <td>31</td>\n",
       "      <td>369</td>\n",
       "      <td>1007</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>489</td>\n",
       "      <td>72</td>\n",
       "      <td>1060</td>\n",
       "      <td>72</td>\n",
       "      <td>115</td>\n",
       "      <td>2</td>\n",
       "      <td>2112</td>\n",
       "      <td>149</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>345</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>338</td>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>431</td>\n",
       "      <td>244</td>\n",
       "      <td>464</td>\n",
       "      <td>434</td>\n",
       "      <td>123</td>\n",
       "      <td>720</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>172</td>\n",
       "      <td>1004</td>\n",
       "      <td>2693</td>\n",
       "      <td>1204</td>\n",
       "      <td>3821</td>\n",
       "      <td>144</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>126</td>\n",
       "      <td>2416</td>\n",
       "      <td>78</td>\n",
       "      <td>15511.0</td>\n",
       "      <td>0.314615</td>\n",
       "      <td>0.541938</td>\n",
       "      <td>0.012636</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049126</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.050996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037135</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.028367</td>\n",
       "      <td>0.062149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.113174</td>\n",
       "      <td>-0.144975</td>\n",
       "      <td>-0.158093</td>\n",
       "      <td>-0.119558</td>\n",
       "      <td>-0.107860</td>\n",
       "      <td>-0.104501</td>\n",
       "      <td>-0.082456</td>\n",
       "      <td>-0.159880</td>\n",
       "      <td>-0.135831</td>\n",
       "      <td>-0.197376</td>\n",
       "      <td>409.0</td>\n",
       "      <td>-0.133436</td>\n",
       "      <td>0.091780</td>\n",
       "      <td>-0.490622</td>\n",
       "      <td>0.139780</td>\n",
       "      <td>0.003562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>207</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>409</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>92</td>\n",
       "      <td>555</td>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>97</td>\n",
       "      <td>881</td>\n",
       "      <td>38</td>\n",
       "      <td>335</td>\n",
       "      <td>52</td>\n",
       "      <td>69</td>\n",
       "      <td>46</td>\n",
       "      <td>334</td>\n",
       "      <td>778</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>253</td>\n",
       "      <td>25</td>\n",
       "      <td>786</td>\n",
       "      <td>202</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>852</td>\n",
       "      <td>163</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>409</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>92</td>\n",
       "      <td>555</td>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>207</td>\n",
       "      <td>881</td>\n",
       "      <td>2006</td>\n",
       "      <td>1013</td>\n",
       "      <td>1932</td>\n",
       "      <td>91</td>\n",
       "      <td>17</td>\n",
       "      <td>64</td>\n",
       "      <td>56</td>\n",
       "      <td>1495</td>\n",
       "      <td>86</td>\n",
       "      <td>11745.0</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.626564</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.029459</td>\n",
       "      <td>0.030055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016688</td>\n",
       "      <td>0.082077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.083933</td>\n",
       "      <td>-0.083389</td>\n",
       "      <td>-0.065732</td>\n",
       "      <td>-0.069459</td>\n",
       "      <td>-0.088702</td>\n",
       "      <td>-0.096248</td>\n",
       "      <td>-0.096313</td>\n",
       "      <td>-0.085411</td>\n",
       "      <td>-0.094288</td>\n",
       "      <td>-0.087477</td>\n",
       "      <td>160.0</td>\n",
       "      <td>-0.085095</td>\n",
       "      <td>0.039779</td>\n",
       "      <td>-0.227422</td>\n",
       "      <td>0.013461</td>\n",
       "      <td>0.009964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.013282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>119</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>160</td>\n",
       "      <td>727</td>\n",
       "      <td>727</td>\n",
       "      <td>51</td>\n",
       "      <td>326</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>609</td>\n",
       "      <td>13</td>\n",
       "      <td>176</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>214</td>\n",
       "      <td>453</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>147</td>\n",
       "      <td>28</td>\n",
       "      <td>583</td>\n",
       "      <td>90</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>913</td>\n",
       "      <td>62</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>160</td>\n",
       "      <td>727</td>\n",
       "      <td>727</td>\n",
       "      <td>51</td>\n",
       "      <td>326</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>609</td>\n",
       "      <td>1066</td>\n",
       "      <td>701</td>\n",
       "      <td>1412</td>\n",
       "      <td>73</td>\n",
       "      <td>53</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "      <td>2105</td>\n",
       "      <td>53</td>\n",
       "      <td>8490.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.666549</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.019081</td>\n",
       "      <td>0.031684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>0.057244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4702</th>\n",
       "      <td>-0.085858</td>\n",
       "      <td>-0.063337</td>\n",
       "      <td>-0.079229</td>\n",
       "      <td>-0.091919</td>\n",
       "      <td>-0.096248</td>\n",
       "      <td>-0.080136</td>\n",
       "      <td>-0.082838</td>\n",
       "      <td>-0.095074</td>\n",
       "      <td>-0.079131</td>\n",
       "      <td>-0.070018</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-0.079781</td>\n",
       "      <td>0.032987</td>\n",
       "      <td>-0.186941</td>\n",
       "      <td>-0.022269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.025962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025632</td>\n",
       "      <td>0.106830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015078</td>\n",
       "      <td>0.008755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>0.017902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>98</td>\n",
       "      <td>645</td>\n",
       "      <td>645</td>\n",
       "      <td>24</td>\n",
       "      <td>302</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>41</td>\n",
       "      <td>410</td>\n",
       "      <td>14</td>\n",
       "      <td>108</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>103</td>\n",
       "      <td>343</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>112</td>\n",
       "      <td>28</td>\n",
       "      <td>452</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>754</td>\n",
       "      <td>59</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>98</td>\n",
       "      <td>645</td>\n",
       "      <td>645</td>\n",
       "      <td>24</td>\n",
       "      <td>302</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>410</td>\n",
       "      <td>742</td>\n",
       "      <td>543</td>\n",
       "      <td>1211</td>\n",
       "      <td>66</td>\n",
       "      <td>56</td>\n",
       "      <td>22</td>\n",
       "      <td>35</td>\n",
       "      <td>1761</td>\n",
       "      <td>32</td>\n",
       "      <td>6808.0</td>\n",
       "      <td>0.126322</td>\n",
       "      <td>0.612368</td>\n",
       "      <td>0.011457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.022474</td>\n",
       "      <td>0.026293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>0.058754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4703</th>\n",
       "      <td>-0.121322</td>\n",
       "      <td>-0.141498</td>\n",
       "      <td>-0.219242</td>\n",
       "      <td>-0.130929</td>\n",
       "      <td>-0.151792</td>\n",
       "      <td>-0.103227</td>\n",
       "      <td>-0.153429</td>\n",
       "      <td>-0.103000</td>\n",
       "      <td>-0.161723</td>\n",
       "      <td>-0.161807</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-0.147332</td>\n",
       "      <td>0.064966</td>\n",
       "      <td>-0.303460</td>\n",
       "      <td>0.007940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>154</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>249</td>\n",
       "      <td>6</td>\n",
       "      <td>80</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>145</td>\n",
       "      <td>21</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>229</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>154</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>249</td>\n",
       "      <td>453</td>\n",
       "      <td>176</td>\n",
       "      <td>486</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>284</td>\n",
       "      <td>24</td>\n",
       "      <td>2627.0</td>\n",
       "      <td>0.301104</td>\n",
       "      <td>0.607156</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.024362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>-0.152871</td>\n",
       "      <td>-0.128073</td>\n",
       "      <td>-0.141993</td>\n",
       "      <td>-0.139232</td>\n",
       "      <td>-0.129344</td>\n",
       "      <td>-0.111234</td>\n",
       "      <td>-0.150500</td>\n",
       "      <td>-0.171991</td>\n",
       "      <td>-0.144837</td>\n",
       "      <td>-0.121257</td>\n",
       "      <td>121.0</td>\n",
       "      <td>-0.139362</td>\n",
       "      <td>0.063697</td>\n",
       "      <td>-0.286635</td>\n",
       "      <td>0.078633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027937</td>\n",
       "      <td>0.009381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.006199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>121</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "      <td>42</td>\n",
       "      <td>199</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>30</td>\n",
       "      <td>316</td>\n",
       "      <td>45</td>\n",
       "      <td>134</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>119</td>\n",
       "      <td>423</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>251</td>\n",
       "      <td>19</td>\n",
       "      <td>365</td>\n",
       "      <td>23</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>698</td>\n",
       "      <td>55</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>121</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "      <td>42</td>\n",
       "      <td>199</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>316</td>\n",
       "      <td>1048</td>\n",
       "      <td>407</td>\n",
       "      <td>1240</td>\n",
       "      <td>102</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>21</td>\n",
       "      <td>664</td>\n",
       "      <td>46</td>\n",
       "      <td>5565.0</td>\n",
       "      <td>0.337646</td>\n",
       "      <td>0.527583</td>\n",
       "      <td>0.026415</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038275</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.007188</td>\n",
       "      <td>0.057502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4705</th>\n",
       "      <td>-0.120232</td>\n",
       "      <td>-0.113216</td>\n",
       "      <td>-0.140783</td>\n",
       "      <td>-0.153671</td>\n",
       "      <td>-0.156706</td>\n",
       "      <td>-0.136795</td>\n",
       "      <td>-0.133603</td>\n",
       "      <td>-0.123864</td>\n",
       "      <td>-0.126231</td>\n",
       "      <td>-0.151991</td>\n",
       "      <td>193.0</td>\n",
       "      <td>-0.135289</td>\n",
       "      <td>0.055741</td>\n",
       "      <td>-0.292574</td>\n",
       "      <td>0.087972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>236</td>\n",
       "      <td>68</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>199</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>386</td>\n",
       "      <td>193</td>\n",
       "      <td>364</td>\n",
       "      <td>304</td>\n",
       "      <td>94</td>\n",
       "      <td>509</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>74</td>\n",
       "      <td>53</td>\n",
       "      <td>954</td>\n",
       "      <td>57</td>\n",
       "      <td>431</td>\n",
       "      <td>59</td>\n",
       "      <td>68</td>\n",
       "      <td>35</td>\n",
       "      <td>321</td>\n",
       "      <td>934</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>366</td>\n",
       "      <td>38</td>\n",
       "      <td>939</td>\n",
       "      <td>39</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>1694</td>\n",
       "      <td>106</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>343</td>\n",
       "      <td>7</td>\n",
       "      <td>199</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>386</td>\n",
       "      <td>193</td>\n",
       "      <td>364</td>\n",
       "      <td>304</td>\n",
       "      <td>94</td>\n",
       "      <td>509</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>127</td>\n",
       "      <td>954</td>\n",
       "      <td>2369</td>\n",
       "      <td>1016</td>\n",
       "      <td>3060</td>\n",
       "      <td>210</td>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>140</td>\n",
       "      <td>1852</td>\n",
       "      <td>97</td>\n",
       "      <td>13374.0</td>\n",
       "      <td>0.284358</td>\n",
       "      <td>0.581501</td>\n",
       "      <td>0.010094</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.004636</td>\n",
       "      <td>0.053836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038508</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.026469</td>\n",
       "      <td>0.052490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4706</th>\n",
       "      <td>-0.134740</td>\n",
       "      <td>-0.175553</td>\n",
       "      <td>-0.138711</td>\n",
       "      <td>-0.187195</td>\n",
       "      <td>-0.170077</td>\n",
       "      <td>-0.124311</td>\n",
       "      <td>-0.142302</td>\n",
       "      <td>-0.127974</td>\n",
       "      <td>-0.101206</td>\n",
       "      <td>-0.106120</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-0.138139</td>\n",
       "      <td>0.074304</td>\n",
       "      <td>-0.296549</td>\n",
       "      <td>0.094817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.014279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>98</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>44</td>\n",
       "      <td>207</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>262</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>33</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>137</td>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>103</td>\n",
       "      <td>6</td>\n",
       "      <td>264</td>\n",
       "      <td>59</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>346</td>\n",
       "      <td>72</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>98</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>44</td>\n",
       "      <td>207</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "      <td>262</td>\n",
       "      <td>770</td>\n",
       "      <td>329</td>\n",
       "      <td>803</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>475</td>\n",
       "      <td>45</td>\n",
       "      <td>4203.0</td>\n",
       "      <td>0.298834</td>\n",
       "      <td>0.599572</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.017844</td>\n",
       "      <td>0.028789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009517</td>\n",
       "      <td>0.072567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4707</th>\n",
       "      <td>-0.128066</td>\n",
       "      <td>-0.125542</td>\n",
       "      <td>-0.109559</td>\n",
       "      <td>-0.134165</td>\n",
       "      <td>-0.127037</td>\n",
       "      <td>-0.122411</td>\n",
       "      <td>-0.126918</td>\n",
       "      <td>-0.146291</td>\n",
       "      <td>-0.114619</td>\n",
       "      <td>-0.136114</td>\n",
       "      <td>232.0</td>\n",
       "      <td>-0.126829</td>\n",
       "      <td>0.055062</td>\n",
       "      <td>-0.246590</td>\n",
       "      <td>0.102430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.007546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.007240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007516</td>\n",
       "      <td>0.008703</td>\n",
       "      <td>0.038409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007193</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>218</td>\n",
       "      <td>51</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>513</td>\n",
       "      <td>232</td>\n",
       "      <td>501</td>\n",
       "      <td>477</td>\n",
       "      <td>141</td>\n",
       "      <td>600</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>84</td>\n",
       "      <td>59</td>\n",
       "      <td>929</td>\n",
       "      <td>69</td>\n",
       "      <td>439</td>\n",
       "      <td>53</td>\n",
       "      <td>62</td>\n",
       "      <td>35</td>\n",
       "      <td>310</td>\n",
       "      <td>874</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>380</td>\n",
       "      <td>38</td>\n",
       "      <td>934</td>\n",
       "      <td>55</td>\n",
       "      <td>154</td>\n",
       "      <td>2</td>\n",
       "      <td>1742</td>\n",
       "      <td>127</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>485</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>513</td>\n",
       "      <td>232</td>\n",
       "      <td>501</td>\n",
       "      <td>477</td>\n",
       "      <td>141</td>\n",
       "      <td>600</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>143</td>\n",
       "      <td>929</td>\n",
       "      <td>2312</td>\n",
       "      <td>1027</td>\n",
       "      <td>3403</td>\n",
       "      <td>117</td>\n",
       "      <td>53</td>\n",
       "      <td>38</td>\n",
       "      <td>123</td>\n",
       "      <td>2464</td>\n",
       "      <td>95</td>\n",
       "      <td>14024.0</td>\n",
       "      <td>0.296064</td>\n",
       "      <td>0.542213</td>\n",
       "      <td>0.010767</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049843</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.052767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038862</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.026954</td>\n",
       "      <td>0.059327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4708</th>\n",
       "      <td>-0.201320</td>\n",
       "      <td>-0.190717</td>\n",
       "      <td>-0.244932</td>\n",
       "      <td>-0.212910</td>\n",
       "      <td>-0.237600</td>\n",
       "      <td>-0.221831</td>\n",
       "      <td>-0.177502</td>\n",
       "      <td>-0.173041</td>\n",
       "      <td>-0.167976</td>\n",
       "      <td>-0.238571</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-0.203505</td>\n",
       "      <td>0.087052</td>\n",
       "      <td>-0.460204</td>\n",
       "      <td>0.088223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019505</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>68</td>\n",
       "      <td>121</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>166</td>\n",
       "      <td>22</td>\n",
       "      <td>120</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>96</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>45</td>\n",
       "      <td>227</td>\n",
       "      <td>38</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>68</td>\n",
       "      <td>121</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>166</td>\n",
       "      <td>679</td>\n",
       "      <td>310</td>\n",
       "      <td>838</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>388</td>\n",
       "      <td>29</td>\n",
       "      <td>3851.0</td>\n",
       "      <td>0.325370</td>\n",
       "      <td>0.525837</td>\n",
       "      <td>0.075305</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.017138</td>\n",
       "      <td>0.025967</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>0.053493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4709</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.135031</td>\n",
       "      <td>0.045189</td>\n",
       "      <td>-0.189186</td>\n",
       "      <td>-0.074113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>66</td>\n",
       "      <td>32</td>\n",
       "      <td>92</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>0.335681</td>\n",
       "      <td>0.556338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051643</td>\n",
       "      <td>0.056338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4710</th>\n",
       "      <td>-0.107876</td>\n",
       "      <td>-0.203350</td>\n",
       "      <td>-0.176367</td>\n",
       "      <td>-0.191980</td>\n",
       "      <td>-0.203361</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>-0.134112</td>\n",
       "      <td>-0.140574</td>\n",
       "      <td>-0.109193</td>\n",
       "      <td>-0.086428</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-0.148018</td>\n",
       "      <td>0.057453</td>\n",
       "      <td>-0.247001</td>\n",
       "      <td>-0.047546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>56</td>\n",
       "      <td>201</td>\n",
       "      <td>86</td>\n",
       "      <td>263</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>125</td>\n",
       "      <td>5</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0.363713</td>\n",
       "      <td>0.516456</td>\n",
       "      <td>0.040506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036287</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>0.061603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4711</th>\n",
       "      <td>-0.125395</td>\n",
       "      <td>-0.164166</td>\n",
       "      <td>-0.090420</td>\n",
       "      <td>-0.157351</td>\n",
       "      <td>-0.161474</td>\n",
       "      <td>-0.131401</td>\n",
       "      <td>-0.198607</td>\n",
       "      <td>-0.164848</td>\n",
       "      <td>-0.136990</td>\n",
       "      <td>-0.125657</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-0.142489</td>\n",
       "      <td>0.047514</td>\n",
       "      <td>-0.269744</td>\n",
       "      <td>-0.049292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>77</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>27</td>\n",
       "      <td>184</td>\n",
       "      <td>13</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>65</td>\n",
       "      <td>219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>40</td>\n",
       "      <td>169</td>\n",
       "      <td>21</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>214</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>77</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>184</td>\n",
       "      <td>512</td>\n",
       "      <td>230</td>\n",
       "      <td>591</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>195</td>\n",
       "      <td>28</td>\n",
       "      <td>3182.0</td>\n",
       "      <td>0.260214</td>\n",
       "      <td>0.599937</td>\n",
       "      <td>0.087681</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.036141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4712</th>\n",
       "      <td>-0.165578</td>\n",
       "      <td>-0.151789</td>\n",
       "      <td>-0.151372</td>\n",
       "      <td>-0.130441</td>\n",
       "      <td>-0.133698</td>\n",
       "      <td>-0.166450</td>\n",
       "      <td>-0.164515</td>\n",
       "      <td>-0.148081</td>\n",
       "      <td>-0.133122</td>\n",
       "      <td>-0.165576</td>\n",
       "      <td>514.0</td>\n",
       "      <td>-0.150983</td>\n",
       "      <td>0.084808</td>\n",
       "      <td>-0.428837</td>\n",
       "      <td>0.118114</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>270</td>\n",
       "      <td>112</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "      <td>514</td>\n",
       "      <td>385</td>\n",
       "      <td>364</td>\n",
       "      <td>81</td>\n",
       "      <td>408</td>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>218</td>\n",
       "      <td>90</td>\n",
       "      <td>1201</td>\n",
       "      <td>103</td>\n",
       "      <td>727</td>\n",
       "      <td>110</td>\n",
       "      <td>61</td>\n",
       "      <td>104</td>\n",
       "      <td>302</td>\n",
       "      <td>1441</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>748</td>\n",
       "      <td>35</td>\n",
       "      <td>1448</td>\n",
       "      <td>76</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>2405</td>\n",
       "      <td>218</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>576</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>434</td>\n",
       "      <td>1</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "      <td>421</td>\n",
       "      <td>3</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "      <td>514</td>\n",
       "      <td>385</td>\n",
       "      <td>364</td>\n",
       "      <td>81</td>\n",
       "      <td>408</td>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>308</td>\n",
       "      <td>1201</td>\n",
       "      <td>3623</td>\n",
       "      <td>1559</td>\n",
       "      <td>4879</td>\n",
       "      <td>156</td>\n",
       "      <td>33</td>\n",
       "      <td>163</td>\n",
       "      <td>79</td>\n",
       "      <td>1946</td>\n",
       "      <td>133</td>\n",
       "      <td>20649.0</td>\n",
       "      <td>0.354932</td>\n",
       "      <td>0.522640</td>\n",
       "      <td>0.027411</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.008572</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.007942</td>\n",
       "      <td>0.044651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4713</th>\n",
       "      <td>-0.207456</td>\n",
       "      <td>-0.205270</td>\n",
       "      <td>-0.167800</td>\n",
       "      <td>-0.141775</td>\n",
       "      <td>-0.187631</td>\n",
       "      <td>-0.177274</td>\n",
       "      <td>-0.175009</td>\n",
       "      <td>-0.175240</td>\n",
       "      <td>-0.176716</td>\n",
       "      <td>-0.189538</td>\n",
       "      <td>142.0</td>\n",
       "      <td>-0.180398</td>\n",
       "      <td>0.065842</td>\n",
       "      <td>-0.362528</td>\n",
       "      <td>-0.013263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026449</td>\n",
       "      <td>0.026214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>142</td>\n",
       "      <td>127</td>\n",
       "      <td>83</td>\n",
       "      <td>46</td>\n",
       "      <td>92</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>345</td>\n",
       "      <td>27</td>\n",
       "      <td>187</td>\n",
       "      <td>33</td>\n",
       "      <td>26</td>\n",
       "      <td>15</td>\n",
       "      <td>180</td>\n",
       "      <td>472</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>232</td>\n",
       "      <td>38</td>\n",
       "      <td>403</td>\n",
       "      <td>80</td>\n",
       "      <td>115</td>\n",
       "      <td>3</td>\n",
       "      <td>620</td>\n",
       "      <td>89</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>4</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>142</td>\n",
       "      <td>127</td>\n",
       "      <td>83</td>\n",
       "      <td>46</td>\n",
       "      <td>92</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>345</td>\n",
       "      <td>1183</td>\n",
       "      <td>521</td>\n",
       "      <td>1294</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "      <td>563</td>\n",
       "      <td>59</td>\n",
       "      <td>6431.0</td>\n",
       "      <td>0.271342</td>\n",
       "      <td>0.629762</td>\n",
       "      <td>0.013217</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>0.028145</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.010885</td>\n",
       "      <td>0.036386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>-0.054248</td>\n",
       "      <td>-0.030412</td>\n",
       "      <td>-0.017290</td>\n",
       "      <td>-0.057763</td>\n",
       "      <td>-0.050354</td>\n",
       "      <td>-0.062937</td>\n",
       "      <td>-0.113614</td>\n",
       "      <td>-0.059191</td>\n",
       "      <td>-0.050771</td>\n",
       "      <td>-0.068958</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-0.057248</td>\n",
       "      <td>0.040235</td>\n",
       "      <td>-0.196028</td>\n",
       "      <td>0.039355</td>\n",
       "      <td>0.014376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.012775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019322</td>\n",
       "      <td>0.028998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>52</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>12</td>\n",
       "      <td>152</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>90</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>166</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>52</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>12</td>\n",
       "      <td>152</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>182</td>\n",
       "      <td>330</td>\n",
       "      <td>210</td>\n",
       "      <td>343</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>592</td>\n",
       "      <td>9</td>\n",
       "      <td>2412.0</td>\n",
       "      <td>0.078773</td>\n",
       "      <td>0.691542</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026534</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013267</td>\n",
       "      <td>0.084577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4715</th>\n",
       "      <td>-0.156127</td>\n",
       "      <td>-0.211235</td>\n",
       "      <td>-0.163014</td>\n",
       "      <td>-0.118812</td>\n",
       "      <td>-0.154546</td>\n",
       "      <td>-0.139603</td>\n",
       "      <td>-0.161676</td>\n",
       "      <td>-0.262062</td>\n",
       "      <td>-0.179215</td>\n",
       "      <td>-0.165041</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.168931</td>\n",
       "      <td>0.090626</td>\n",
       "      <td>-0.369241</td>\n",
       "      <td>0.053870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>90</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>116</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>90</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>90</td>\n",
       "      <td>353</td>\n",
       "      <td>150</td>\n",
       "      <td>342</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>182</td>\n",
       "      <td>27</td>\n",
       "      <td>1796.0</td>\n",
       "      <td>0.326281</td>\n",
       "      <td>0.574610</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020045</td>\n",
       "      <td>0.033408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4716</th>\n",
       "      <td>-0.261848</td>\n",
       "      <td>-0.182337</td>\n",
       "      <td>-0.293268</td>\n",
       "      <td>-0.164469</td>\n",
       "      <td>-0.172912</td>\n",
       "      <td>-0.175728</td>\n",
       "      <td>-0.128874</td>\n",
       "      <td>-0.115546</td>\n",
       "      <td>-0.082037</td>\n",
       "      <td>-0.200381</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-0.180353</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>-0.428633</td>\n",
       "      <td>0.231218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>46</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>113</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>126</td>\n",
       "      <td>19</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>90</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>42</td>\n",
       "      <td>186</td>\n",
       "      <td>58</td>\n",
       "      <td>116</td>\n",
       "      <td>6</td>\n",
       "      <td>299</td>\n",
       "      <td>27</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>46</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>113</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>126</td>\n",
       "      <td>584</td>\n",
       "      <td>286</td>\n",
       "      <td>723</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>226</td>\n",
       "      <td>18</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>0.328354</td>\n",
       "      <td>0.579026</td>\n",
       "      <td>0.032865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.011951</td>\n",
       "      <td>0.024798</td>\n",
       "      <td>0.002390</td>\n",
       "      <td>0.005378</td>\n",
       "      <td>0.047505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4717</th>\n",
       "      <td>-0.221290</td>\n",
       "      <td>-0.170864</td>\n",
       "      <td>-0.098491</td>\n",
       "      <td>-0.150004</td>\n",
       "      <td>-0.135836</td>\n",
       "      <td>-0.236786</td>\n",
       "      <td>-0.215624</td>\n",
       "      <td>-0.163024</td>\n",
       "      <td>-0.097864</td>\n",
       "      <td>-0.174253</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-0.174888</td>\n",
       "      <td>0.071249</td>\n",
       "      <td>-0.345566</td>\n",
       "      <td>-0.052140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>74</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>134</td>\n",
       "      <td>16</td>\n",
       "      <td>73</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>80</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>21</td>\n",
       "      <td>155</td>\n",
       "      <td>48</td>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "      <td>218</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>74</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>134</td>\n",
       "      <td>465</td>\n",
       "      <td>224</td>\n",
       "      <td>497</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>166</td>\n",
       "      <td>10</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>0.284517</td>\n",
       "      <td>0.640064</td>\n",
       "      <td>0.015962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018755</td>\n",
       "      <td>0.027534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003192</td>\n",
       "      <td>0.047486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4718</th>\n",
       "      <td>-0.215841</td>\n",
       "      <td>-0.172014</td>\n",
       "      <td>-0.162727</td>\n",
       "      <td>-0.148903</td>\n",
       "      <td>-0.122497</td>\n",
       "      <td>-0.222249</td>\n",
       "      <td>-0.120452</td>\n",
       "      <td>-0.154253</td>\n",
       "      <td>-0.112007</td>\n",
       "      <td>-0.301705</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-0.184203</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.401055</td>\n",
       "      <td>0.165973</td>\n",
       "      <td>0.049323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>120</td>\n",
       "      <td>69</td>\n",
       "      <td>139</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>3</td>\n",
       "      <td>636.0</td>\n",
       "      <td>0.287736</td>\n",
       "      <td>0.614780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.015723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>0.066038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4719</th>\n",
       "      <td>-0.144916</td>\n",
       "      <td>-0.149149</td>\n",
       "      <td>-0.149216</td>\n",
       "      <td>-0.134031</td>\n",
       "      <td>-0.157642</td>\n",
       "      <td>-0.159852</td>\n",
       "      <td>-0.149372</td>\n",
       "      <td>-0.184161</td>\n",
       "      <td>-0.146648</td>\n",
       "      <td>-0.133118</td>\n",
       "      <td>270.0</td>\n",
       "      <td>-0.150811</td>\n",
       "      <td>0.062526</td>\n",
       "      <td>-0.352741</td>\n",
       "      <td>0.052689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102015</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006441</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>288</td>\n",
       "      <td>71</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>632</td>\n",
       "      <td>270</td>\n",
       "      <td>528</td>\n",
       "      <td>490</td>\n",
       "      <td>179</td>\n",
       "      <td>706</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>122</td>\n",
       "      <td>128</td>\n",
       "      <td>1106</td>\n",
       "      <td>60</td>\n",
       "      <td>489</td>\n",
       "      <td>110</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>450</td>\n",
       "      <td>1051</td>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "      <td>422</td>\n",
       "      <td>76</td>\n",
       "      <td>1085</td>\n",
       "      <td>189</td>\n",
       "      <td>194</td>\n",
       "      <td>8</td>\n",
       "      <td>1539</td>\n",
       "      <td>141</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>338</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>287</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "      <td>5</td>\n",
       "      <td>393</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>632</td>\n",
       "      <td>270</td>\n",
       "      <td>528</td>\n",
       "      <td>490</td>\n",
       "      <td>179</td>\n",
       "      <td>706</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>250</td>\n",
       "      <td>1106</td>\n",
       "      <td>2826</td>\n",
       "      <td>1350</td>\n",
       "      <td>3323</td>\n",
       "      <td>167</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>210</td>\n",
       "      <td>2805</td>\n",
       "      <td>98</td>\n",
       "      <td>16763.0</td>\n",
       "      <td>0.285868</td>\n",
       "      <td>0.559387</td>\n",
       "      <td>0.015510</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.022132</td>\n",
       "      <td>0.027203</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.032810</td>\n",
       "      <td>0.058223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4720</th>\n",
       "      <td>-0.181920</td>\n",
       "      <td>-0.116458</td>\n",
       "      <td>-0.148547</td>\n",
       "      <td>-0.158607</td>\n",
       "      <td>-0.068560</td>\n",
       "      <td>-0.195990</td>\n",
       "      <td>-0.131663</td>\n",
       "      <td>-0.159923</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>-0.192125</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-0.152520</td>\n",
       "      <td>0.052501</td>\n",
       "      <td>-0.223938</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>58</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>39</td>\n",
       "      <td>161</td>\n",
       "      <td>81</td>\n",
       "      <td>180</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>6</td>\n",
       "      <td>928.0</td>\n",
       "      <td>0.311422</td>\n",
       "      <td>0.623922</td>\n",
       "      <td>0.012931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015086</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.037716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>-0.148465</td>\n",
       "      <td>-0.180180</td>\n",
       "      <td>-0.119831</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>-0.124579</td>\n",
       "      <td>-0.147619</td>\n",
       "      <td>-0.156491</td>\n",
       "      <td>-0.154822</td>\n",
       "      <td>-0.160716</td>\n",
       "      <td>-0.161211</td>\n",
       "      <td>199.0</td>\n",
       "      <td>-0.147177</td>\n",
       "      <td>0.070223</td>\n",
       "      <td>-0.384161</td>\n",
       "      <td>0.196279</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.013188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009978</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016626</td>\n",
       "      <td>0.009293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026197</td>\n",
       "      <td>0.006491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>59</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>379</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>369</td>\n",
       "      <td>138</td>\n",
       "      <td>630</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "      <td>102</td>\n",
       "      <td>842</td>\n",
       "      <td>40</td>\n",
       "      <td>369</td>\n",
       "      <td>76</td>\n",
       "      <td>53</td>\n",
       "      <td>51</td>\n",
       "      <td>392</td>\n",
       "      <td>884</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>337</td>\n",
       "      <td>52</td>\n",
       "      <td>920</td>\n",
       "      <td>135</td>\n",
       "      <td>143</td>\n",
       "      <td>5</td>\n",
       "      <td>1396</td>\n",
       "      <td>111</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>379</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>369</td>\n",
       "      <td>138</td>\n",
       "      <td>630</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>223</td>\n",
       "      <td>842</td>\n",
       "      <td>2287</td>\n",
       "      <td>1107</td>\n",
       "      <td>2598</td>\n",
       "      <td>121</td>\n",
       "      <td>48</td>\n",
       "      <td>30</td>\n",
       "      <td>102</td>\n",
       "      <td>2135</td>\n",
       "      <td>66</td>\n",
       "      <td>13606.0</td>\n",
       "      <td>0.280758</td>\n",
       "      <td>0.561370</td>\n",
       "      <td>0.022931</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.026385</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.025283</td>\n",
       "      <td>0.060929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4722</th>\n",
       "      <td>-0.063428</td>\n",
       "      <td>-0.068324</td>\n",
       "      <td>-0.079974</td>\n",
       "      <td>-0.089506</td>\n",
       "      <td>-0.098025</td>\n",
       "      <td>-0.085413</td>\n",
       "      <td>-0.070186</td>\n",
       "      <td>-0.055048</td>\n",
       "      <td>-0.067396</td>\n",
       "      <td>-0.074446</td>\n",
       "      <td>142.0</td>\n",
       "      <td>-0.075465</td>\n",
       "      <td>0.037825</td>\n",
       "      <td>-0.158405</td>\n",
       "      <td>0.018317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019798</td>\n",
       "      <td>0.022572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.014726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>106</td>\n",
       "      <td>41</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>142</td>\n",
       "      <td>767</td>\n",
       "      <td>767</td>\n",
       "      <td>46</td>\n",
       "      <td>320</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>51</td>\n",
       "      <td>597</td>\n",
       "      <td>5</td>\n",
       "      <td>172</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>184</td>\n",
       "      <td>396</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>126</td>\n",
       "      <td>35</td>\n",
       "      <td>509</td>\n",
       "      <td>86</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>836</td>\n",
       "      <td>92</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>142</td>\n",
       "      <td>767</td>\n",
       "      <td>767</td>\n",
       "      <td>46</td>\n",
       "      <td>320</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>597</td>\n",
       "      <td>982</td>\n",
       "      <td>630</td>\n",
       "      <td>1441</td>\n",
       "      <td>95</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>2130</td>\n",
       "      <td>77</td>\n",
       "      <td>8374.0</td>\n",
       "      <td>0.115357</td>\n",
       "      <td>0.643898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019226</td>\n",
       "      <td>0.031168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>0.055171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4723</th>\n",
       "      <td>-0.167057</td>\n",
       "      <td>-0.126165</td>\n",
       "      <td>-0.143099</td>\n",
       "      <td>-0.185517</td>\n",
       "      <td>-0.141920</td>\n",
       "      <td>-0.156715</td>\n",
       "      <td>-0.176208</td>\n",
       "      <td>-0.162119</td>\n",
       "      <td>-0.092583</td>\n",
       "      <td>-0.179953</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-0.156543</td>\n",
       "      <td>0.046266</td>\n",
       "      <td>-0.213069</td>\n",
       "      <td>-0.001866</td>\n",
       "      <td>0.020347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>123</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>112</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>186</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>123</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>67</td>\n",
       "      <td>311</td>\n",
       "      <td>123</td>\n",
       "      <td>391</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>191</td>\n",
       "      <td>19</td>\n",
       "      <td>1768.0</td>\n",
       "      <td>0.356900</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.008484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045814</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.007919</td>\n",
       "      <td>0.083145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4724</th>\n",
       "      <td>-0.144127</td>\n",
       "      <td>-0.128513</td>\n",
       "      <td>-0.146529</td>\n",
       "      <td>-0.232164</td>\n",
       "      <td>-0.210389</td>\n",
       "      <td>-0.107859</td>\n",
       "      <td>-0.056922</td>\n",
       "      <td>-0.142439</td>\n",
       "      <td>-0.209709</td>\n",
       "      <td>-0.043173</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.142182</td>\n",
       "      <td>0.063042</td>\n",
       "      <td>-0.232164</td>\n",
       "      <td>-0.043173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>135</td>\n",
       "      <td>54</td>\n",
       "      <td>137</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>696.0</td>\n",
       "      <td>0.261494</td>\n",
       "      <td>0.669540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.060345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4725</th>\n",
       "      <td>-0.152424</td>\n",
       "      <td>-0.132677</td>\n",
       "      <td>-0.129951</td>\n",
       "      <td>-0.142684</td>\n",
       "      <td>-0.153350</td>\n",
       "      <td>-0.138757</td>\n",
       "      <td>-0.155047</td>\n",
       "      <td>-0.111022</td>\n",
       "      <td>-0.140244</td>\n",
       "      <td>-0.105691</td>\n",
       "      <td>396.0</td>\n",
       "      <td>-0.136218</td>\n",
       "      <td>0.085022</td>\n",
       "      <td>-0.468893</td>\n",
       "      <td>0.129150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>62</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>131</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>222</td>\n",
       "      <td>396</td>\n",
       "      <td>76</td>\n",
       "      <td>67</td>\n",
       "      <td>116</td>\n",
       "      <td>569</td>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>83</td>\n",
       "      <td>887</td>\n",
       "      <td>45</td>\n",
       "      <td>385</td>\n",
       "      <td>73</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>319</td>\n",
       "      <td>905</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>366</td>\n",
       "      <td>42</td>\n",
       "      <td>776</td>\n",
       "      <td>179</td>\n",
       "      <td>229</td>\n",
       "      <td>6</td>\n",
       "      <td>1014</td>\n",
       "      <td>223</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>286</td>\n",
       "      <td>5</td>\n",
       "      <td>131</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>222</td>\n",
       "      <td>396</td>\n",
       "      <td>76</td>\n",
       "      <td>67</td>\n",
       "      <td>116</td>\n",
       "      <td>569</td>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>887</td>\n",
       "      <td>2233</td>\n",
       "      <td>997</td>\n",
       "      <td>2294</td>\n",
       "      <td>139</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "      <td>36</td>\n",
       "      <td>1446</td>\n",
       "      <td>101</td>\n",
       "      <td>12389.0</td>\n",
       "      <td>0.300993</td>\n",
       "      <td>0.591412</td>\n",
       "      <td>0.010090</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.025991</td>\n",
       "      <td>0.031641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.077892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4726</th>\n",
       "      <td>-0.084071</td>\n",
       "      <td>-0.067437</td>\n",
       "      <td>-0.057563</td>\n",
       "      <td>-0.074639</td>\n",
       "      <td>-0.071882</td>\n",
       "      <td>-0.079999</td>\n",
       "      <td>-0.109161</td>\n",
       "      <td>-0.119563</td>\n",
       "      <td>-0.077010</td>\n",
       "      <td>-0.061073</td>\n",
       "      <td>130.0</td>\n",
       "      <td>-0.080240</td>\n",
       "      <td>0.088660</td>\n",
       "      <td>-0.891896</td>\n",
       "      <td>0.036641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01998</td>\n",
       "      <td>0.017507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.399806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017393</td>\n",
       "      <td>0.017387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>130</td>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>15</td>\n",
       "      <td>172</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>29</td>\n",
       "      <td>334</td>\n",
       "      <td>8</td>\n",
       "      <td>89</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>127</td>\n",
       "      <td>285</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>315</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>408</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>130</td>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>15</td>\n",
       "      <td>172</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>79</td>\n",
       "      <td>334</td>\n",
       "      <td>614</td>\n",
       "      <td>370</td>\n",
       "      <td>669</td>\n",
       "      <td>44</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>1080</td>\n",
       "      <td>12</td>\n",
       "      <td>4586.0</td>\n",
       "      <td>0.086132</td>\n",
       "      <td>0.679023</td>\n",
       "      <td>0.016354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016790</td>\n",
       "      <td>0.033362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>0.065853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4727</th>\n",
       "      <td>-0.161635</td>\n",
       "      <td>-0.159153</td>\n",
       "      <td>-0.204958</td>\n",
       "      <td>-0.143037</td>\n",
       "      <td>-0.170804</td>\n",
       "      <td>-0.156172</td>\n",
       "      <td>-0.207502</td>\n",
       "      <td>-0.182191</td>\n",
       "      <td>-0.143094</td>\n",
       "      <td>-0.105638</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.171518</td>\n",
       "      <td>0.078435</td>\n",
       "      <td>-0.495515</td>\n",
       "      <td>0.002742</td>\n",
       "      <td>0.020893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>151</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>143</td>\n",
       "      <td>16</td>\n",
       "      <td>69</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>71</td>\n",
       "      <td>227</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>18</td>\n",
       "      <td>193</td>\n",
       "      <td>27</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>267</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>151</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>143</td>\n",
       "      <td>528</td>\n",
       "      <td>238</td>\n",
       "      <td>563</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>275</td>\n",
       "      <td>6</td>\n",
       "      <td>2739.0</td>\n",
       "      <td>0.354509</td>\n",
       "      <td>0.550931</td>\n",
       "      <td>0.011318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.017525</td>\n",
       "      <td>0.017890</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.070099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4728</th>\n",
       "      <td>-0.120242</td>\n",
       "      <td>-0.129960</td>\n",
       "      <td>-0.139428</td>\n",
       "      <td>-0.131272</td>\n",
       "      <td>-0.132119</td>\n",
       "      <td>-0.136361</td>\n",
       "      <td>-0.110012</td>\n",
       "      <td>-0.137535</td>\n",
       "      <td>-0.126243</td>\n",
       "      <td>-0.119633</td>\n",
       "      <td>293.0</td>\n",
       "      <td>-0.127915</td>\n",
       "      <td>0.063033</td>\n",
       "      <td>-0.381321</td>\n",
       "      <td>0.030037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007507</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006205</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>69</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>631</td>\n",
       "      <td>293</td>\n",
       "      <td>543</td>\n",
       "      <td>498</td>\n",
       "      <td>179</td>\n",
       "      <td>764</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>86</td>\n",
       "      <td>54</td>\n",
       "      <td>1071</td>\n",
       "      <td>79</td>\n",
       "      <td>505</td>\n",
       "      <td>57</td>\n",
       "      <td>79</td>\n",
       "      <td>45</td>\n",
       "      <td>358</td>\n",
       "      <td>1131</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>435</td>\n",
       "      <td>72</td>\n",
       "      <td>1137</td>\n",
       "      <td>54</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>2226</td>\n",
       "      <td>133</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>1</td>\n",
       "      <td>323</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>631</td>\n",
       "      <td>293</td>\n",
       "      <td>543</td>\n",
       "      <td>498</td>\n",
       "      <td>179</td>\n",
       "      <td>764</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>140</td>\n",
       "      <td>1071</td>\n",
       "      <td>2784</td>\n",
       "      <td>1263</td>\n",
       "      <td>3873</td>\n",
       "      <td>134</td>\n",
       "      <td>60</td>\n",
       "      <td>37</td>\n",
       "      <td>298</td>\n",
       "      <td>2908</td>\n",
       "      <td>75</td>\n",
       "      <td>16485.0</td>\n",
       "      <td>0.307795</td>\n",
       "      <td>0.527631</td>\n",
       "      <td>0.011829</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057386</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.059569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029360</td>\n",
       "      <td>0.064119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4729</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.099566</td>\n",
       "      <td>0.087906</td>\n",
       "      <td>-0.212082</td>\n",
       "      <td>-0.015499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>46</td>\n",
       "      <td>26</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>317.0</td>\n",
       "      <td>0.312303</td>\n",
       "      <td>0.542587</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053628</td>\n",
       "      <td>0.012618</td>\n",
       "      <td>0.012618</td>\n",
       "      <td>0.075710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4730</th>\n",
       "      <td>-0.052218</td>\n",
       "      <td>-0.051022</td>\n",
       "      <td>-0.052372</td>\n",
       "      <td>-0.074581</td>\n",
       "      <td>-0.063387</td>\n",
       "      <td>-0.055917</td>\n",
       "      <td>-0.069329</td>\n",
       "      <td>-0.050425</td>\n",
       "      <td>-0.057307</td>\n",
       "      <td>-0.046225</td>\n",
       "      <td>165.0</td>\n",
       "      <td>-0.057358</td>\n",
       "      <td>0.048331</td>\n",
       "      <td>-0.183807</td>\n",
       "      <td>0.115533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044889</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009306</td>\n",
       "      <td>0.005558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>165</td>\n",
       "      <td>755</td>\n",
       "      <td>746</td>\n",
       "      <td>43</td>\n",
       "      <td>446</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>59</td>\n",
       "      <td>658</td>\n",
       "      <td>12</td>\n",
       "      <td>215</td>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>177</td>\n",
       "      <td>459</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>118</td>\n",
       "      <td>29</td>\n",
       "      <td>606</td>\n",
       "      <td>68</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>863</td>\n",
       "      <td>118</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>165</td>\n",
       "      <td>755</td>\n",
       "      <td>746</td>\n",
       "      <td>43</td>\n",
       "      <td>446</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>658</td>\n",
       "      <td>1097</td>\n",
       "      <td>703</td>\n",
       "      <td>1450</td>\n",
       "      <td>126</td>\n",
       "      <td>78</td>\n",
       "      <td>36</td>\n",
       "      <td>56</td>\n",
       "      <td>2267</td>\n",
       "      <td>31</td>\n",
       "      <td>9279.0</td>\n",
       "      <td>0.101088</td>\n",
       "      <td>0.667744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.018536</td>\n",
       "      <td>0.028882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009699</td>\n",
       "      <td>0.065848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4731</th>\n",
       "      <td>-0.126338</td>\n",
       "      <td>-0.151145</td>\n",
       "      <td>-0.146070</td>\n",
       "      <td>-0.160818</td>\n",
       "      <td>-0.116093</td>\n",
       "      <td>-0.143521</td>\n",
       "      <td>-0.148397</td>\n",
       "      <td>-0.132973</td>\n",
       "      <td>-0.154395</td>\n",
       "      <td>-0.162108</td>\n",
       "      <td>239.0</td>\n",
       "      <td>-0.144025</td>\n",
       "      <td>0.060721</td>\n",
       "      <td>-0.387698</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.013332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>63</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>441</td>\n",
       "      <td>239</td>\n",
       "      <td>407</td>\n",
       "      <td>369</td>\n",
       "      <td>115</td>\n",
       "      <td>501</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>71</td>\n",
       "      <td>45</td>\n",
       "      <td>813</td>\n",
       "      <td>79</td>\n",
       "      <td>397</td>\n",
       "      <td>61</td>\n",
       "      <td>67</td>\n",
       "      <td>17</td>\n",
       "      <td>310</td>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>349</td>\n",
       "      <td>43</td>\n",
       "      <td>879</td>\n",
       "      <td>50</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>1727</td>\n",
       "      <td>116</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>353</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>282</td>\n",
       "      <td>1</td>\n",
       "      <td>322</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>441</td>\n",
       "      <td>239</td>\n",
       "      <td>407</td>\n",
       "      <td>369</td>\n",
       "      <td>115</td>\n",
       "      <td>501</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>116</td>\n",
       "      <td>813</td>\n",
       "      <td>2240</td>\n",
       "      <td>972</td>\n",
       "      <td>3195</td>\n",
       "      <td>159</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>92</td>\n",
       "      <td>2072</td>\n",
       "      <td>54</td>\n",
       "      <td>12830.0</td>\n",
       "      <td>0.300156</td>\n",
       "      <td>0.541154</td>\n",
       "      <td>0.017537</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042557</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.044583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032424</td>\n",
       "      <td>0.057677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4732 rows × 19570 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      negaposi_split_0  negaposi_split_1  negaposi_split_2  negaposi_split_3  \\\n",
       "0            -0.189030         -0.130570         -0.175781         -0.159920   \n",
       "1            -0.112867         -0.149643         -0.163352         -0.187332   \n",
       "2                  NaN               NaN               NaN               NaN   \n",
       "3            -0.208239         -0.178418         -0.181850         -0.188961   \n",
       "4            -0.169961         -0.244158         -0.193420         -0.150912   \n",
       "5            -0.157527         -0.165958         -0.161920         -0.168265   \n",
       "6            -0.151227         -0.137505         -0.194724         -0.120212   \n",
       "7            -0.075585         -0.135701         -0.201506         -0.098010   \n",
       "8            -0.110723         -0.116878         -0.152853         -0.130457   \n",
       "9            -0.150055         -0.149588         -0.160721         -0.139322   \n",
       "10           -0.153396         -0.197875         -0.206584         -0.195620   \n",
       "11           -0.141464         -0.013136         -0.180173         -0.135678   \n",
       "12           -0.182168         -0.091016         -0.066492         -0.083976   \n",
       "13           -0.151403         -0.142008         -0.157573         -0.178763   \n",
       "14           -0.147732         -0.137332         -0.111306         -0.149204   \n",
       "15           -0.139510         -0.182005         -0.147175         -0.180676   \n",
       "16           -0.049277         -0.065759         -0.077839         -0.086413   \n",
       "17           -0.152688         -0.165167         -0.207856         -0.113340   \n",
       "18           -0.166528         -0.161682         -0.150449         -0.146219   \n",
       "19           -0.186360         -0.216576         -0.075086         -0.147251   \n",
       "20           -0.184261         -0.142509         -0.299094         -0.159134   \n",
       "21           -0.098940         -0.165344         -0.156370         -0.117170   \n",
       "22           -0.123098         -0.161399         -0.087035         -0.152334   \n",
       "23           -0.135801         -0.144990         -0.125146         -0.125077   \n",
       "24           -0.139765         -0.128652         -0.057141         -0.142251   \n",
       "25           -0.116753         -0.307689         -0.283229         -0.351025   \n",
       "26           -0.232832         -0.275567         -0.185545         -0.271512   \n",
       "27           -0.115138         -0.133862         -0.114637         -0.128288   \n",
       "28           -0.113174         -0.144975         -0.158093         -0.119558   \n",
       "29           -0.083933         -0.083389         -0.065732         -0.069459   \n",
       "...                ...               ...               ...               ...   \n",
       "4702         -0.085858         -0.063337         -0.079229         -0.091919   \n",
       "4703         -0.121322         -0.141498         -0.219242         -0.130929   \n",
       "4704         -0.152871         -0.128073         -0.141993         -0.139232   \n",
       "4705         -0.120232         -0.113216         -0.140783         -0.153671   \n",
       "4706         -0.134740         -0.175553         -0.138711         -0.187195   \n",
       "4707         -0.128066         -0.125542         -0.109559         -0.134165   \n",
       "4708         -0.201320         -0.190717         -0.244932         -0.212910   \n",
       "4709               NaN               NaN               NaN               NaN   \n",
       "4710         -0.107876         -0.203350         -0.176367         -0.191980   \n",
       "4711         -0.125395         -0.164166         -0.090420         -0.157351   \n",
       "4712         -0.165578         -0.151789         -0.151372         -0.130441   \n",
       "4713         -0.207456         -0.205270         -0.167800         -0.141775   \n",
       "4714         -0.054248         -0.030412         -0.017290         -0.057763   \n",
       "4715         -0.156127         -0.211235         -0.163014         -0.118812   \n",
       "4716         -0.261848         -0.182337         -0.293268         -0.164469   \n",
       "4717         -0.221290         -0.170864         -0.098491         -0.150004   \n",
       "4718         -0.215841         -0.172014         -0.162727         -0.148903   \n",
       "4719         -0.144916         -0.149149         -0.149216         -0.134031   \n",
       "4720         -0.181920         -0.116458         -0.148547         -0.158607   \n",
       "4721         -0.148465         -0.180180         -0.119831         -0.121515   \n",
       "4722         -0.063428         -0.068324         -0.079974         -0.089506   \n",
       "4723         -0.167057         -0.126165         -0.143099         -0.185517   \n",
       "4724         -0.144127         -0.128513         -0.146529         -0.232164   \n",
       "4725         -0.152424         -0.132677         -0.129951         -0.142684   \n",
       "4726         -0.084071         -0.067437         -0.057563         -0.074639   \n",
       "4727         -0.161635         -0.159153         -0.204958         -0.143037   \n",
       "4728         -0.120242         -0.129960         -0.139428         -0.131272   \n",
       "4729               NaN               NaN               NaN               NaN   \n",
       "4730         -0.052218         -0.051022         -0.052372         -0.074581   \n",
       "4731         -0.126338         -0.151145         -0.146070         -0.160818   \n",
       "\n",
       "      negaposi_split_4  negaposi_split_5  negaposi_split_6  negaposi_split_7  \\\n",
       "0            -0.241814         -0.230414         -0.068126         -0.187654   \n",
       "1            -0.184833         -0.069419         -0.144469         -0.173366   \n",
       "2                  NaN               NaN               NaN               NaN   \n",
       "3            -0.181443         -0.170108         -0.146874         -0.193307   \n",
       "4            -0.082719         -0.179402         -0.157739         -0.172234   \n",
       "5            -0.154132         -0.147501         -0.180346         -0.164280   \n",
       "6            -0.153066         -0.132765         -0.117842         -0.156904   \n",
       "7            -0.087101         -0.104366         -0.048110         -0.095735   \n",
       "8            -0.144178         -0.125764         -0.119872         -0.121663   \n",
       "9            -0.134723         -0.145973         -0.171107         -0.120812   \n",
       "10           -0.229857         -0.128126         -0.125542         -0.129539   \n",
       "11           -0.136750         -0.098015         -0.151664         -0.137001   \n",
       "12           -0.242783         -0.071326         -0.181013         -0.166716   \n",
       "13           -0.186329         -0.185452         -0.165364         -0.167758   \n",
       "14           -0.140745         -0.144403         -0.180030         -0.207033   \n",
       "15           -0.170807         -0.214253         -0.162942         -0.153300   \n",
       "16           -0.091345         -0.059374         -0.081060         -0.085671   \n",
       "17           -0.127995         -0.108571         -0.143030         -0.173354   \n",
       "18           -0.163028         -0.127862         -0.164866         -0.166148   \n",
       "19           -0.161008         -0.123934         -0.216487         -0.128981   \n",
       "20           -0.163160         -0.224339         -0.058496         -0.145703   \n",
       "21           -0.137270         -0.140546         -0.131642         -0.111756   \n",
       "22           -0.175499         -0.148570         -0.105941         -0.169547   \n",
       "23           -0.128930         -0.131722         -0.114753         -0.162246   \n",
       "24           -0.289034         -0.158395         -0.129012         -0.158177   \n",
       "25           -0.187428         -0.165614         -0.106582         -0.111569   \n",
       "26           -0.237491         -0.251401         -0.230924         -0.246818   \n",
       "27           -0.139880         -0.158612         -0.155952         -0.139727   \n",
       "28           -0.107860         -0.104501         -0.082456         -0.159880   \n",
       "29           -0.088702         -0.096248         -0.096313         -0.085411   \n",
       "...                ...               ...               ...               ...   \n",
       "4702         -0.096248         -0.080136         -0.082838         -0.095074   \n",
       "4703         -0.151792         -0.103227         -0.153429         -0.103000   \n",
       "4704         -0.129344         -0.111234         -0.150500         -0.171991   \n",
       "4705         -0.156706         -0.136795         -0.133603         -0.123864   \n",
       "4706         -0.170077         -0.124311         -0.142302         -0.127974   \n",
       "4707         -0.127037         -0.122411         -0.126918         -0.146291   \n",
       "4708         -0.237600         -0.221831         -0.177502         -0.173041   \n",
       "4709               NaN               NaN               NaN               NaN   \n",
       "4710         -0.203361         -0.126936         -0.134112         -0.140574   \n",
       "4711         -0.161474         -0.131401         -0.198607         -0.164848   \n",
       "4712         -0.133698         -0.166450         -0.164515         -0.148081   \n",
       "4713         -0.187631         -0.177274         -0.175009         -0.175240   \n",
       "4714         -0.050354         -0.062937         -0.113614         -0.059191   \n",
       "4715         -0.154546         -0.139603         -0.161676         -0.262062   \n",
       "4716         -0.172912         -0.175728         -0.128874         -0.115546   \n",
       "4717         -0.135836         -0.236786         -0.215624         -0.163024   \n",
       "4718         -0.122497         -0.222249         -0.120452         -0.154253   \n",
       "4719         -0.157642         -0.159852         -0.149372         -0.184161   \n",
       "4720         -0.068560         -0.195990         -0.131663         -0.159923   \n",
       "4721         -0.124579         -0.147619         -0.156491         -0.154822   \n",
       "4722         -0.098025         -0.085413         -0.070186         -0.055048   \n",
       "4723         -0.141920         -0.156715         -0.176208         -0.162119   \n",
       "4724         -0.210389         -0.107859         -0.056922         -0.142439   \n",
       "4725         -0.153350         -0.138757         -0.155047         -0.111022   \n",
       "4726         -0.071882         -0.079999         -0.109161         -0.119563   \n",
       "4727         -0.170804         -0.156172         -0.207502         -0.182191   \n",
       "4728         -0.132119         -0.136361         -0.110012         -0.137535   \n",
       "4729               NaN               NaN               NaN               NaN   \n",
       "4730         -0.063387         -0.055917         -0.069329         -0.050425   \n",
       "4731         -0.116093         -0.143521         -0.148397         -0.132973   \n",
       "\n",
       "      negaposi_split_8  negaposi_split_9   count      mean       std  \\\n",
       "0            -0.146884         -0.111569    14.0 -0.152729  0.053904   \n",
       "1            -0.115937         -0.085715    20.0 -0.138693  0.054466   \n",
       "2                  NaN               NaN     8.0 -0.178221  0.108472   \n",
       "3            -0.158766         -0.204739   114.0 -0.181201  0.067120   \n",
       "4            -0.209102         -0.164302   139.0 -0.169167  0.092031   \n",
       "5            -0.151534         -0.153664   256.0 -0.160886  0.064332   \n",
       "6            -0.151693         -0.201058    39.0 -0.147916  0.050383   \n",
       "7            -0.159340         -0.108527    61.0 -0.110820  0.078236   \n",
       "8            -0.137912         -0.151600   190.0 -0.131190  0.065456   \n",
       "9            -0.148324         -0.086332   232.0 -0.140937  0.088358   \n",
       "10           -0.179630         -0.118513   116.0 -0.165060  0.095575   \n",
       "11           -0.149854         -0.130725    22.0 -0.125844  0.066257   \n",
       "12           -0.169445         -0.127867    14.0 -0.150682  0.070542   \n",
       "13           -0.159983         -0.153251  1542.0 -0.164876  0.097569   \n",
       "14           -0.164988         -0.205033    70.0 -0.158781  0.078643   \n",
       "15           -0.193539         -0.180861    86.0 -0.175878  0.072713   \n",
       "16           -0.066280         -0.058310   123.0 -0.071585  0.041681   \n",
       "17           -0.156123         -0.144980   156.0 -0.151004  0.079988   \n",
       "18           -0.155919         -0.175893   684.0 -0.157716  0.117248   \n",
       "19           -0.170237         -0.183629    25.0 -0.157586  0.055253   \n",
       "20           -0.090445         -0.122038    35.0 -0.159208  0.089172   \n",
       "21           -0.157792         -0.172346   134.0 -0.137740  0.055292   \n",
       "22           -0.175066         -0.167333    75.0 -0.142327  0.081791   \n",
       "23           -0.139520         -0.127093   196.0 -0.133564  0.069623   \n",
       "24           -0.186548         -0.141902    63.0 -0.151812  0.089345   \n",
       "25           -0.157823         -0.196584    13.0 -0.188787  0.077924   \n",
       "26           -0.225742         -0.203368    29.0 -0.225312  0.059647   \n",
       "27           -0.166809         -0.140100   244.0 -0.138963  0.070131   \n",
       "28           -0.135831         -0.197376   409.0 -0.133436  0.091780   \n",
       "29           -0.094288         -0.087477   160.0 -0.085095  0.039779   \n",
       "...                ...               ...     ...       ...       ...   \n",
       "4702         -0.079131         -0.070018    98.0 -0.079781  0.032987   \n",
       "4703         -0.161723         -0.161807    59.0 -0.147332  0.064966   \n",
       "4704         -0.144837         -0.121257   121.0 -0.139362  0.063697   \n",
       "4705         -0.126231         -0.151991   193.0 -0.135289  0.055741   \n",
       "4706         -0.101206         -0.106120    98.0 -0.138139  0.074304   \n",
       "4707         -0.114619         -0.136114   232.0 -0.126829  0.055062   \n",
       "4708         -0.167976         -0.238571    85.0 -0.203505  0.087052   \n",
       "4709               NaN               NaN     6.0 -0.135031  0.045189   \n",
       "4710         -0.109193         -0.086428    20.0 -0.148018  0.057453   \n",
       "4711         -0.136990         -0.125657    38.0 -0.142489  0.047514   \n",
       "4712         -0.133122         -0.165576   514.0 -0.150983  0.084808   \n",
       "4713         -0.176716         -0.189538   142.0 -0.180398  0.065842   \n",
       "4714         -0.050771         -0.068958    52.0 -0.057248  0.040235   \n",
       "4715         -0.179215         -0.165041    62.0 -0.168931  0.090626   \n",
       "4716         -0.082037         -0.200381    46.0 -0.180353  0.093764   \n",
       "4717         -0.097864         -0.174253    45.0 -0.174888  0.071249   \n",
       "4718         -0.112007         -0.301705    33.0 -0.184203  0.109379   \n",
       "4719         -0.146648         -0.133118   270.0 -0.150811  0.062526   \n",
       "4720          0.001004         -0.192125    18.0 -0.152520  0.052501   \n",
       "4721         -0.160716         -0.161211   199.0 -0.147177  0.070223   \n",
       "4722         -0.067396         -0.074446   142.0 -0.075465  0.037825   \n",
       "4723         -0.092583         -0.179953    24.0 -0.156543  0.046266   \n",
       "4724         -0.209709         -0.043173    10.0 -0.142182  0.063042   \n",
       "4725         -0.140244         -0.105691   396.0 -0.136218  0.085022   \n",
       "4726         -0.077010         -0.061073   130.0 -0.080240  0.088660   \n",
       "4727         -0.143094         -0.105638    41.0 -0.171518  0.078435   \n",
       "4728         -0.126243         -0.119633   293.0 -0.127915  0.063033   \n",
       "4729               NaN               NaN     5.0 -0.099566  0.087906   \n",
       "4730         -0.057307         -0.046225   165.0 -0.057358  0.048331   \n",
       "4731         -0.154395         -0.162108   239.0 -0.144025  0.060721   \n",
       "\n",
       "           min       max        ああ        あい  あいかわらず      あいさつ       あいだ  あいつ  \\\n",
       "0    -0.241814 -0.068126  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "1    -0.250899 -0.037045  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "2    -0.259274  0.067481  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "3    -0.333250  0.004930  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4    -0.418720  0.061273  0.015162  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "5    -0.345864  0.244552  0.000000  0.005527     0.0  0.000000  0.000000  0.0   \n",
       "6    -0.240097 -0.057175  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "7    -0.311974  0.039568  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "8    -0.311596  0.068172  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "9    -0.438225  0.193158  0.006842  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "10   -0.423257  0.073995  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "11   -0.195397  0.110409  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "12   -0.313393 -0.066492  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "13   -0.609971  0.499435  0.004382  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "14   -0.414533 -0.031413  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "15   -0.443683  0.015547  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "16   -0.204666  0.074664  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "17   -0.442094  0.098970  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "18   -0.852791  0.497453  0.002825  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "19   -0.277219 -0.019623  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "20   -0.395959 -0.016645  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "21   -0.352908  0.043999  0.000000  0.016610     0.0  0.000000  0.000000  0.0   \n",
       "22   -0.342087  0.194327  0.099095  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "23   -0.379930  0.123655  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "24   -0.498852  0.184373  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "25   -0.351025 -0.106582  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "26   -0.325741 -0.074291  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "27   -0.439416  0.102146  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "28   -0.490622  0.139780  0.003562  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "29   -0.227422  0.013461  0.009964  0.000000     0.0  0.000000  0.007310  0.0   \n",
       "...        ...       ...       ...       ...     ...       ...       ...  ...   \n",
       "4702 -0.186941 -0.022269  0.000000  0.014676     0.0  0.000000  0.027967  0.0   \n",
       "4703 -0.303460  0.007940  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4704 -0.286635  0.078633  0.000000  0.010742     0.0  0.000000  0.000000  0.0   \n",
       "4705 -0.292574  0.087972  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4706 -0.296549  0.094817  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4707 -0.246590  0.102430  0.000000  0.006273     0.0  0.000000  0.000000  0.0   \n",
       "4708 -0.460204  0.088223  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4709 -0.189186 -0.074113  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4710 -0.247001 -0.047546  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4711 -0.269744 -0.049292  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4712 -0.428837  0.118114  0.003007  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4713 -0.362528 -0.013263  0.000000  0.000000     0.0  0.019691  0.000000  0.0   \n",
       "4714 -0.196028  0.039355  0.014376  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4715 -0.369241  0.053870  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4716 -0.428633  0.231218  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4717 -0.345566 -0.052140  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4718 -0.401055  0.165973  0.049323  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4719 -0.352741  0.052689  0.000000  0.022468     0.0  0.000000  0.000000  0.0   \n",
       "4720 -0.223938  0.001004  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4721 -0.384161  0.196279  0.004947  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4722 -0.158405  0.018317  0.000000  0.008506     0.0  0.000000  0.016210  0.0   \n",
       "4723 -0.213069 -0.001866  0.020347  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4724 -0.232164 -0.043173  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4725 -0.468893  0.129150  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4726 -0.891896  0.036641  0.000000  0.000000     0.0  0.000000  0.014453  0.0   \n",
       "4727 -0.495515  0.002742  0.020893  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4728 -0.381321  0.030037  0.000000  0.005411     0.0  0.000000  0.000000  0.0   \n",
       "4729 -0.212082 -0.015499  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "4730 -0.183807  0.115533  0.000000  0.014465     0.0  0.000000  0.006891  0.0   \n",
       "4731 -0.387698  0.005213  0.000000  0.000000     0.0  0.000000  0.000000  0.0   \n",
       "\n",
       "          あいにく      あいまい        あう        あか       あかい       あかり       あがる  \\\n",
       "0     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3     0.000000  0.000000  0.000000  0.020863  0.000000  0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5     0.006294  0.000000  0.000000  0.000000  0.000000  0.006189  0.000000   \n",
       "6     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.041478   \n",
       "10    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15    0.000000  0.024003  0.000000  0.000000  0.000000  0.000000  0.013807   \n",
       "16    0.000000  0.000000  0.000000  0.021468  0.000000  0.000000  0.000000   \n",
       "17    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "18    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "20    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "21    0.000000  0.000000  0.000000  0.007346  0.000000  0.009300  0.000000   \n",
       "22    0.000000  0.000000  0.059197  0.000000  0.000000  0.000000  0.000000   \n",
       "23    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "24    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "25    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "26    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "28    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.008638   \n",
       "29    0.000000  0.000000  0.005952  0.000000  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4702  0.000000  0.000000  0.011385  0.025962  0.000000  0.000000  0.000000   \n",
       "4703  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4704  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4705  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4706  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4707  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4708  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4709  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4710  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4711  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4712  0.005274  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4713  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4714  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4715  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4716  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4717  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4718  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4719  0.000000  0.000000  0.004358  0.000000  0.000000  0.000000  0.000000   \n",
       "4720  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4721  0.008675  0.000000  0.000000  0.006738  0.000000  0.000000  0.000000   \n",
       "4722  0.000000  0.000000  0.019798  0.022572  0.000000  0.000000  0.000000   \n",
       "4723  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4724  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4725  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4726  0.000000  0.000000  0.000000  0.013417  0.000000  0.000000  0.011943   \n",
       "4727  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4728  0.000000  0.000000  0.000000  0.000000  0.000000  0.018179  0.000000   \n",
       "4729  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4730  0.000000  0.000000  0.044889  0.006397  0.009878  0.000000  0.005694   \n",
       "4731  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "            あき  あきらか      あきらめ     あきらめる       あきる      あきれる        あく  あくどい  \\\n",
       "0     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "1     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "2     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "3     0.000000   0.0  0.000000  0.000000  0.023662  0.000000  0.000000   0.0   \n",
       "4     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "5     0.005599   0.0  0.000000  0.000000  0.000000  0.000000  0.004248   0.0   \n",
       "6     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "7     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "8     0.000000   0.0  0.010851  0.000000  0.000000  0.000000  0.012049   0.0   \n",
       "9     0.000000   0.0  0.014584  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "10    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "11    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "12    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "13    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "14    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "15    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "16    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.009326   0.0   \n",
       "17    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "18    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "19    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.026752   0.0   \n",
       "20    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "21    0.008413   0.0  0.000000  0.000000  0.016663  0.000000  0.000000   0.0   \n",
       "22    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "23    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "24    0.018419   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "25    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "26    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "27    0.000000   0.0  0.000000  0.000000  0.006222  0.000000  0.009532   0.0   \n",
       "28    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "29    0.007772   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "...        ...   ...       ...       ...       ...       ...       ...   ...   \n",
       "4702  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.011278   0.0   \n",
       "4703  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4704  0.021763   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4705  0.006976   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4706  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4707  0.000000   0.0  0.000000  0.000000  0.006293  0.000000  0.000000   0.0   \n",
       "4708  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4709  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4710  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4711  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4712  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4713  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4714  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4715  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4716  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4717  0.000000   0.0  0.000000  0.026987  0.000000  0.000000  0.020238   0.0   \n",
       "4718  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4719  0.005690   0.0  0.000000  0.005756  0.011270  0.000000  0.012950   0.0   \n",
       "4720  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4721  0.000000   0.0  0.000000  0.007807  0.000000  0.000000  0.011709   0.0   \n",
       "4722  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4723  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4724  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4725  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4726  0.000000   0.0  0.000000  0.000000  0.015217  0.000000  0.000000   0.0   \n",
       "4727  0.000000   0.0  0.000000  0.032973  0.000000  0.000000  0.000000   0.0   \n",
       "4728  0.000000   0.0  0.000000  0.000000  0.005429  0.000000  0.008317   0.0   \n",
       "4729  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
       "4730  0.000000   0.0  0.000000  0.007412  0.000000  0.009306  0.005558   0.0   \n",
       "4731  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.005918   0.0   \n",
       "\n",
       "      あくび      あくまで  あくまでも       あぐら       あける       あげく       あげる      あこがれ  \\\n",
       "0     0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1     0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2     0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3     0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4     0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5     0.0  0.000000    0.0  0.000000  0.004124  0.000000  0.009004  0.000000   \n",
       "6     0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7     0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8     0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.012771  0.000000   \n",
       "9     0.0  0.000000    0.0  0.000000  0.007861  0.011414  0.017164  0.000000   \n",
       "10    0.0  0.015582    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.073014  0.000000   \n",
       "13    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.006108  0.000000   \n",
       "14    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "16    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.019769  0.000000   \n",
       "17    0.0  0.000000    0.0  0.000000  0.000000  0.011729  0.000000  0.000000   \n",
       "18    0.0  0.000000    0.0  0.000000  0.009736  0.000000  0.000000  0.000000   \n",
       "19    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "20    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "21    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.004510  0.000000   \n",
       "22    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "23    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.003994  0.000000   \n",
       "24    0.0  0.000000    0.0  0.000000  0.013566  0.000000  0.000000  0.000000   \n",
       "25    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "26    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "28    0.0  0.000000    0.0  0.000000  0.004093  0.000000  0.002979  0.000000   \n",
       "29    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.016665  0.000000   \n",
       "...   ...       ...    ...       ...       ...       ...       ...       ...   \n",
       "4702  0.0  0.000000    0.0  0.000000  0.000000  0.015899  0.000000  0.000000   \n",
       "4703  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4704  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4705  0.0  0.000000    0.0  0.000000  0.005138  0.000000  0.003740  0.000000   \n",
       "4706  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4707  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.017831   \n",
       "4708  0.0  0.000000    0.0  0.000000  0.000000  0.019505  0.009777  0.000000   \n",
       "4709  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4710  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4711  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4712  0.0  0.000000    0.0  0.000000  0.010366  0.000000  0.000000  0.000000   \n",
       "4713  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4714  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.012022  0.000000   \n",
       "4715  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4716  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4717  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4718  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4719  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.009151  0.000000   \n",
       "4720  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4721  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.008274  0.000000   \n",
       "4722  0.0  0.010110    0.0  0.000000  0.000000  0.009215  0.000000  0.000000   \n",
       "4723  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4724  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4725  0.0  0.000000    0.0  0.009147  0.000000  0.000000  0.008001  0.000000   \n",
       "4726  0.0  0.000000    0.0  0.000000  0.056584  0.000000  0.016474  0.000000   \n",
       "4727  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.017472  0.000000   \n",
       "4728  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.002939  0.000000   \n",
       "4729  0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4730  0.0  0.008596    0.0  0.000000  0.000000  0.000000  0.007855  0.000000   \n",
       "4731  0.0  0.000000    0.0  0.000000  0.005745  0.000000  0.012546  0.000000   \n",
       "\n",
       "      あこがれる        あご       あさい     あさましい        あざ      あざやか        あし  \\\n",
       "0       0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1       0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2       0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3       0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4       0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5       0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6       0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7       0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8       0.0  0.008248  0.000000  0.000000  0.009550  0.000000  0.000000   \n",
       "9       0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.009201   \n",
       "10      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "16      0.0  0.000000  0.000000  0.000000  0.029565  0.000000  0.000000   \n",
       "17      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "18      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "20      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "21      0.0  0.000000  0.009991  0.000000  0.010117  0.000000  0.000000   \n",
       "22      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "23      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "24      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "25      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "26      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27      0.0  0.006525  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "28      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "29      0.0  0.000000  0.000000  0.010183  0.000000  0.000000  0.020100   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "4702    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.025632   \n",
       "4703    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4704    0.0  0.000000  0.000000  0.000000  0.000000  0.027937  0.009381   \n",
       "4705    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.006014   \n",
       "4706    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4707    0.0  0.006600  0.007546  0.000000  0.000000  0.000000  0.005478   \n",
       "4708    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4709    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4710    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4711    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4712    0.0  0.000000  0.000000  0.000000  0.005641  0.000000  0.000000   \n",
       "4713    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4714    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.019333   \n",
       "4715    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4716    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4717    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4718    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4719    0.0  0.011819  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4720    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4721    0.0  0.000000  0.009164  0.000000  0.000000  0.000000  0.013305   \n",
       "4722    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.022285   \n",
       "4723    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4724    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4725    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4726    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4727    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4728    0.0  0.017079  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4729    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4730    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.037897   \n",
       "4731    0.0  0.008102  0.000000  0.000000  0.112572  0.000000  0.006725   \n",
       "\n",
       "           あした        あす       あすこ  あずかる  あずける       あせる  あそこ       あそぶ  あたかも  \\\n",
       "0     0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "1     0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "2     0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "3     0.000000  0.000000  0.024770   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4     0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "5     0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "6     0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "7     0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "8     0.000000  0.000000  0.008233   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "9     0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "10    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "11    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "12    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "13    0.002339  0.000000  0.002362   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "14    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "15    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "16    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "17    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "18    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "19    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "20    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "21    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "22    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "23    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "24    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "25    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "26    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "27    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "28    0.000000  0.000000  0.005761   0.0   0.0  0.000000  0.0  0.007295   0.0   \n",
       "29    0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "...        ...       ...       ...   ...   ...       ...  ...       ...   ...   \n",
       "4702  0.106830  0.000000  0.000000   0.0   0.0  0.016293  0.0  0.000000   0.0   \n",
       "4703  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4704  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4705  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4706  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4707  0.000000  0.000000  0.000000   0.0   0.0  0.006965  0.0  0.000000   0.0   \n",
       "4708  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4709  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4710  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4711  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4712  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4713  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4714  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4715  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4716  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4717  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4718  0.078990  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4719  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4720  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4721  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4722  0.000000  0.010177  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4723  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4724  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4725  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4726  0.015774  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.020172   0.0   \n",
       "4727  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4728  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4729  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "4730  0.000000  0.000000  0.000000   0.0   0.0  0.008030  0.0  0.000000   0.0   \n",
       "4731  0.000000  0.000000  0.000000   0.0   0.0  0.000000  0.0  0.000000   0.0   \n",
       "\n",
       "           あたし  あたたかい      あたら       あたり     あたりまえ      あたり前       あたる  \\\n",
       "0     0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "1     0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "2     0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "3     0.000000    0.0  0.00000  0.013611  0.000000  0.000000  0.000000   \n",
       "4     0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "5     0.000000    0.0  0.00000  0.003190  0.000000  0.000000  0.000000   \n",
       "6     0.000000    0.0  0.00000  0.014155  0.000000  0.000000  0.000000   \n",
       "7     0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "8     0.000000    0.0  0.00000  0.004524  0.000000  0.000000  0.000000   \n",
       "9     0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "10    0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "11    0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "12    0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "13    0.000000    0.0  0.00000  0.019471  0.000000  0.000000  0.002397   \n",
       "14    0.000000    0.0  0.00000  0.032287  0.000000  0.000000  0.000000   \n",
       "15    0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "16    0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "17    0.000000    0.0  0.00000  0.000000  0.013721  0.015387  0.000000   \n",
       "18    0.094512    0.0  0.00000  0.005020  0.000000  0.000000  0.000000   \n",
       "19    0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "20    0.000000    0.0  0.00000  0.046228  0.000000  0.000000  0.000000   \n",
       "21    0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "22    0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "23    0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "24    0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "25    0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "26    0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "27    0.000000    0.0  0.00000  0.007158  0.000000  0.000000  0.000000   \n",
       "28    0.000000    0.0  0.00000  0.009497  0.000000  0.000000  0.005847   \n",
       "29    0.000000    0.0  0.00000  0.013282  0.000000  0.000000  0.000000   \n",
       "...        ...    ...      ...       ...       ...       ...       ...   \n",
       "4702  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.015640   \n",
       "4703  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4704  0.000000    0.0  0.00000  0.006199  0.000000  0.000000  0.000000   \n",
       "4705  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4706  0.000000    0.0  0.00000  0.014279  0.000000  0.000000  0.000000   \n",
       "4707  0.000000    0.0  0.00000  0.007240  0.000000  0.000000  0.000000   \n",
       "4708  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4709  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4710  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4711  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4712  0.000000    0.0  0.00000  0.002673  0.000000  0.000000  0.000000   \n",
       "4713  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4714  0.000000    0.0  0.00000  0.012775  0.000000  0.000000  0.000000   \n",
       "4715  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4716  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4717  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4718  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4719  0.000000    0.0  0.00000  0.006483  0.000000  0.000000  0.000000   \n",
       "4720  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4721  0.000000    0.0  0.00000  0.013188  0.000000  0.000000  0.000000   \n",
       "4722  0.000000    0.0  0.00000  0.014726  0.000000  0.000000  0.000000   \n",
       "4723  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4724  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4725  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4726  0.000000    0.0  0.01998  0.017507  0.000000  0.000000  0.048498   \n",
       "4727  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4728  0.000000    0.0  0.00000  0.006245  0.000000  0.000000  0.000000   \n",
       "4729  0.000000    0.0  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4730  0.000000    0.0  0.00000  0.008347  0.000000  0.000000  0.000000   \n",
       "4731  0.000000    0.0  0.00000  0.013332  0.000000  0.000000  0.000000   \n",
       "\n",
       "           あだな  あちこち       あちら        あっ       あっけ      あっさり       あっし  \\\n",
       "0     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.092659   \n",
       "6     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9     0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10    0.000000   0.0  0.011442  0.000000  0.000000  0.000000  0.000000   \n",
       "11    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13    0.000000   0.0  0.007853  0.000000  0.000000  0.000000  0.000000   \n",
       "14    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "16    0.000000   0.0  0.021183  0.000000  0.015984  0.000000  0.000000   \n",
       "17    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "18    0.000000   0.0  0.007593  0.000000  0.000000  0.000000  0.000000   \n",
       "19    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "20    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "21    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "22    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "23    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "24    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "25    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "26    0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27    0.008471   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "28    0.000000   0.0  0.004788  0.000000  0.000000  0.000000  0.000000   \n",
       "29    0.000000   0.0  0.006696  0.000000  0.000000  0.000000  0.000000   \n",
       "...        ...   ...       ...       ...       ...       ...       ...   \n",
       "4702  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4703  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4704  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4705  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4706  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4707  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4708  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4709  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4710  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4711  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4712  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4713  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4714  0.000000   0.0  0.019322  0.028998  0.000000  0.000000  0.000000   \n",
       "4715  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4716  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4717  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4718  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4719  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.102015   \n",
       "4720  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4721  0.000000   0.0  0.000000  0.009978  0.030103  0.000000  0.010643   \n",
       "4722  0.000000   0.0  0.014848  0.000000  0.000000  0.000000  0.000000   \n",
       "4723  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4724  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4725  0.000000   0.0  0.006429  0.000000  0.000000  0.000000  0.000000   \n",
       "4726  0.000000   0.0  0.013239  0.000000  0.000000  0.000000  0.000000   \n",
       "4727  0.000000   0.0  0.000000  0.000000  0.000000  0.045612  0.000000   \n",
       "4728  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4729  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4730  0.000000   0.0  0.006312  0.000000  0.000000  0.000000  0.000000   \n",
       "4731  0.000000   0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "           あっち       あつい  あつかう      あつまる  あつめる   あて       あてる        あと  \\\n",
       "0     0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "1     0.000000  0.082680   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "2     0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "3     0.000000  0.239196   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4     0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "5     0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.003016   \n",
       "6     0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.013384   \n",
       "7     0.000000  0.017031   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "8     0.000000  0.037413   0.0  0.000000   0.0  0.0  0.000000  0.017110   \n",
       "9     0.000000  0.006285   0.0  0.000000   0.0  0.0  0.000000  0.011498   \n",
       "10    0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "11    0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "12    0.000000  0.080209   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "13    0.000000  0.000000   0.0  0.000000   0.0  0.0  0.006691  0.000000   \n",
       "14    0.000000  0.100130   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "15    0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.009568   \n",
       "16    0.000000  0.007239   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "17    0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "18    0.000000  0.000000   0.0  0.000000   0.0  0.0  0.004313  0.004747   \n",
       "19    0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "20    0.000000  0.286727   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "21    0.000000  0.039634   0.0  0.000000   0.0  0.0  0.000000  0.009063   \n",
       "22    0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "23    0.000000  0.070194   0.0  0.000000   0.0  0.0  0.000000  0.012038   \n",
       "24    0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "25    0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "26    0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "27    0.000000  0.051795   0.0  0.000000   0.0  0.0  0.000000  0.013536   \n",
       "28    0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.014966   \n",
       "29    0.000000  0.022884   0.0  0.000000   0.0  0.0  0.000000  0.008373   \n",
       "...        ...       ...   ...       ...   ...  ...       ...       ...   \n",
       "4702  0.015078  0.008755   0.0  0.000000   0.0  0.0  0.000000  0.024023   \n",
       "4703  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4704  0.000000  0.134571   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4705  0.000000  0.061621   0.0  0.000000   0.0  0.0  0.000000  0.003758   \n",
       "4706  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.013501   \n",
       "4707  0.000000  0.026195   0.0  0.000000   0.0  0.0  0.000000  0.003423   \n",
       "4708  0.000000  0.000000   0.0  0.025036   0.0  0.0  0.000000  0.000000   \n",
       "4709  0.000000  0.064334   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4710  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4711  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4712  0.000000  0.008288   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4713  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.016786   \n",
       "4714  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.060398   \n",
       "4715  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4716  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4717  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4718  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4719  0.005771  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.003065   \n",
       "4720  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4721  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.016626   \n",
       "4722  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.004641   \n",
       "4723  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4724  0.000000  0.049861   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4725  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.008039   \n",
       "4726  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4727  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4728  0.000000  0.032281   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4729  0.000000  0.076473   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4730  0.000000  0.000000   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "4731  0.000000  0.087285   0.0  0.000000   0.0  0.0  0.000000  0.000000   \n",
       "\n",
       "            あな      あながち       あなた        あに       あにい        あの  あはれ  あばれる  \\\n",
       "0     0.000000  0.000000  0.000000  0.000000  0.000000  0.033818  0.0   0.0   \n",
       "1     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "2     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "3     0.000000  0.000000  0.000000  0.000000  0.000000  0.020631  0.0   0.0   \n",
       "4     0.000000  0.000000  0.015764  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "5     0.000000  0.000000  0.000000  0.013244  0.007668  0.014503  0.0   0.0   \n",
       "6     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "7     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "8     0.000000  0.000000  0.000000  0.009392  0.000000  0.034286  0.0   0.0   \n",
       "9     0.000000  0.000000  0.021340  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "10    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "11    0.000000  0.058499  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "12    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "13    0.000000  0.000000  0.037966  0.000000  0.000000  0.005903  0.0   0.0   \n",
       "14    0.000000  0.000000  0.075547  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "15    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "16    0.000000  0.000000  0.008193  0.000000  0.000000  0.010614  0.0   0.0   \n",
       "17    0.000000  0.000000  0.000000  0.000000  0.000000  0.009470  0.0   0.0   \n",
       "18    0.000000  0.000000  0.000000  0.000000  0.000000  0.019024  0.0   0.0   \n",
       "19    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "20    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "21    0.000000  0.000000  0.000000  0.009950  0.000000  0.036322  0.0   0.0   \n",
       "22    0.000000  0.000000  0.000000  0.000000  0.000000  0.011124  0.0   0.0   \n",
       "23    0.000000  0.000000  0.000000  0.000000  0.000000  0.012866  0.0   0.0   \n",
       "24    0.000000  0.000000  0.012276  0.000000  0.000000  0.031809  0.0   0.0   \n",
       "25    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "26    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "27    0.000000  0.000000  0.000000  0.007430  0.008604  0.016274  0.0   0.0   \n",
       "28    0.000000  0.000000  0.081480  0.000000  0.000000  0.021593  0.0   0.0   \n",
       "29    0.000000  0.000000  0.005180  0.000000  0.000000  0.016777  0.0   0.0   \n",
       "...        ...       ...       ...       ...       ...       ...  ...   ...   \n",
       "4702  0.017902  0.000000  0.000000  0.000000  0.000000  0.019255  0.0   0.0   \n",
       "4703  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4704  0.000000  0.000000  0.007252  0.000000  0.000000  0.009396  0.0   0.0   \n",
       "4705  0.000000  0.000000  0.000000  0.000000  0.000000  0.075295  0.0   0.0   \n",
       "4706  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4707  0.000000  0.000000  0.000000  0.007516  0.008703  0.038409  0.0   0.0   \n",
       "4708  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4709  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4710  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4711  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4712  0.000000  0.000000  0.000000  0.000000  0.000000  0.010127  0.0   0.0   \n",
       "4713  0.000000  0.000000  0.000000  0.000000  0.000000  0.006727  0.0   0.0   \n",
       "4714  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4715  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4716  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4717  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4718  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4719  0.000000  0.000000  0.000000  0.020189  0.000000  0.017196  0.0   0.0   \n",
       "4720  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4721  0.009293  0.000000  0.000000  0.000000  0.000000  0.006663  0.0   0.0   \n",
       "4722  0.000000  0.000000  0.017228  0.000000  0.000000  0.018601  0.0   0.0   \n",
       "4723  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4724  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4725  0.000000  0.000000  0.029840  0.000000  0.000000  0.019331  0.0   0.0   \n",
       "4726  0.000000  0.000000  0.000000  0.399806  0.000000  0.000000  0.0   0.0   \n",
       "4727  0.000000  0.000000  0.000000  0.000000  0.000000  0.014072  0.0   0.0   \n",
       "4728  0.000000  0.000000  0.000000  0.000000  0.007507  0.018933  0.0   0.0   \n",
       "4729  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4730  0.000000  0.000000  0.083009  0.000000  0.000000  0.000000  0.0   0.0   \n",
       "4731  0.000000  0.000000  0.000000  0.000000  0.000000  0.023576  0.0   0.0   \n",
       "\n",
       "           あびる       あふる      あふれる      あぶない       あぶる  あべこべ        あま  \\\n",
       "0     0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "1     0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "2     0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "3     0.000000  0.000000  0.000000  0.000000  0.024883   0.0  0.000000   \n",
       "4     0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "5     0.000000  0.006791  0.000000  0.000000  0.005831   0.0  0.000000   \n",
       "6     0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "7     0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "8     0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.008989   \n",
       "9     0.058071  0.000000  0.011685  0.000000  0.000000   0.0  0.000000   \n",
       "10    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "11    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "12    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "13    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "14    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "15    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "16    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "17    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "18    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "19    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "20    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "21    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "22    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "23    0.000000  0.000000  0.000000  0.000000  0.007758   0.0  0.000000   \n",
       "24    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "25    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "26    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "27    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "28    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "29    0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "...        ...       ...       ...       ...       ...   ...       ...   \n",
       "4702  0.000000  0.000000  0.000000  0.000000  0.015482   0.0  0.000000   \n",
       "4703  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4704  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4705  0.000000  0.000000  0.000000  0.008101  0.000000   0.0  0.000000   \n",
       "4706  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4707  0.000000  0.007708  0.000000  0.000000  0.006618   0.0  0.007193   \n",
       "4708  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4709  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4710  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4711  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4712  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4713  0.000000  0.000000  0.017060  0.000000  0.000000   0.0  0.000000   \n",
       "4714  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4715  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4716  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4717  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4718  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4719  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.006441   \n",
       "4720  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4721  0.000000  0.009361  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4722  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4723  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4724  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4725  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4726  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.017393   \n",
       "4727  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4728  0.000000  0.000000  0.000000  0.000000  0.005709   0.0  0.006205   \n",
       "4729  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4730  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "4731  0.000000  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "\n",
       "           あまり      あまりに  あまる    ...     名詞特殊助動詞語幹  名詞非自立一般  名詞非自立副詞可能  \\\n",
       "0     0.000000  0.000000  0.0    ...             0       15          1   \n",
       "1     0.000000  0.000000  0.0    ...             0       17         14   \n",
       "2     0.000000  0.000000  0.0    ...             0        5          2   \n",
       "3     0.013518  0.000000  0.0    ...             0       22         35   \n",
       "4     0.000000  0.000000  0.0    ...             0       35         45   \n",
       "5     0.006335  0.000000  0.0    ...             4      257         90   \n",
       "6     0.000000  0.000000  0.0    ...             3       55         15   \n",
       "7     0.016363  0.000000  0.0    ...             0       63         11   \n",
       "8     0.008986  0.000000  0.0    ...             0      189         51   \n",
       "9     0.006039  0.000000  0.0    ...             4      180         55   \n",
       "10    0.000000  0.000000  0.0    ...             0      111         23   \n",
       "11    0.000000  0.000000  0.0    ...             0       30         10   \n",
       "12    0.000000  0.000000  0.0    ...             0       18          3   \n",
       "13    0.000000  0.000000  0.0    ...             3      495        235   \n",
       "14    0.000000  0.000000  0.0    ...             0       36         11   \n",
       "15    0.010051  0.014942  0.0    ...             0       49         22   \n",
       "16    0.013910  0.000000  0.0    ...             1       42         17   \n",
       "17    0.000000  0.000000  0.0    ...             0      167         41   \n",
       "18    0.004986  0.000000  0.0    ...             0      323         70   \n",
       "19    0.000000  0.000000  0.0    ...             0       35          6   \n",
       "20    0.000000  0.000000  0.0    ...             0       17         13   \n",
       "21    0.009520  0.000000  0.0    ...             0      193         35   \n",
       "22    0.014577  0.000000  0.0    ...             0       32         10   \n",
       "23    0.004215  0.000000  0.0    ...             0      218         34   \n",
       "24    0.000000  0.000000  0.0    ...             1       41         11   \n",
       "25    0.000000  0.000000  0.0    ...             0        7          6   \n",
       "26    0.000000  0.000000  0.0    ...             0       19         12   \n",
       "27    0.021327  0.000000  0.0    ...             0      237         55   \n",
       "28    0.012577  0.000000  0.0    ...             7      207         30   \n",
       "29    0.004397  0.000000  0.0    ...             1      119         44   \n",
       "...        ...       ...  ...    ...           ...      ...        ...   \n",
       "4702  0.000000  0.012505  0.0    ...             3       80         32   \n",
       "4703  0.000000  0.000000  0.0    ...             0       59         13   \n",
       "4704  0.000000  0.000000  0.0    ...             0       40         40   \n",
       "4705  0.007894  0.000000  0.0    ...             1      236         68   \n",
       "4706  0.014182  0.000000  0.0    ...             1       79         18   \n",
       "4707  0.003595  0.000000  0.0    ...             1      218         51   \n",
       "4708  0.000000  0.000000  0.0    ...             0       49         20   \n",
       "4709  0.000000  0.000000  0.0    ...             0        6          6   \n",
       "4710  0.000000  0.000000  0.0    ...             0       14          6   \n",
       "4711  0.000000  0.000000  0.0    ...             1       51         13   \n",
       "4712  0.000000  0.000000  0.0    ...             0      270        112   \n",
       "4713  0.026449  0.026214  0.0    ...             0       95         40   \n",
       "4714  0.000000  0.000000  0.0    ...             1       26         11   \n",
       "4715  0.000000  0.000000  0.0    ...             0       25         19   \n",
       "4716  0.000000  0.000000  0.0    ...             0       25         13   \n",
       "4717  0.000000  0.000000  0.0    ...             1       29         19   \n",
       "4718  0.000000  0.000000  0.0    ...             0        4          3   \n",
       "4719  0.003219  0.000000  0.0    ...             5      288         71   \n",
       "4720  0.000000  0.000000  0.0    ...             0        6         12   \n",
       "4721  0.026197  0.006491  0.0    ...             0      165         59   \n",
       "4722  0.000000  0.007248  0.0    ...             9      106         41   \n",
       "4723  0.000000  0.000000  0.0    ...             0       31          6   \n",
       "4724  0.000000  0.000000  0.0    ...             0       17          5   \n",
       "4725  0.004222  0.000000  0.0    ...             0      193         62   \n",
       "4726  0.017387  0.000000  0.0    ...             5       41         25   \n",
       "4727  0.036881  0.000000  0.0    ...             3       39         23   \n",
       "4728  0.012406  0.004611  0.0    ...             1      205         69   \n",
       "4729  0.000000  0.109231  0.0    ...             0        4          1   \n",
       "4730  0.020726  0.000000  0.0    ...             0      142         24   \n",
       "4731  0.004414  0.000000  0.0    ...             1      230         63   \n",
       "\n",
       "      名詞非自立助動詞語幹  名詞非自立形容動詞語幹  形容詞接尾*  形容詞自立*  形容詞非自立*  感動詞**  接続詞**  \\\n",
       "0              1            0       1       6        1      0      3   \n",
       "1              4            1       0      17        0      2      7   \n",
       "2              1            0       0       4        0      0      1   \n",
       "3              7            0       0      66        1      9     18   \n",
       "4             16            0       0      63        2      3     45   \n",
       "5             33            1       1     123        6     27     48   \n",
       "6              6            0       0      46        1      0      8   \n",
       "7              2            0       0      14        0      5     14   \n",
       "8             34            0       4      99        0     21     29   \n",
       "9             29            0       1      96        3      8     32   \n",
       "10             2            0       0      64        2      0     26   \n",
       "11             2            0       0      14        0      0      6   \n",
       "12             1            0       0      12        1      1      6   \n",
       "13           141            3       5     546       15     48    211   \n",
       "14             9            0       0      25        1      3     10   \n",
       "15            16            0       0      55        5      5     43   \n",
       "16            13            0       1      28        1     45     29   \n",
       "17            44            0       0      85        4      0     69   \n",
       "18            44            7       3     157       10     20    137   \n",
       "19            10            0       0      33        0      0      6   \n",
       "20             3            0       0      23        1      1      5   \n",
       "21            27            0       5     122        1     22     38   \n",
       "22            15            0       0      54        3      6     20   \n",
       "23            20            0       2     109        1     20     23   \n",
       "24            10            0       1      39        0      6      8   \n",
       "25             2            0       0       6        0      0      3   \n",
       "26             1            1       0      10        0      0     11   \n",
       "27            35            2       0     141        3     38     48   \n",
       "28            32            2       0      88        3     17     64   \n",
       "29             6            0       0      73        0     53     35   \n",
       "...          ...          ...     ...     ...      ...    ...    ...   \n",
       "4702           9            2       0      65        1     56     22   \n",
       "4703           2            0       0      12        0      0     19   \n",
       "4704          27            0       0     102        0      8     46   \n",
       "4705          38            1       7     199        4     28     32   \n",
       "4706           9            1       0      43        4      8     26   \n",
       "4707          42            0       0     114        3     53     38   \n",
       "4708           3            0       0      31        0      1     17   \n",
       "4709           2            0       1       6        2      0      0   \n",
       "4710           4            0       0       9        2      0      5   \n",
       "4711          16            0       0      22        0      1     33   \n",
       "4712          38            1       3     152        1     33    163   \n",
       "4713          36            0       4      69        2      4     34   \n",
       "4714           6            0       0      24        0     19     10   \n",
       "4715           7            0       0      18        0      0     15   \n",
       "4716           9            0       0      37        0      0      9   \n",
       "4717           4            0       0      22        2      0     13   \n",
       "4718           3            0       0       6        0      1      1   \n",
       "4719          33            1       1     161        5     53     59   \n",
       "4720           3            0       0      13        1      1      3   \n",
       "4721          31            0       1     118        2     48     30   \n",
       "4722          11            1       0      88        7     55     36   \n",
       "4723           3            0       0      10        1      4     12   \n",
       "4724           1            0       0       9        0      0      4   \n",
       "4725          31            0       5     131        3     10     89   \n",
       "4726           3            0       0      43        1     30     19   \n",
       "4727           5            0       0      40        2      0      6   \n",
       "4728          49            0       1     131        2     60     37   \n",
       "4729           0            0       0       2        1      1      4   \n",
       "4730          21            0       0     122        4     78     36   \n",
       "4731          29            0       1     155        3     33     33   \n",
       "\n",
       "      接頭詞動詞接続*  接頭詞名詞接続*  接頭詞形容詞接続*  接頭詞数接続*  記号アルファベット*  記号一般*  記号句点*  \\\n",
       "0            0         0          0        0           0     12     14   \n",
       "1            0        11          0        0           0      7     20   \n",
       "2            0         1          0        0           0      8      8   \n",
       "3            2        19          0        2           0     92    114   \n",
       "4            0         9          0        0           2     69    139   \n",
       "5            2       101          0        2           0    540    256   \n",
       "6            0         3          0        0           0     20     39   \n",
       "7            0         7          1        0           0     65     61   \n",
       "8            1        78          0        8           0    397    190   \n",
       "9            0        39          0        6           4    123    232   \n",
       "10           0         2          0        0           0     73    116   \n",
       "11           0         2          0        0           0     17     22   \n",
       "12           0         4          0        0           0     12     14   \n",
       "13           1        85          0       13           0   1029   1542   \n",
       "14           0        17          0        1           4    103     70   \n",
       "15           0         5          0        0           0     30     86   \n",
       "16           0        11          0        1           0     72    123   \n",
       "17           0        10          0        0           0     81    156   \n",
       "18          10        25          1        2           4    431    684   \n",
       "19           0         0          0        1           0     14     25   \n",
       "20           0        13          0        0           0     32     35   \n",
       "21           0       130          0        2           0    386    134   \n",
       "22           0         7          0        1           2    401     75   \n",
       "23           0        68          2        0           0    350    196   \n",
       "24           0         5          0        1           0     69     63   \n",
       "25           0         1          0        6           0     15     13   \n",
       "26           0         8          0        1           1     23     29   \n",
       "27           1       120          0        5           0    431    244   \n",
       "28           1        54          0        1           0    243    409   \n",
       "29           0        22          0        4           0    114    160   \n",
       "...        ...       ...        ...      ...         ...    ...    ...   \n",
       "4702         0        33          1        1           0     47     98   \n",
       "4703         1         5          0        3           0     41     59   \n",
       "4704         3        17          0        1           4     73    121   \n",
       "4705         4       133          0        3           2    386    193   \n",
       "4706         0        17          0        1           0     85     98   \n",
       "4707         2       117          0        4           0    513    232   \n",
       "4708         1         8          0        1           0     86     85   \n",
       "4709         0         2          0        0           0     10      6   \n",
       "4710         0         3          0        0           0     28     20   \n",
       "4711         0         2          0        1           3     21     38   \n",
       "4712         2        62          0       15           0    194    514   \n",
       "4713         1        15          0        1           1     72    142   \n",
       "4714         0        18          0        0           0     34     52   \n",
       "4715         0         3          0        6           8     16     62   \n",
       "4716         0         8          0        0           0     21     46   \n",
       "4717         1         5          0        1           0     22     45   \n",
       "4718         0         2          0        0           0     13     33   \n",
       "4719         0       209          0        1           0    632    270   \n",
       "4720         0         2          0        0           0     11     18   \n",
       "4721         3        97          0        2           0    379    199   \n",
       "4722         0        21          0        0           0     88    142   \n",
       "4723         0         6          0        0           0     18     24   \n",
       "4724         0         2          0        0           0     10     10   \n",
       "4725         0        35          0        1           0    222    396   \n",
       "4726         0        32          0        0           0     69    130   \n",
       "4727         0        26          0        3           0     33     41   \n",
       "4728         2       285          0       11           0    631    293   \n",
       "4729         0         1          0        0           0      6      5   \n",
       "4730         1        55          0        0           0    112    165   \n",
       "4731         3        88          0        1           0    441    239   \n",
       "\n",
       "      記号括弧閉*  記号括弧開*  記号空白*  記号読点*  連体詞**  BOS/EOS*  その他間投  フィラー*  副詞一般  \\\n",
       "0          4       4      6     24      6         2      0      4     4   \n",
       "1          7       7      3     87     18         2      0      1    12   \n",
       "2          1       1      4     15      4         2      0      0     4   \n",
       "3        109      87     41    170     31         2      0      4    34   \n",
       "4         27      20     31    133     39         2      0      0    40   \n",
       "5        501     456    179    732     87         2      4      8   123   \n",
       "6          4       2     15     86     25         2      0      0    18   \n",
       "7         11      12    167     83     20         2      0      4    23   \n",
       "8        385     362    125    429     75         2      1     10    68   \n",
       "9         40      40     99    309     57         2      0      0    68   \n",
       "10        12       2     52    202     48         2      0      0    79   \n",
       "11         5       3      8     48     16         2      0      0    16   \n",
       "12         2       2      2     47     17         2      0      3    11   \n",
       "13       478     389    503   1356    269         2      0     16   523   \n",
       "14        39      39     70    155     22         2      0      2    14   \n",
       "15        27      26     21     64     36         2      0      2    49   \n",
       "16       448     448     31    268     35         2      3      1    32   \n",
       "17        10      10     73    372     70         2      0      0    73   \n",
       "18        88       8    132    830    140         2      0      2   229   \n",
       "19         6       3      9     49     23         2      0      0    10   \n",
       "20         8       8     22     63     16         2      0      3    16   \n",
       "21       369     347     75    418     73         2      0     11    50   \n",
       "22        25      21    174     96     35         2      0      1    30   \n",
       "23       263     236     99    392     60         2      0      8    61   \n",
       "24        92      30     27    118     15         2      0      0    31   \n",
       "25        10       9      3     35      3         2      0      0     2   \n",
       "26         5       4     28     60      7         2      0      0     8   \n",
       "27       464     434    123    720     78         2      2      9   113   \n",
       "28        98      98     92    555     86         2      0      1   110   \n",
       "29       727     727     51    326     53         2      6      0    42   \n",
       "...      ...     ...    ...    ...    ...       ...    ...    ...   ...   \n",
       "4702     645     645     24    302     32         2      4      1    42   \n",
       "4703       5       5     20    154     24         2      0      0    26   \n",
       "4704     113     112     42    199     46         2      0      0    48   \n",
       "4705     364     304     94    509     97         2      1      8    74   \n",
       "4706      22      19     44    207     45         2      0      3    51   \n",
       "4707     501     477    141    600     95         2      2     13    84   \n",
       "4708      18      10     68    121     29         2      0      1    24   \n",
       "4709      11      11      1     18      0         2      0      0     4   \n",
       "4710       6       6     12     53      5         2      0      0    17   \n",
       "4711      22      21     13     77     28         2      0      0    36   \n",
       "4712     385     364     81    408    133         2      3      8   218   \n",
       "4713     127      83     46     92     59         2      0      1    39   \n",
       "4714     171     171     12    152      9         2      0      0    13   \n",
       "4715       0       0      6     90     27         2      0      0    18   \n",
       "4716      18      13     15    113     18         2      0      0    21   \n",
       "4717       7       5     13     74     10         2      0      0    16   \n",
       "4718       7       4      2      9      3         2      0      1     7   \n",
       "4719     528     490    179    706     98         2      3      9   122   \n",
       "4720       4       3      4     17      6         2      0      0     9   \n",
       "4721     420     369    138    630     66         2      3      6   121   \n",
       "4722     767     767     46    320     77         2     11      0    63   \n",
       "4723       8       8     10    123     19         2      0      0    20   \n",
       "4724       3       3      7     32      5         2      0      0     9   \n",
       "4725      76      67    116    569    101         2      0      0   101   \n",
       "4726     347     347     15    172     12         2      4      8    50   \n",
       "4727      14      13     23    151      6         2      0      0    31   \n",
       "4728     543     498    179    764     75         2      0     15    86   \n",
       "4729       5       5      2     19      3         2      0      0     3   \n",
       "4730     755     746     43    446     31         2      5      1    97   \n",
       "4731     407     369    115    501     54         2      1      6    71   \n",
       "\n",
       "      副詞助詞類接続  助動詞*  助詞並立助詞  助詞係助詞  助詞副助詞  助詞副助詞／並立助詞／終助詞  助詞副詞化  助詞接続助詞  \\\n",
       "0           3    44       1     22      3               0      1      13   \n",
       "1           6    69      13     38      8               5      5      31   \n",
       "2           3    31       2      8      1               0      1      20   \n",
       "3          28   232      18    112      7              10     16     103   \n",
       "4          14   283      26    122      9               6     11     141   \n",
       "5         135  1086      70    459     88              61     60     504   \n",
       "6          10   172       8     66     10              10      7      88   \n",
       "7           3   202       6     89     14               6      5      39   \n",
       "8          36   783      71    372     47              76     31     276   \n",
       "9          54   653      23    300     52              29     30     339   \n",
       "10         22   286      31    176     34              14      9     122   \n",
       "11          5    96       3     42     12               4      4      46   \n",
       "12          7    59       5     32      5               4      2      24   \n",
       "13        330  2930     123   1064    197             163    227    1542   \n",
       "14         12   211      10     78      7              14      7      61   \n",
       "15         18   223       9    137     19               5     14      85   \n",
       "16         27   407       3    103     12              20      5     136   \n",
       "17         45   519      39    209     54              28     33     217   \n",
       "18        148   935      47    537    121             101     78     490   \n",
       "19         12    71      10     53      9               9     11      80   \n",
       "20          9   108       3     35      5               1      4      49   \n",
       "21         38   806      60    341     37              60     21     260   \n",
       "22         15   169      16    103      4              18     24      50   \n",
       "23         36   717      80    361     42              48     20     220   \n",
       "24         18   141       7     68     18               6     10      52   \n",
       "25          3    49       6     15      3               3      2      14   \n",
       "26          7    59       8     37      6               0      9      64   \n",
       "27         59  1004      81    485     71              64     31     369   \n",
       "28         97   881      38    335     52              69     46     334   \n",
       "29         43   609      13    176     12              24      9     214   \n",
       "...       ...   ...     ...    ...    ...             ...    ...     ...   \n",
       "4702       41   410      14    108      9              13     11     103   \n",
       "4703       10   249       6     80     12               6      8      50   \n",
       "4704       30   316      45    134     12              24     20     119   \n",
       "4705       53   954      57    431     59              68     35     321   \n",
       "4706       22   262      12    128     33              17      3     137   \n",
       "4707       59   929      69    439     53              62     35     310   \n",
       "4708       18   166      22    120     22              11     10      96   \n",
       "4709        3    29       1     12      0               0      2       8   \n",
       "4710        3    56       5     30      7               4      1      26   \n",
       "4711       27   184      13     70      9               6     23      65   \n",
       "4712       90  1201     103    727    110              61    104     302   \n",
       "4713       36   345      27    187     33              26     15     180   \n",
       "4714        2   182       0     34      3               8      4      90   \n",
       "4715        8    90      11     64     12               3     15      44   \n",
       "4716        6   126      19     58      8               3     14      90   \n",
       "4717       11   134      16     73      5               7      7      80   \n",
       "4718        6    15       4     22      3               4      4      12   \n",
       "4719      128  1106      60    489    110              63     53     450   \n",
       "4720        3    39      10     19      1               0      5      25   \n",
       "4721      102   842      40    369     76              53     51     392   \n",
       "4722       51   597       5    172     20              32     14     184   \n",
       "4723       13    67       7     49      9               2      2      41   \n",
       "4724        2    43       4     19      2               2      0      20   \n",
       "4725       83   887      45    385     73              50     51     319   \n",
       "4726       29   334       8     89      6              20     10     127   \n",
       "4727        9   143      16     69     20               7     15      71   \n",
       "4728       54  1071      79    505     57              79     45     358   \n",
       "4729        4    17       2      9      1               1      3       6   \n",
       "4730       59   658      12    215     28              31     21     177   \n",
       "4731       45   813      79    397     61              67     17     310   \n",
       "\n",
       "      助詞格助詞  助詞特殊  助詞終助詞  助詞連体化  動詞接尾  動詞自立  動詞非自立  名詞サ変接続  名詞ナイ形容詞語幹  名詞一般  \\\n",
       "0        55     0      0     22     2    42      8      15          1    56   \n",
       "1       122     0      1     48     4    84      5      15          0   144   \n",
       "2        39     0      2     14     3    33     14      10          0    49   \n",
       "3       373     0      9    158     8   297     22      56          2   576   \n",
       "4       383     0      9    195    19   314     72      68          0   475   \n",
       "5      1080     2    116    448    70  1120    199     250          6  1582   \n",
       "6       223     0      5     88    19   171     40      67          0   218   \n",
       "7       179     0      0     50    16   150      8      45          1   198   \n",
       "8       719     2    101    273    50   839     52      87          0  1517   \n",
       "9       713     0     25    280    26   606    103     179          4   893   \n",
       "10      372     0      2    158    20   335     58     132          1   418   \n",
       "11      116     0      1     48    16    80     23      23          1   116   \n",
       "12       67     0      2     26    11    57      5      20          2    73   \n",
       "13     3455     0    163    972   151  3334    885     503         14  3609   \n",
       "14      242     0      6    101     7   214     16      68          0   340   \n",
       "15      331     0      1    132    18   238     33      96          2   344   \n",
       "16      298     0     35     59    19   363     46      30          1   531   \n",
       "17      566     2      8    179    39   541     95     172          1   511   \n",
       "18     1159     0    176    322    54  1175    307     290          9  1301   \n",
       "19      158     0      0     77    16   102     54      35          0   169   \n",
       "20      139     0      0     58     7   128     10      27          0   195   \n",
       "21      751     0     89    284    28   679     34      92          0  1445   \n",
       "22      249     0     16    132    21   278     13      49          1   415   \n",
       "23      699     0     47    313    45   677     34     149          2  1291   \n",
       "24      177     0     35     57     8   274     24      49          0   263   \n",
       "25       56     0      0     41     7    43     11      29          0    75   \n",
       "26      136     0      0     66    11   106     33      91          2   163   \n",
       "27     1007     0     96    489    72  1060     72     115          2  2112   \n",
       "28      778     0    101    253    25   786    202     169          4   852   \n",
       "29      453     0     18    147    28   583     90      54          1   913   \n",
       "...     ...   ...    ...    ...   ...   ...    ...     ...        ...   ...   \n",
       "4702    343     0     29    112    28   452     63      60          0   754   \n",
       "4703    204     0      0     87    10   145     21      65          1   229   \n",
       "4704    423     2     18    251    19   365     23      72          0   698   \n",
       "4705    934     0     98    366    38   939     39     155          1  1694   \n",
       "4706    329     0      8    103     6   264     59      85          0   346   \n",
       "4707    874     1     89    380    38   934     55     154          2  1742   \n",
       "4708    288     0      1    109    45   227     38     124          1   393   \n",
       "4709     32     0      0     11     3    29      0      12          1    44   \n",
       "4710     95     0      0     33     2    81      3      32          1   135   \n",
       "4711    219     1      0    106    40   169     21     113          1   214   \n",
       "4712   1441     1     26    748    35  1448     76     359          4  2405   \n",
       "4713    472     0     11    232    38   403     80     115          3   620   \n",
       "4714    141     0     17     33     9   166     35      10          0   214   \n",
       "4715    153     0      0     51     6   116     28      29          1   163   \n",
       "4716    243     0      1    148    42   186     58     116          6   299   \n",
       "4717    185     0      1     91    21   155     48      69          5   218   \n",
       "4718     51     0      4     16     2    64      3       8          0    75   \n",
       "4719   1051     1    127    422    76  1085    189     194          8  1539   \n",
       "4720     71     0      1     29     6    58     17      14          0    85   \n",
       "4721    884     1     84    337    52   920    135     143          5  1396   \n",
       "4722    396     0     33    126    35   509     86      31          2   836   \n",
       "4723    140     0      5     56     5   112      6      37          3   186   \n",
       "4724     71     0      0     17     3    44      7      18          1    55   \n",
       "4725    905     0     39    366    42   776    179     229          6  1014   \n",
       "4726    285     1     24     44    14   315     41      30          0   408   \n",
       "4727    227     0      2    101    18   193     27      55          1   267   \n",
       "4728   1131     0     95    435    72  1137     54     168          0  2226   \n",
       "4729     19     0      1      4     1    22      3       7          0    22   \n",
       "4730    459     0     36    118    29   606     68      48          1   863   \n",
       "4731    842     0    118    349    43   879     50     115          0  1727   \n",
       "\n",
       "      名詞代名詞  名詞副詞可能  名詞動詞非自立的  名詞固有名詞  名詞引用文字列  名詞形容動詞語幹  名詞接尾  名詞接続詞的  名詞数  \\\n",
       "0        12       2         0       5        0         7     6       0    0   \n",
       "1        18      13         0      19        0        10    23       0    5   \n",
       "2         5       4         0       5        0         0     9       0    6   \n",
       "3        81      20         1      78        0        24    89       0   47   \n",
       "4        69      36         0      13        0        20    60       0   22   \n",
       "5       122      79         1     198        0       121   322       0  346   \n",
       "6        23      18         0      13        0        30    66       0    8   \n",
       "7        33      18         0      64        0        16    33       0   23   \n",
       "8       112      43         0     237        0        40   213       0  214   \n",
       "9       136      70         4     105        0        53   214       7  131   \n",
       "10       41      28         0       6        0        77    25       0    8   \n",
       "11       12      16         0       5        0        22    24       0   14   \n",
       "12       15       3         0      14        0         4    17       0    2   \n",
       "13      745     212         5     355        0       318   437       1  226   \n",
       "14       48      17         0      33        0        25    64       0   17   \n",
       "15       32      42         0      15        0        59    42       0    5   \n",
       "16       36      25         1      46        0        25    60       0   11   \n",
       "17      145      49         1      20        0        65   104       0   26   \n",
       "18      447      68         1     215        0       128   258       0  143   \n",
       "19        7      19         0       3        0        23    35       2   12   \n",
       "20       11      19         0      57        0        13    26       0    7   \n",
       "21       89      49         0     267        0        59   185       5  211   \n",
       "22       69      41         1      26        0        30    54       0   44   \n",
       "23       91      51         0     276        0        59   205       0  235   \n",
       "24       36       9         0       9        0        12    29       0   15   \n",
       "25        8      14         0      17        0         9    37       0   45   \n",
       "26        8      25         0       7        0        20    71       0   29   \n",
       "27      149      72         0     345        0       102   338       0  257   \n",
       "28      163      45         1     154        0        92   136       0   38   \n",
       "29       62      19         0      72        0        20    89       0   12   \n",
       "...     ...     ...       ...     ...      ...       ...   ...     ...  ...   \n",
       "4702     59      38         0      75        0        26    62       2    9   \n",
       "4703     37      20         0       2        0        21    30       0    7   \n",
       "4704     55      32         1     115        0        35    88       0   37   \n",
       "4705    106      44         0     261        0        73   209       0  173   \n",
       "4706     72      14         0      13        0        44    60       1   60   \n",
       "4707    127      56         0     485        0        71   272       0  182   \n",
       "4708     24      25         1      24        0        25    86       0   63   \n",
       "4709      2       2         0       2        0         7     7       0    1   \n",
       "4710     14       7         1      23        0         6    19       0    1   \n",
       "4711     32      30         0      25        0        34    48       0   13   \n",
       "4712    218     109         0     576        0       159   434       1  193   \n",
       "4713     89      49         0      40        0        65    89       0   53   \n",
       "4714     19       8         0      17        0         4    26       0    1   \n",
       "4715     23      14         0       2        0        21    28       0   10   \n",
       "4716     27      37         0      32        0        31    71       0   57   \n",
       "4717     21      20         0      15        0        17    45       0   34   \n",
       "4718      9       9         0       0        0         2    12       0   14   \n",
       "4719    141      77         0     338        0        97   287       0  244   \n",
       "4720      4       6         0      18        0         8    14       0   10   \n",
       "4721    111      55         0     182        0        83   211       0  157   \n",
       "4722     92      41         0     116        0        27   101       2   25   \n",
       "4723     23      11         0      28        0        17    40       0    6   \n",
       "4724     17       6         0       3        0         5     6       0    3   \n",
       "4725    223      80         0     136        0       111   170       0   39   \n",
       "4726     20      18         0      61        0         9    45       0    4   \n",
       "4727     23      19         0      27        0        25    61       0   15   \n",
       "4728    133      85         0     445        0        68   267       0  157   \n",
       "4729      6       2         0       2        0         7     7       0    8   \n",
       "4730    118      20         0      83        0        24    87       0   19   \n",
       "4731    116      39         0     353        0        60   180       0  282   \n",
       "\n",
       "      名詞特殊  名詞非自立  形容詞接尾  形容詞自立  形容詞非自立  感動詞*  接続詞*  接頭詞動詞接続  接頭詞名詞接続  \\\n",
       "0        0     17      1      6       1     0     3        0        0   \n",
       "1        0     36      0     17       0     2     7        0       11   \n",
       "2        0      8      0      4       0     0     1        0        1   \n",
       "3        0     64      0     66       1     9    18        2       19   \n",
       "4        0     96      0     63       2     3    45        0        9   \n",
       "5        4    381      1    123       6    27    48        2      101   \n",
       "6        3     76      0     46       1     0     8        0        3   \n",
       "7        0     76      0     14       0     5    14        0        7   \n",
       "8        0    274      4     99       0    21    29        1       78   \n",
       "9        4    264      1     96       3     8    32        0       39   \n",
       "10       0    136      0     64       2     0    26        0        2   \n",
       "11       0     42      0     14       0     0     6        0        2   \n",
       "12       0     22      0     12       1     1     6        0        4   \n",
       "13       3    874      5    546      15    48   211        1       85   \n",
       "14       0     56      0     25       1     3    10        0       17   \n",
       "15       0     87      0     55       5     5    43        0        5   \n",
       "16       1     72      1     28       1    45    29        0       11   \n",
       "17       0    252      0     85       4     0    69        0       10   \n",
       "18       0    444      3    157      10    20   137       10       25   \n",
       "19       0     51      0     33       0     0     6        0        0   \n",
       "20       0     33      0     23       1     1     5        0       13   \n",
       "21       0    255      5    122       1    22    38        0      130   \n",
       "22       0     57      0     54       3     6    20        0        7   \n",
       "23       0    272      2    109       1    20    23        0       68   \n",
       "24       1     62      1     39       0     6     8        0        5   \n",
       "25       0     15      0      6       0     0     3        0        1   \n",
       "26       0     33      0     10       0     0    11        0        8   \n",
       "27       0    329      0    141       3    38    48        1      120   \n",
       "28       7    271      0     88       3    17    64        1       54   \n",
       "29       1    169      0     73       0    53    35        0       22   \n",
       "...    ...    ...    ...    ...     ...   ...   ...      ...      ...   \n",
       "4702     3    123      0     65       1    56    22        0       33   \n",
       "4703     0     74      0     12       0     0    19        1        5   \n",
       "4704     0    107      0    102       0     8    46        3       17   \n",
       "4705     1    343      7    199       4    28    32        4      133   \n",
       "4706     1    107      0     43       4     8    26        0       17   \n",
       "4707     1    311      0    114       3    53    38        2      117   \n",
       "4708     0     72      0     31       0     1    17        1        8   \n",
       "4709     0     14      1      6       2     0     0        0        2   \n",
       "4710     0     24      0      9       2     0     5        0        3   \n",
       "4711     1     80      0     22       0     1    33        0        2   \n",
       "4712     0    421      3    152       1    33   163        2       62   \n",
       "4713     0    171      4     69       2     4    34        1       15   \n",
       "4714     1     43      0     24       0    19    10        0       18   \n",
       "4715     0     51      0     18       0     0    15        0        3   \n",
       "4716     0     47      0     37       0     0     9        0        8   \n",
       "4717     1     52      0     22       2     0    13        1        5   \n",
       "4718     0     10      0      6       0     1     1        0        2   \n",
       "4719     5    393      1    161       5    53    59        0      209   \n",
       "4720     0     21      0     13       1     1     3        0        2   \n",
       "4721     0    255      1    118       2    48    30        3       97   \n",
       "4722     9    159      0     88       7    55    36        0       21   \n",
       "4723     0     40      0     10       1     4    12        0        6   \n",
       "4724     0     23      0      9       0     0     4        0        2   \n",
       "4725     0    286      5    131       3    10    89        0       35   \n",
       "4726     5     69      0     43       1    30    19        0       32   \n",
       "4727     3     67      0     40       2     0     6        0       26   \n",
       "4728     1    323      1    131       2    60    37        2      285   \n",
       "4729     0      5      0      2       1     1     4        0        1   \n",
       "4730     0    187      0    122       4    78    36        1       55   \n",
       "4731     1    322      1    155       3    33    33        3       88   \n",
       "\n",
       "      接頭詞形容詞接続  接頭詞数接続  記号アルファベット  記号一般  記号句点  記号括弧閉  記号括弧開  記号空白  記号読点  連体詞*  \\\n",
       "0            0       0          0    12    14      4      4     6    24     6   \n",
       "1            0       0          0     7    20      7      7     3    87    18   \n",
       "2            0       0          0     8     8      1      1     4    15     4   \n",
       "3            0       2          0    92   114    109     87    41   170    31   \n",
       "4            0       0          2    69   139     27     20    31   133    39   \n",
       "5            0       2          0   540   256    501    456   179   732    87   \n",
       "6            0       0          0    20    39      4      2    15    86    25   \n",
       "7            1       0          0    65    61     11     12   167    83    20   \n",
       "8            0       8          0   397   190    385    362   125   429    75   \n",
       "9            0       6          4   123   232     40     40    99   309    57   \n",
       "10           0       0          0    73   116     12      2    52   202    48   \n",
       "11           0       0          0    17    22      5      3     8    48    16   \n",
       "12           0       0          0    12    14      2      2     2    47    17   \n",
       "13           0      13          0  1029  1542    478    389   503  1356   269   \n",
       "14           0       1          4   103    70     39     39    70   155    22   \n",
       "15           0       0          0    30    86     27     26    21    64    36   \n",
       "16           0       1          0    72   123    448    448    31   268    35   \n",
       "17           0       0          0    81   156     10     10    73   372    70   \n",
       "18           1       2          4   431   684     88      8   132   830   140   \n",
       "19           0       1          0    14    25      6      3     9    49    23   \n",
       "20           0       0          0    32    35      8      8    22    63    16   \n",
       "21           0       2          0   386   134    369    347    75   418    73   \n",
       "22           0       1          2   401    75     25     21   174    96    35   \n",
       "23           2       0          0   350   196    263    236    99   392    60   \n",
       "24           0       1          0    69    63     92     30    27   118    15   \n",
       "25           0       6          0    15    13     10      9     3    35     3   \n",
       "26           0       1          1    23    29      5      4    28    60     7   \n",
       "27           0       5          0   431   244    464    434   123   720    78   \n",
       "28           0       1          0   243   409     98     98    92   555    86   \n",
       "29           0       4          0   114   160    727    727    51   326    53   \n",
       "...        ...     ...        ...   ...   ...    ...    ...   ...   ...   ...   \n",
       "4702         1       1          0    47    98    645    645    24   302    32   \n",
       "4703         0       3          0    41    59      5      5    20   154    24   \n",
       "4704         0       1          4    73   121    113    112    42   199    46   \n",
       "4705         0       3          2   386   193    364    304    94   509    97   \n",
       "4706         0       1          0    85    98     22     19    44   207    45   \n",
       "4707         0       4          0   513   232    501    477   141   600    95   \n",
       "4708         0       1          0    86    85     18     10    68   121    29   \n",
       "4709         0       0          0    10     6     11     11     1    18     0   \n",
       "4710         0       0          0    28    20      6      6    12    53     5   \n",
       "4711         0       1          3    21    38     22     21    13    77    28   \n",
       "4712         0      15          0   194   514    385    364    81   408   133   \n",
       "4713         0       1          1    72   142    127     83    46    92    59   \n",
       "4714         0       0          0    34    52    171    171    12   152     9   \n",
       "4715         0       6          8    16    62      0      0     6    90    27   \n",
       "4716         0       0          0    21    46     18     13    15   113    18   \n",
       "4717         0       1          0    22    45      7      5    13    74    10   \n",
       "4718         0       0          0    13    33      7      4     2     9     3   \n",
       "4719         0       1          0   632   270    528    490   179   706    98   \n",
       "4720         0       0          0    11    18      4      3     4    17     6   \n",
       "4721         0       2          0   379   199    420    369   138   630    66   \n",
       "4722         0       0          0    88   142    767    767    46   320    77   \n",
       "4723         0       0          0    18    24      8      8    10   123    19   \n",
       "4724         0       0          0    10    10      3      3     7    32     5   \n",
       "4725         0       1          0   222   396     76     67   116   569   101   \n",
       "4726         0       0          0    69   130    347    347    15   172    12   \n",
       "4727         0       3          0    33    41     14     13    23   151     6   \n",
       "4728         0      11          0   631   293    543    498   179   764    75   \n",
       "4729         0       0          0     6     5      5      5     2    19     3   \n",
       "4730         0       0          0   112   165    755    746    43   446    31   \n",
       "4731         0       1          0   441   239    407    369   115   501    54   \n",
       "\n",
       "      BOS/EOS  その他  フィラー   副詞   助動詞    助詞    動詞    名詞  形容詞  感動詞  接続詞  接頭詞  \\\n",
       "0           2    0     4    7    44   117    52   121    8    0    3    0   \n",
       "1           2    0     1   18    69   271    93   283   17    2    7   11   \n",
       "2           2    0     0    7    31    87    50    96    4    0    1    1   \n",
       "3           2    0     4   62   232   806   327  1038   67    9   18   23   \n",
       "4           2    0     0   54   283   902   405   859   65    3   45    9   \n",
       "5           2    4     8  258  1086  2888  1389  3412  130   27   48  105   \n",
       "6           2    0     0   28   172   505   230   522   47    0    8    3   \n",
       "7           2    0     4   26   202   388   174   507   14    5   14    8   \n",
       "8           2    1    10  104   783  1968   941  2737  103   21   29   87   \n",
       "9           2    0     0  122   653  1791   735  2064  100    8   32   45   \n",
       "10          2    0     0  101   286   918   413   872   66    0   26    2   \n",
       "11          2    0     0   21    96   276   119   275   14    0    6    2   \n",
       "12          2    0     3   18    59   167    73   172   13    1    6    4   \n",
       "13          2    0    16  853  2930  7906  4370  7302  566   48  211   99   \n",
       "14          2    0     2   26   211   526   237   668   26    3   10   18   \n",
       "15          2    0     2   67   223   733   289   724   60    5   43    5   \n",
       "16          2    3     1   59   407   671   428   839   30   45   29   12   \n",
       "17          2    0     0  118   519  1335   675  1346   89    0   69   10   \n",
       "18          2    0     2  377   935  3031  1536  3304  170   20  137   38   \n",
       "19          2    0     0   22    71   407   172   356   33    0    6    1   \n",
       "20          2    0     3   25   108   294   145   388   24    1    5   13   \n",
       "21          2    0    11   88   806  1903   741  2657  128   22   38  132   \n",
       "22          2    0     1   45   169   612   312   787   57    6   20    8   \n",
       "23          2    0     8   97   717  1830   756  2631  112   20   23   70   \n",
       "24          2    0     0   49   141   430   306   485   40    6    8    6   \n",
       "25          2    0     0    5    49   140    61   249    6    0    3    7   \n",
       "26          2    0     0   15    59   326   150   449   10    0   11    9   \n",
       "27          2    2     9  172  1004  2693  1204  3821  144   38   48  126   \n",
       "28          2    0     1  207   881  2006  1013  1932   91   17   64   56   \n",
       "29          2    6     0   85   609  1066   701  1412   73   53   35   26   \n",
       "...       ...  ...   ...  ...   ...   ...   ...   ...  ...  ...  ...  ...   \n",
       "4702        2    4     1   83   410   742   543  1211   66   56   22   35   \n",
       "4703        2    0     0   36   249   453   176   486   12    0   19    9   \n",
       "4704        2    0     0   78   316  1048   407  1240  102    8   46   21   \n",
       "4705        2    1     8  127   954  2369  1016  3060  210   28   32  140   \n",
       "4706        2    0     3   73   262   770   329   803   47    8   26   18   \n",
       "4707        2    2    13  143   929  2312  1027  3403  117   53   38  123   \n",
       "4708        2    0     1   42   166   679   310   838   31    1   17   10   \n",
       "4709        2    0     0    7    29    66    32    92    9    0    0    2   \n",
       "4710        2    0     0   20    56   201    86   263   11    0    5    3   \n",
       "4711        2    0     0   63   184   512   230   591   22    1   33    3   \n",
       "4712        2    3     8  308  1201  3623  1559  4879  156   33  163   79   \n",
       "4713        2    0     1   75   345  1183   521  1294   75    4   34   17   \n",
       "4714        2    0     0   15   182   330   210   343   24   19   10   18   \n",
       "4715        2    0     0   26    90   353   150   342   18    0   15    9   \n",
       "4716        2    0     0   27   126   584   286   723   37    0    9    8   \n",
       "4717        2    0     0   27   134   465   224   497   24    0   13    7   \n",
       "4718        2    0     1   13    15   120    69   139    6    1    1    2   \n",
       "4719        2    3     9  250  1106  2826  1350  3323  167   53   59  210   \n",
       "4720        2    0     0   12    39   161    81   180   14    1    3    2   \n",
       "4721        2    3     6  223   842  2287  1107  2598  121   48   30  102   \n",
       "4722        2   11     0  114   597   982   630  1441   95   55   36   21   \n",
       "4723        2    0     0   33    67   311   123   391   11    4   12    6   \n",
       "4724        2    0     0   11    43   135    54   137    9    0    4    2   \n",
       "4725        2    0     0  184   887  2233   997  2294  139   10   89   36   \n",
       "4726        2    4     8   79   334   614   370   669   44   30   19   32   \n",
       "4727        2    0     0   40   143   528   238   563   42    0    6   29   \n",
       "4728        2    0    15  140  1071  2784  1263  3873  134   60   37  298   \n",
       "4729        2    0     0    7    17    46    26    66    3    1    4    1   \n",
       "4730        2    5     1  156   658  1097   703  1450  126   78   36   56   \n",
       "4731        2    1     6  116   813  2240   972  3195  159   33   33   92   \n",
       "\n",
       "        記号  連体詞  letter_number     kanji  hiragana  katakana     digit  \\\n",
       "0       64    6          651.0  0.298003  0.594470  0.036866  0.000000   \n",
       "1      131   18         1363.0  0.319883  0.562729  0.026412  0.000000   \n",
       "2       37    4          482.0  0.338174  0.556017  0.051867  0.000000   \n",
       "3      613   31         4301.0  0.332946  0.526157  0.004185  0.004185   \n",
       "4      421   39         4273.0  0.334659  0.578750  0.001170  0.000000   \n",
       "5     2664   87        16845.0  0.287029  0.557673  0.022737  0.000594   \n",
       "6      166   25         2722.0  0.273328  0.666789  0.008817  0.000000   \n",
       "7      399   20         2319.0  0.270806  0.567055  0.068564  0.000000   \n",
       "8     1888   75        11568.0  0.295816  0.552472  0.016425  0.001037   \n",
       "9      847   57         9791.0  0.279542  0.582065  0.071903  0.000000   \n",
       "10     457   48         4828.0  0.339478  0.580157  0.000000  0.000000   \n",
       "11     103   16         1472.0  0.275136  0.656250  0.000000  0.003397   \n",
       "12      79   17          905.0  0.277348  0.640884  0.006630  0.000000   \n",
       "13    5297  269        42552.0  0.285157  0.606951  0.003361  0.002021   \n",
       "14     480   22         3193.0  0.295647  0.473849  0.121829  0.000000   \n",
       "15     254   36         3913.0  0.286481  0.655763  0.004344  0.000256   \n",
       "16    1390   35         5458.0  0.102968  0.657933  0.001832  0.000000   \n",
       "17     702   70         7572.0  0.260301  0.626519  0.035394  0.000000   \n",
       "18    2177  140        17876.0  0.262195  0.614399  0.004140  0.002461   \n",
       "19     106   23         1860.0  0.293548  0.646237  0.011290  0.000000   \n",
       "20     168   16         1691.0  0.351271  0.573034  0.005322  0.000000   \n",
       "21    1729   73        11050.0  0.304163  0.551312  0.009683  0.000543   \n",
       "22     794   35         3478.0  0.351064  0.536515  0.032490  0.000000   \n",
       "23    1536   60        10685.0  0.333271  0.527936  0.016846  0.000655   \n",
       "24     399   15         2648.0  0.245846  0.580816  0.004532  0.004154   \n",
       "25      85    3          952.0  0.328782  0.476891  0.108193  0.002101   \n",
       "26     150    7         1774.0  0.436866  0.450395  0.049042  0.000564   \n",
       "27    2416   78        15511.0  0.314615  0.541938  0.012636  0.000451   \n",
       "28    1495   86        11745.0  0.251000  0.626564  0.018731  0.000000   \n",
       "29    2105   53         8490.0  0.100000  0.666549  0.000942  0.000000   \n",
       "...    ...  ...            ...       ...       ...       ...       ...   \n",
       "4702  1761   32         6808.0  0.126322  0.612368  0.011457  0.000000   \n",
       "4703   284   24         2627.0  0.301104  0.607156  0.004187  0.000000   \n",
       "4704   664   46         5565.0  0.337646  0.527583  0.026415  0.000180   \n",
       "4705  1852   97        13374.0  0.284358  0.581501  0.010094  0.000523   \n",
       "4706   475   45         4203.0  0.298834  0.599572  0.004283  0.001190   \n",
       "4707  2464   95        14024.0  0.296064  0.542213  0.010767  0.001284   \n",
       "4708   388   29         3851.0  0.325370  0.525837  0.075305  0.001298   \n",
       "4709    57    0          426.0  0.335681  0.556338  0.000000  0.000000   \n",
       "4710   125    5         1185.0  0.363713  0.516456  0.040506  0.000000   \n",
       "4711   195   28         3182.0  0.260214  0.599937  0.087681  0.000314   \n",
       "4712  1946  133        20649.0  0.354932  0.522640  0.027411  0.001937   \n",
       "4713   563   59         6431.0  0.271342  0.629762  0.013217  0.002954   \n",
       "4714   592    9         2412.0  0.078773  0.691542  0.002488  0.000000   \n",
       "4715   182   27         1796.0  0.326281  0.574610  0.002784  0.000000   \n",
       "4716   226   18         3347.0  0.328354  0.579026  0.032865  0.000000   \n",
       "4717   166   10         2506.0  0.284517  0.640064  0.015962  0.000000   \n",
       "4718    68    3          636.0  0.287736  0.614780  0.000000  0.001572   \n",
       "4719  2805   98        16763.0  0.285868  0.559387  0.015510  0.000477   \n",
       "4720    57    6          928.0  0.311422  0.623922  0.012931  0.000000   \n",
       "4721  2135   66        13606.0  0.280758  0.561370  0.022931  0.000588   \n",
       "4722  2130   77         8374.0  0.115357  0.643898  0.000000  0.000000   \n",
       "4723   191   19         1768.0  0.356900  0.538462  0.008484  0.000000   \n",
       "4724    65    5          696.0  0.261494  0.669540  0.000000  0.000000   \n",
       "4725  1446  101        12389.0  0.300993  0.591412  0.010090  0.000888   \n",
       "4726  1080   12         4586.0  0.086132  0.679023  0.016354  0.000000   \n",
       "4727   275    6         2739.0  0.354509  0.550931  0.011318  0.000000   \n",
       "4728  2908   75        16485.0  0.307795  0.527631  0.011829  0.002002   \n",
       "4729    42    3          317.0  0.312303  0.542587  0.034700  0.000000   \n",
       "4730  2267   31         9279.0  0.101088  0.667744  0.000000  0.000323   \n",
       "4731  2072   54        12830.0  0.300156  0.541154  0.017537  0.001013   \n",
       "\n",
       "      alphabet  old_kanji  old_hiragana       old  lowercase     hagyo  \\\n",
       "0     0.000000   0.001536      0.000000  0.001536   0.007680  0.016897   \n",
       "1     0.000000   0.000000      0.005136  0.005136   0.000000  0.037417   \n",
       "2     0.000000   0.000000      0.000000  0.000000   0.022822  0.018672   \n",
       "3     0.000465   0.000698      0.007673  0.008370   0.000000  0.034876   \n",
       "4     0.000468   0.000468      0.000000  0.000468   0.025743  0.026445   \n",
       "5     0.001306   0.000712      0.000000  0.000712   0.023212  0.026061   \n",
       "6     0.000000   0.000000      0.000000  0.000000   0.019838  0.023145   \n",
       "7     0.006900   0.003450      0.007331  0.010781   0.000000  0.050022   \n",
       "8     0.000000   0.049101      0.002334  0.051435   0.000000  0.037344   \n",
       "9     0.000409   0.000204      0.000000  0.000204   0.019508  0.028904   \n",
       "10    0.000000   0.000000      0.000000  0.000000   0.008492  0.028583   \n",
       "11    0.000000   0.002038      0.000000  0.002038   0.016984  0.025136   \n",
       "12    0.000000   0.005525      0.000000  0.005525   0.000000  0.054144   \n",
       "13    0.000000   0.000329      0.000000  0.000329   0.035792  0.025169   \n",
       "14    0.004071   0.005324      0.005951  0.011275   0.000000  0.033511   \n",
       "15    0.000000   0.000256      0.000000  0.000256   0.012778  0.032200   \n",
       "16    0.000000   0.000366      0.000000  0.000366   0.023085  0.026383   \n",
       "17    0.001189   0.000000      0.000000  0.000000   0.015716  0.027073   \n",
       "18    0.000224   0.002294      0.000000  0.002294   0.024894  0.025789   \n",
       "19    0.000000   0.000000      0.000000  0.000000   0.018280  0.025806   \n",
       "20    0.000000   0.001183      0.005322  0.006505   0.000000  0.043170   \n",
       "21    0.000000   0.044072      0.000814  0.044887   0.000000  0.039819   \n",
       "22    0.000575   0.000288      0.000000  0.000288   0.007476  0.030190   \n",
       "23    0.000000   0.050725      0.003556  0.054282   0.000000  0.036032   \n",
       "24    0.000000   0.000000      0.000000  0.000000   0.025680  0.047583   \n",
       "25    0.000000   0.000000      0.000000  0.000000   0.014706  0.018908   \n",
       "26    0.000564   0.000564      0.000000  0.000564   0.018602  0.020293   \n",
       "27    0.000000   0.049126      0.001870  0.050996   0.000000  0.037135   \n",
       "28    0.000000   0.000170      0.000000  0.000170   0.029459  0.030055   \n",
       "29    0.000000   0.000353      0.000000  0.000353   0.019081  0.031684   \n",
       "...        ...        ...           ...       ...        ...       ...   \n",
       "4702  0.000000   0.005288      0.000000  0.005288   0.022474  0.026293   \n",
       "4703  0.000000   0.004949      0.000000  0.004949   0.012181  0.024362   \n",
       "4704  0.003414   0.000719      0.009524  0.010243   0.000000  0.038275   \n",
       "4705  0.000150   0.049200      0.004636  0.053836   0.000000  0.038508   \n",
       "4706  0.000000   0.000476      0.000000  0.000476   0.017844  0.028789   \n",
       "4707  0.000000   0.049843      0.002924  0.052767   0.000000  0.038862   \n",
       "4708  0.000000   0.000779      0.000000  0.000779   0.017138  0.025967   \n",
       "4709  0.000000   0.000000      0.004695  0.004695   0.000000  0.042254   \n",
       "4710  0.000000   0.004219      0.005907  0.010127   0.000000  0.036287   \n",
       "4711  0.000943   0.000314      0.000000  0.000314   0.011314  0.021056   \n",
       "4712  0.000436   0.001259      0.007313  0.008572   0.000097  0.044700   \n",
       "4713  0.001088   0.000155      0.000000  0.000155   0.015550  0.028145   \n",
       "4714  0.000000   0.000000      0.000000  0.000000   0.026534  0.024876   \n",
       "4715  0.004454   0.000000      0.000000  0.000000   0.020045  0.033408   \n",
       "4716  0.000000   0.001793      0.000000  0.001793   0.011951  0.024798   \n",
       "4717  0.000000   0.000000      0.000000  0.000000   0.018755  0.027534   \n",
       "4718  0.000000   0.001572      0.014151  0.015723   0.000000  0.033019   \n",
       "4719  0.000000   0.000477      0.000000  0.000477   0.022132  0.027203   \n",
       "4720  0.000000   0.000000      0.000000  0.000000   0.015086  0.018319   \n",
       "4721  0.000000   0.000661      0.000000  0.000661   0.021682  0.026385   \n",
       "4722  0.000000   0.000000      0.000000  0.000000   0.019226  0.031168   \n",
       "4723  0.000000   0.000566      0.004525  0.005090   0.000000  0.045814   \n",
       "4724  0.000000   0.000000      0.002874  0.002874   0.000000  0.045977   \n",
       "4725  0.000000   0.000646      0.000000  0.000646   0.025991  0.031641   \n",
       "4726  0.000000   0.000000      0.000000  0.000000   0.016790  0.033362   \n",
       "4727  0.000000   0.000365      0.000000  0.000365   0.017525  0.017890   \n",
       "4728  0.000000   0.057386      0.002184  0.059569   0.000000  0.038277   \n",
       "4729  0.000000   0.000000      0.003155  0.003155   0.000000  0.053628   \n",
       "4730  0.000000   0.000323      0.000000  0.000323   0.018536  0.028882   \n",
       "4731  0.000000   0.042557      0.002027  0.044583   0.000000  0.037958   \n",
       "\n",
       "         kakko  kagikakko   kutoten  \n",
       "0     0.000000   0.012289  0.058372  \n",
       "1     0.000000   0.010271  0.078503  \n",
       "2     0.000000   0.004149  0.047718  \n",
       "3     0.000000   0.012555  0.066031  \n",
       "4     0.000468   0.010297  0.063656  \n",
       "5     0.000831   0.028614  0.058652  \n",
       "6     0.000000   0.000735  0.045922  \n",
       "7     0.000862   0.000862  0.062096  \n",
       "8     0.000000   0.031812  0.053510  \n",
       "9     0.000409   0.006945  0.055255  \n",
       "10    0.000000   0.002486  0.065866  \n",
       "11    0.000000   0.004076  0.047554  \n",
       "12    0.000000   0.004420  0.067403  \n",
       "13    0.000000   0.015322  0.068105  \n",
       "14    0.001253   0.020670  0.070467  \n",
       "15    0.000511   0.004089  0.038334  \n",
       "16    0.000000   0.010993  0.071638  \n",
       "17    0.001849   0.000264  0.069731  \n",
       "18    0.000336   0.000783  0.084695  \n",
       "19    0.000000   0.002151  0.039785  \n",
       "20    0.000000   0.001183  0.057954  \n",
       "21    0.000181   0.034751  0.049955  \n",
       "22    0.004025   0.001725  0.049166  \n",
       "23    0.000749   0.025456  0.055030  \n",
       "24    0.000755   0.035498  0.068353  \n",
       "25    0.000000   0.014706  0.050420  \n",
       "26    0.003382   0.000000  0.050169  \n",
       "27    0.000129   0.028367  0.062149  \n",
       "28    0.000000   0.016688  0.082077  \n",
       "29    0.000000   0.007067  0.057244  \n",
       "...        ...        ...       ...  \n",
       "4702  0.000000   0.003819  0.058754  \n",
       "4703  0.000000   0.000000  0.081081  \n",
       "4704  0.000359   0.007188  0.057502  \n",
       "4705  0.000150   0.026469  0.052490  \n",
       "4706  0.000000   0.009517  0.072567  \n",
       "4707  0.000143   0.026954  0.059327  \n",
       "4708  0.001558   0.003635  0.053493  \n",
       "4709  0.000000   0.051643  0.056338  \n",
       "4710  0.001688   0.006751  0.061603  \n",
       "4711  0.001257   0.002514  0.036141  \n",
       "4712  0.001840   0.007942  0.044651  \n",
       "4713  0.000622   0.010885  0.036386  \n",
       "4714  0.000000   0.013267  0.084577  \n",
       "4715  0.000000   0.000000  0.084633  \n",
       "4716  0.002390   0.005378  0.047505  \n",
       "4717  0.000000   0.003192  0.047486  \n",
       "4718  0.000000   0.012579  0.066038  \n",
       "4719  0.000119   0.032810  0.058223  \n",
       "4720  0.002155   0.002155  0.037716  \n",
       "4721  0.000735   0.025283  0.060929  \n",
       "4722  0.000000   0.005016  0.055171  \n",
       "4723  0.001131   0.007919  0.083145  \n",
       "4724  0.000000   0.008621  0.060345  \n",
       "4725  0.000000   0.011300  0.077892  \n",
       "4726  0.000000   0.018753  0.065853  \n",
       "4727  0.000730   0.005111  0.070099  \n",
       "4728  0.000000   0.029360  0.064119  \n",
       "4729  0.012618   0.012618  0.075710  \n",
       "4730  0.000000   0.009699  0.065848  \n",
       "4731  0.000000   0.032424  0.057677  \n",
       "\n",
       "[4732 rows x 19570 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15925"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del negaposi_sentence_split10, negaposi_describe, word_count100_tfidf, word_count100, word_0102, katsuyo0, katsuyo01, adjective012, adjective01, adjective0, hyoki\n",
    "del negaposi_sentence_split10, negaposi_describe, word_count100_tfidf, word_count100, katsuyo0, katsuyo01, adjective012, adjective01, adjective0, hyoki\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"../input/target.csv\", index_col=0, names=[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4702</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4703</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4705</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4706</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4707</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4708</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4709</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4710</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4711</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4712</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4713</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4715</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4716</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4717</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4718</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4719</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4720</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4722</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4723</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4724</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4725</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4726</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4727</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4728</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4729</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4730</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4731</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4732 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target\n",
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        1.0\n",
       "4        0.0\n",
       "5        0.0\n",
       "6        0.0\n",
       "7        1.0\n",
       "8        0.0\n",
       "9        0.0\n",
       "10       0.0\n",
       "11       0.0\n",
       "12       0.0\n",
       "13       0.0\n",
       "14       0.0\n",
       "15       0.0\n",
       "16       0.0\n",
       "17       0.0\n",
       "18       0.0\n",
       "19       0.0\n",
       "20       0.0\n",
       "21       0.0\n",
       "22       0.0\n",
       "23       0.0\n",
       "24       0.0\n",
       "25       0.0\n",
       "26       0.0\n",
       "27       0.0\n",
       "28       0.0\n",
       "29       0.0\n",
       "...      ...\n",
       "4702     NaN\n",
       "4703     NaN\n",
       "4704     NaN\n",
       "4705     NaN\n",
       "4706     NaN\n",
       "4707     NaN\n",
       "4708     NaN\n",
       "4709     NaN\n",
       "4710     NaN\n",
       "4711     NaN\n",
       "4712     NaN\n",
       "4713     NaN\n",
       "4714     NaN\n",
       "4715     NaN\n",
       "4716     NaN\n",
       "4717     NaN\n",
       "4718     NaN\n",
       "4719     NaN\n",
       "4720     NaN\n",
       "4721     NaN\n",
       "4722     NaN\n",
       "4723     NaN\n",
       "4724     NaN\n",
       "4725     NaN\n",
       "4726     NaN\n",
       "4727     NaN\n",
       "4728     NaN\n",
       "4729     NaN\n",
       "4730     NaN\n",
       "4731     NaN\n",
       "\n",
       "[4732 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_index = y[y[\"target\"].notna()].index\n",
    "test_index = y[y[\"target\"].isna()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid = data.iloc[train_index]\n",
    "X_test = data.iloc[test_index]\n",
    "y_train_valid = y.iloc[train_index]\n",
    "y_train_valid = y_train_valid.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習・予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hold out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.3, random_state=0, stratify=y_train)\n",
    "# col_name = X_valid.columns.values\n",
    "# X_train.columns = range(X_train.shape[1])\n",
    "# X_valid.columns = range(X_valid.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = lgb.Dataset(X_train, label=y_train)\n",
    "# valid = lgb.Dataset(X_valid, label=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'boosting_type':'gbdt', \n",
    "#     'objective':'binary',\n",
    "#     \"metric\":\"binary_logloss\", \n",
    "#     'n_estimators':1000,\n",
    "#     'random_state':0,\n",
    "#     'learning_rate':[0.1],\n",
    "#     'max_depth':[5], #3~9\n",
    "#     \"min_chlid_wight\":[1], #1~5\n",
    "#     'gamma':[0], #0~0.4\n",
    "#     'colsample_bytree':[0.8], #\n",
    "#     'subsamples':[0.8], #\n",
    "#     'reg_alpha':[0], #\n",
    "#     'reg_lambda':[1], #\n",
    "# }\n",
    "\n",
    "# model = lgb.train(params=params,\n",
    "#                   train_set=train,\n",
    "#                   valid_sets=[train, valid],\n",
    "#                   num_boost_round=1000,\n",
    "#                   early_stopping_rounds=50,\n",
    "#                   verbose_eval=1,\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = X_train_valid.columns.values\n",
    "X_train_valid.columns = range(X_train_valid.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 81 candidates, totalling 324 fits\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105073\n",
      "[20]\tvalid's binary_logloss: 0.0634969\n",
      "[30]\tvalid's binary_logloss: 0.043598\n",
      "[40]\tvalid's binary_logloss: 0.0325657\n",
      "[50]\tvalid's binary_logloss: 0.0259775\n",
      "[60]\tvalid's binary_logloss: 0.0216568\n",
      "[70]\tvalid's binary_logloss: 0.0183566\n",
      "[80]\tvalid's binary_logloss: 0.0160596\n",
      "[90]\tvalid's binary_logloss: 0.014584\n",
      "[100]\tvalid's binary_logloss: 0.0135308\n",
      "[110]\tvalid's binary_logloss: 0.0127343\n",
      "[120]\tvalid's binary_logloss: 0.0122699\n",
      "[130]\tvalid's binary_logloss: 0.0118449\n",
      "[140]\tvalid's binary_logloss: 0.0114971\n",
      "[150]\tvalid's binary_logloss: 0.0112897\n",
      "[160]\tvalid's binary_logloss: 0.0110799\n",
      "[170]\tvalid's binary_logloss: 0.0109311\n",
      "[180]\tvalid's binary_logloss: 0.0107706\n",
      "[190]\tvalid's binary_logloss: 0.0105952\n",
      "[200]\tvalid's binary_logloss: 0.0106606\n",
      "[210]\tvalid's binary_logloss: 0.0106479\n",
      "[220]\tvalid's binary_logloss: 0.0106011\n",
      "[230]\tvalid's binary_logloss: 0.0105393\n",
      "[240]\tvalid's binary_logloss: 0.0105146\n",
      "[250]\tvalid's binary_logloss: 0.0104637\n",
      "[260]\tvalid's binary_logloss: 0.0105087\n",
      "[270]\tvalid's binary_logloss: 0.0104558\n",
      "[280]\tvalid's binary_logloss: 0.0105071\n",
      "[290]\tvalid's binary_logloss: 0.0105024\n",
      "Early stopping, best iteration is:\n",
      "[267]\tvalid's binary_logloss: 0.0104283\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  54.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.108514\n",
      "[20]\tvalid's binary_logloss: 0.0659098\n",
      "[30]\tvalid's binary_logloss: 0.0466444\n",
      "[40]\tvalid's binary_logloss: 0.0354487\n",
      "[50]\tvalid's binary_logloss: 0.0282533\n",
      "[60]\tvalid's binary_logloss: 0.0237543\n",
      "[70]\tvalid's binary_logloss: 0.0200102\n",
      "[80]\tvalid's binary_logloss: 0.0172922\n",
      "[90]\tvalid's binary_logloss: 0.0154309\n",
      "[100]\tvalid's binary_logloss: 0.0141211\n",
      "[110]\tvalid's binary_logloss: 0.0129775\n",
      "[120]\tvalid's binary_logloss: 0.0121296\n",
      "[130]\tvalid's binary_logloss: 0.0116095\n",
      "[140]\tvalid's binary_logloss: 0.0112809\n",
      "[150]\tvalid's binary_logloss: 0.0108221\n",
      "[160]\tvalid's binary_logloss: 0.0105229\n",
      "[170]\tvalid's binary_logloss: 0.0102484\n",
      "[180]\tvalid's binary_logloss: 0.0101004\n",
      "[190]\tvalid's binary_logloss: 0.00992344\n",
      "[200]\tvalid's binary_logloss: 0.00991031\n",
      "[210]\tvalid's binary_logloss: 0.00984618\n",
      "[220]\tvalid's binary_logloss: 0.0097944\n",
      "[230]\tvalid's binary_logloss: 0.00977616\n",
      "[240]\tvalid's binary_logloss: 0.00970219\n",
      "[250]\tvalid's binary_logloss: 0.00961072\n",
      "[260]\tvalid's binary_logloss: 0.00959773\n",
      "[270]\tvalid's binary_logloss: 0.00953399\n",
      "[280]\tvalid's binary_logloss: 0.00949878\n",
      "[290]\tvalid's binary_logloss: 0.00949019\n",
      "[300]\tvalid's binary_logloss: 0.00950097\n",
      "Early stopping, best iteration is:\n",
      "[279]\tvalid's binary_logloss: 0.00948442\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9411764705882353, total=110.7min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 111.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.108911\n",
      "[20]\tvalid's binary_logloss: 0.0668806\n",
      "[30]\tvalid's binary_logloss: 0.0465771\n",
      "[40]\tvalid's binary_logloss: 0.0353627\n",
      "[50]\tvalid's binary_logloss: 0.0280265\n",
      "[60]\tvalid's binary_logloss: 0.0235674\n",
      "[70]\tvalid's binary_logloss: 0.0201826\n",
      "[80]\tvalid's binary_logloss: 0.0178578\n",
      "[90]\tvalid's binary_logloss: 0.0165988\n",
      "[100]\tvalid's binary_logloss: 0.0155225\n",
      "[110]\tvalid's binary_logloss: 0.0147456\n",
      "[120]\tvalid's binary_logloss: 0.014327\n",
      "[130]\tvalid's binary_logloss: 0.0139162\n",
      "[140]\tvalid's binary_logloss: 0.0138116\n",
      "[150]\tvalid's binary_logloss: 0.0136314\n",
      "[160]\tvalid's binary_logloss: 0.013631\n",
      "[170]\tvalid's binary_logloss: 0.0136222\n",
      "[180]\tvalid's binary_logloss: 0.0136329\n",
      "[190]\tvalid's binary_logloss: 0.0136771\n",
      "[200]\tvalid's binary_logloss: 0.0137022\n",
      "Early stopping, best iteration is:\n",
      "[173]\tvalid's binary_logloss: 0.0135902\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  50.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.109367\n",
      "[20]\tvalid's binary_logloss: 0.066934\n",
      "[30]\tvalid's binary_logloss: 0.0467975\n",
      "[40]\tvalid's binary_logloss: 0.0352754\n",
      "[50]\tvalid's binary_logloss: 0.0275737\n",
      "[60]\tvalid's binary_logloss: 0.0224697\n",
      "[70]\tvalid's binary_logloss: 0.0186838\n",
      "[80]\tvalid's binary_logloss: 0.0160403\n",
      "[90]\tvalid's binary_logloss: 0.0142614\n",
      "[100]\tvalid's binary_logloss: 0.0127755\n",
      "[110]\tvalid's binary_logloss: 0.0117657\n",
      "[120]\tvalid's binary_logloss: 0.0108088\n",
      "[130]\tvalid's binary_logloss: 0.0102188\n",
      "[140]\tvalid's binary_logloss: 0.0097797\n",
      "[150]\tvalid's binary_logloss: 0.00938666\n",
      "[160]\tvalid's binary_logloss: 0.00906686\n",
      "[170]\tvalid's binary_logloss: 0.00877187\n",
      "[180]\tvalid's binary_logloss: 0.00863268\n",
      "[190]\tvalid's binary_logloss: 0.0084416\n",
      "[200]\tvalid's binary_logloss: 0.00835473\n",
      "[210]\tvalid's binary_logloss: 0.00832613\n",
      "[220]\tvalid's binary_logloss: 0.0082392\n",
      "[230]\tvalid's binary_logloss: 0.008165\n",
      "[240]\tvalid's binary_logloss: 0.00813927\n",
      "[250]\tvalid's binary_logloss: 0.00811384\n",
      "[260]\tvalid's binary_logloss: 0.008079\n",
      "[270]\tvalid's binary_logloss: 0.00813439\n",
      "[280]\tvalid's binary_logloss: 0.00802023\n",
      "[290]\tvalid's binary_logloss: 0.00801244\n",
      "[300]\tvalid's binary_logloss: 0.00803158\n",
      "[310]\tvalid's binary_logloss: 0.00800437\n",
      "[320]\tvalid's binary_logloss: 0.00801984\n",
      "[330]\tvalid's binary_logloss: 0.00803649\n",
      "Early stopping, best iteration is:\n",
      "[308]\tvalid's binary_logloss: 0.00798337\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  58.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.10403\n",
      "[20]\tvalid's binary_logloss: 0.0619253\n",
      "[30]\tvalid's binary_logloss: 0.0412695\n",
      "[40]\tvalid's binary_logloss: 0.0305845\n",
      "[50]\tvalid's binary_logloss: 0.024381\n",
      "[60]\tvalid's binary_logloss: 0.020415\n",
      "[70]\tvalid's binary_logloss: 0.017838\n",
      "[80]\tvalid's binary_logloss: 0.0158273\n",
      "[90]\tvalid's binary_logloss: 0.0143852\n",
      "[100]\tvalid's binary_logloss: 0.0134324\n",
      "[110]\tvalid's binary_logloss: 0.0126329\n",
      "[120]\tvalid's binary_logloss: 0.0119836\n",
      "[130]\tvalid's binary_logloss: 0.0118424\n",
      "[140]\tvalid's binary_logloss: 0.0115534\n",
      "[150]\tvalid's binary_logloss: 0.0114087\n",
      "[160]\tvalid's binary_logloss: 0.0114295\n",
      "[170]\tvalid's binary_logloss: 0.0114933\n",
      "Early stopping, best iteration is:\n",
      "[146]\tvalid's binary_logloss: 0.0113581\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9152542372881356, total=  48.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.104828\n",
      "[20]\tvalid's binary_logloss: 0.0619801\n",
      "[30]\tvalid's binary_logloss: 0.042355\n",
      "[40]\tvalid's binary_logloss: 0.0317094\n",
      "[50]\tvalid's binary_logloss: 0.0253121\n",
      "[60]\tvalid's binary_logloss: 0.0212435\n",
      "[70]\tvalid's binary_logloss: 0.018234\n",
      "[80]\tvalid's binary_logloss: 0.0159163\n",
      "[90]\tvalid's binary_logloss: 0.0140125\n",
      "[100]\tvalid's binary_logloss: 0.0129025\n",
      "[110]\tvalid's binary_logloss: 0.0118438\n",
      "[120]\tvalid's binary_logloss: 0.0110394\n",
      "[130]\tvalid's binary_logloss: 0.010695\n",
      "[140]\tvalid's binary_logloss: 0.0102274\n",
      "[150]\tvalid's binary_logloss: 0.0100192\n",
      "[160]\tvalid's binary_logloss: 0.00994716\n",
      "[170]\tvalid's binary_logloss: 0.00977614\n",
      "[180]\tvalid's binary_logloss: 0.00965711\n",
      "[190]\tvalid's binary_logloss: 0.00966731\n",
      "[200]\tvalid's binary_logloss: 0.00966612\n",
      "[210]\tvalid's binary_logloss: 0.00967999\n",
      "[220]\tvalid's binary_logloss: 0.00960755\n",
      "Early stopping, best iteration is:\n",
      "[195]\tvalid's binary_logloss: 0.00958885\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9411764705882353, total=  46.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105891\n",
      "[20]\tvalid's binary_logloss: 0.064263\n",
      "[30]\tvalid's binary_logloss: 0.0444696\n",
      "[40]\tvalid's binary_logloss: 0.0329888\n",
      "[50]\tvalid's binary_logloss: 0.0257328\n",
      "[60]\tvalid's binary_logloss: 0.020946\n",
      "[70]\tvalid's binary_logloss: 0.0179026\n",
      "[80]\tvalid's binary_logloss: 0.0160167\n",
      "[90]\tvalid's binary_logloss: 0.014951\n",
      "[100]\tvalid's binary_logloss: 0.0139925\n",
      "[110]\tvalid's binary_logloss: 0.0135363\n",
      "[120]\tvalid's binary_logloss: 0.013268\n",
      "[130]\tvalid's binary_logloss: 0.0132706\n",
      "[140]\tvalid's binary_logloss: 0.0131513\n",
      "[150]\tvalid's binary_logloss: 0.013128\n",
      "[160]\tvalid's binary_logloss: 0.0132014\n",
      "[170]\tvalid's binary_logloss: 0.0133134\n",
      "[180]\tvalid's binary_logloss: 0.0135634\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid's binary_logloss: 0.0130475\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.8947368421052632, total=  46.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105213\n",
      "[20]\tvalid's binary_logloss: 0.063666\n",
      "[30]\tvalid's binary_logloss: 0.0439832\n",
      "[40]\tvalid's binary_logloss: 0.0312532\n",
      "[50]\tvalid's binary_logloss: 0.0244658\n",
      "[60]\tvalid's binary_logloss: 0.0196177\n",
      "[70]\tvalid's binary_logloss: 0.01662\n",
      "[80]\tvalid's binary_logloss: 0.0143578\n",
      "[90]\tvalid's binary_logloss: 0.0130189\n",
      "[100]\tvalid's binary_logloss: 0.0119034\n",
      "[110]\tvalid's binary_logloss: 0.011225\n",
      "[120]\tvalid's binary_logloss: 0.0106242\n",
      "[130]\tvalid's binary_logloss: 0.00997445\n",
      "[140]\tvalid's binary_logloss: 0.00969074\n",
      "[150]\tvalid's binary_logloss: 0.00944223\n",
      "[160]\tvalid's binary_logloss: 0.00947977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170]\tvalid's binary_logloss: 0.00919612\n",
      "[180]\tvalid's binary_logloss: 0.0091831\n",
      "[190]\tvalid's binary_logloss: 0.00918958\n",
      "[200]\tvalid's binary_logloss: 0.00915935\n",
      "[210]\tvalid's binary_logloss: 0.00913588\n",
      "[220]\tvalid's binary_logloss: 0.00916788\n",
      "[230]\tvalid's binary_logloss: 0.0092328\n",
      "Early stopping, best iteration is:\n",
      "[203]\tvalid's binary_logloss: 0.00907183\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  46.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.103311\n",
      "[20]\tvalid's binary_logloss: 0.0624206\n",
      "[30]\tvalid's binary_logloss: 0.0420341\n",
      "[40]\tvalid's binary_logloss: 0.0310373\n",
      "[50]\tvalid's binary_logloss: 0.0253095\n",
      "[60]\tvalid's binary_logloss: 0.0209138\n",
      "[70]\tvalid's binary_logloss: 0.0180477\n",
      "[80]\tvalid's binary_logloss: 0.0157355\n",
      "[90]\tvalid's binary_logloss: 0.0143605\n",
      "[100]\tvalid's binary_logloss: 0.013181\n",
      "[110]\tvalid's binary_logloss: 0.0123085\n",
      "[120]\tvalid's binary_logloss: 0.0118526\n",
      "[130]\tvalid's binary_logloss: 0.0115192\n",
      "[140]\tvalid's binary_logloss: 0.011445\n",
      "[150]\tvalid's binary_logloss: 0.0114706\n",
      "[160]\tvalid's binary_logloss: 0.0111286\n",
      "[170]\tvalid's binary_logloss: 0.0113165\n",
      "[180]\tvalid's binary_logloss: 0.0113453\n",
      "[190]\tvalid's binary_logloss: 0.0114473\n",
      "Early stopping, best iteration is:\n",
      "[160]\tvalid's binary_logloss: 0.0111286\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  46.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105185\n",
      "[20]\tvalid's binary_logloss: 0.0624537\n",
      "[30]\tvalid's binary_logloss: 0.0425726\n",
      "[40]\tvalid's binary_logloss: 0.0323354\n",
      "[50]\tvalid's binary_logloss: 0.0261086\n",
      "[60]\tvalid's binary_logloss: 0.0218966\n",
      "[70]\tvalid's binary_logloss: 0.0188421\n",
      "[80]\tvalid's binary_logloss: 0.0164398\n",
      "[90]\tvalid's binary_logloss: 0.0145015\n",
      "[100]\tvalid's binary_logloss: 0.0133203\n",
      "[110]\tvalid's binary_logloss: 0.0125799\n",
      "[120]\tvalid's binary_logloss: 0.0118984\n",
      "[130]\tvalid's binary_logloss: 0.0114483\n",
      "[140]\tvalid's binary_logloss: 0.0109922\n",
      "[150]\tvalid's binary_logloss: 0.0110174\n",
      "[160]\tvalid's binary_logloss: 0.0107704\n",
      "[170]\tvalid's binary_logloss: 0.0106354\n",
      "[180]\tvalid's binary_logloss: 0.0104663\n",
      "[190]\tvalid's binary_logloss: 0.0106085\n",
      "[200]\tvalid's binary_logloss: 0.0105117\n",
      "[210]\tvalid's binary_logloss: 0.0106258\n",
      "Early stopping, best iteration is:\n",
      "[180]\tvalid's binary_logloss: 0.0104663\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  46.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.106244\n",
      "[20]\tvalid's binary_logloss: 0.0641992\n",
      "[30]\tvalid's binary_logloss: 0.0444306\n",
      "[40]\tvalid's binary_logloss: 0.0328603\n",
      "[50]\tvalid's binary_logloss: 0.0258487\n",
      "[60]\tvalid's binary_logloss: 0.0219757\n",
      "[70]\tvalid's binary_logloss: 0.0192139\n",
      "[80]\tvalid's binary_logloss: 0.0170575\n",
      "[90]\tvalid's binary_logloss: 0.0155943\n",
      "[100]\tvalid's binary_logloss: 0.0146334\n",
      "[110]\tvalid's binary_logloss: 0.0140324\n",
      "[120]\tvalid's binary_logloss: 0.0136262\n",
      "[130]\tvalid's binary_logloss: 0.0135501\n",
      "[140]\tvalid's binary_logloss: 0.0136438\n",
      "[150]\tvalid's binary_logloss: 0.0137461\n",
      "[160]\tvalid's binary_logloss: 0.0140524\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid's binary_logloss: 0.0135288\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  43.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.103877\n",
      "[20]\tvalid's binary_logloss: 0.0626593\n",
      "[30]\tvalid's binary_logloss: 0.042861\n",
      "[40]\tvalid's binary_logloss: 0.0308303\n",
      "[50]\tvalid's binary_logloss: 0.0238472\n",
      "[60]\tvalid's binary_logloss: 0.0192712\n",
      "[70]\tvalid's binary_logloss: 0.0163761\n",
      "[80]\tvalid's binary_logloss: 0.0143597\n",
      "[90]\tvalid's binary_logloss: 0.0129875\n",
      "[100]\tvalid's binary_logloss: 0.0115306\n",
      "[110]\tvalid's binary_logloss: 0.0109007\n",
      "[120]\tvalid's binary_logloss: 0.0101833\n",
      "[130]\tvalid's binary_logloss: 0.00939737\n",
      "[140]\tvalid's binary_logloss: 0.00902941\n",
      "[150]\tvalid's binary_logloss: 0.00893926\n",
      "[160]\tvalid's binary_logloss: 0.00872862\n",
      "[170]\tvalid's binary_logloss: 0.00859869\n",
      "[180]\tvalid's binary_logloss: 0.00860693\n",
      "[190]\tvalid's binary_logloss: 0.00877435\n",
      "[200]\tvalid's binary_logloss: 0.00904169\n",
      "Early stopping, best iteration is:\n",
      "[170]\tvalid's binary_logloss: 0.00859869\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  45.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105402\n",
      "[20]\tvalid's binary_logloss: 0.0637107\n",
      "[30]\tvalid's binary_logloss: 0.0441163\n",
      "[40]\tvalid's binary_logloss: 0.0327957\n",
      "[50]\tvalid's binary_logloss: 0.0261293\n",
      "[60]\tvalid's binary_logloss: 0.0216882\n",
      "[70]\tvalid's binary_logloss: 0.0185559\n",
      "[80]\tvalid's binary_logloss: 0.0163256\n",
      "[90]\tvalid's binary_logloss: 0.0147613\n",
      "[100]\tvalid's binary_logloss: 0.013434\n",
      "[110]\tvalid's binary_logloss: 0.0125084\n",
      "[120]\tvalid's binary_logloss: 0.0119322\n",
      "[130]\tvalid's binary_logloss: 0.0114275\n",
      "[140]\tvalid's binary_logloss: 0.0108397\n",
      "[150]\tvalid's binary_logloss: 0.0107366\n",
      "[160]\tvalid's binary_logloss: 0.0104506\n",
      "[170]\tvalid's binary_logloss: 0.0102861\n",
      "[180]\tvalid's binary_logloss: 0.0102196\n",
      "[190]\tvalid's binary_logloss: 0.0101319\n",
      "[200]\tvalid's binary_logloss: 0.0100741\n",
      "[210]\tvalid's binary_logloss: 0.010062\n",
      "[220]\tvalid's binary_logloss: 0.00998625\n",
      "[230]\tvalid's binary_logloss: 0.0099825\n",
      "[240]\tvalid's binary_logloss: 0.00996906\n",
      "[250]\tvalid's binary_logloss: 0.00995443\n",
      "[260]\tvalid's binary_logloss: 0.00994827\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid's binary_logloss: 0.00993362\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  46.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.10869\n",
      "[20]\tvalid's binary_logloss: 0.0668116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalid's binary_logloss: 0.0475331\n",
      "[40]\tvalid's binary_logloss: 0.0361981\n",
      "[50]\tvalid's binary_logloss: 0.0291257\n",
      "[60]\tvalid's binary_logloss: 0.0242729\n",
      "[70]\tvalid's binary_logloss: 0.021108\n",
      "[80]\tvalid's binary_logloss: 0.0181725\n",
      "[90]\tvalid's binary_logloss: 0.0159832\n",
      "[100]\tvalid's binary_logloss: 0.0146482\n",
      "[110]\tvalid's binary_logloss: 0.0134766\n",
      "[120]\tvalid's binary_logloss: 0.0125574\n",
      "[130]\tvalid's binary_logloss: 0.0119916\n",
      "[140]\tvalid's binary_logloss: 0.0114239\n",
      "[150]\tvalid's binary_logloss: 0.0110395\n",
      "[160]\tvalid's binary_logloss: 0.0108293\n",
      "[170]\tvalid's binary_logloss: 0.0105654\n",
      "[180]\tvalid's binary_logloss: 0.0103841\n",
      "[190]\tvalid's binary_logloss: 0.0102782\n",
      "[200]\tvalid's binary_logloss: 0.0101514\n",
      "[210]\tvalid's binary_logloss: 0.00996005\n",
      "[220]\tvalid's binary_logloss: 0.00986726\n",
      "[230]\tvalid's binary_logloss: 0.00974528\n",
      "[240]\tvalid's binary_logloss: 0.00974271\n",
      "[250]\tvalid's binary_logloss: 0.00967982\n",
      "[260]\tvalid's binary_logloss: 0.00959037\n",
      "[270]\tvalid's binary_logloss: 0.00947986\n",
      "[280]\tvalid's binary_logloss: 0.00943935\n",
      "[290]\tvalid's binary_logloss: 0.00939352\n",
      "[300]\tvalid's binary_logloss: 0.00938363\n",
      "[310]\tvalid's binary_logloss: 0.00932688\n",
      "[320]\tvalid's binary_logloss: 0.00929186\n",
      "[330]\tvalid's binary_logloss: 0.00929904\n",
      "[340]\tvalid's binary_logloss: 0.00925993\n",
      "[350]\tvalid's binary_logloss: 0.00920759\n",
      "[360]\tvalid's binary_logloss: 0.00920123\n",
      "[370]\tvalid's binary_logloss: 0.00919729\n",
      "[380]\tvalid's binary_logloss: 0.00917266\n",
      "[390]\tvalid's binary_logloss: 0.00917508\n",
      "[400]\tvalid's binary_logloss: 0.00913585\n",
      "[410]\tvalid's binary_logloss: 0.00912738\n",
      "[420]\tvalid's binary_logloss: 0.00912283\n",
      "[430]\tvalid's binary_logloss: 0.00910591\n",
      "[440]\tvalid's binary_logloss: 0.00910413\n",
      "[450]\tvalid's binary_logloss: 0.00910395\n",
      "[460]\tvalid's binary_logloss: 0.00910385\n",
      "Early stopping, best iteration is:\n",
      "[436]\tvalid's binary_logloss: 0.00910289\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  50.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.109109\n",
      "[20]\tvalid's binary_logloss: 0.0672676\n",
      "[30]\tvalid's binary_logloss: 0.0476093\n",
      "[40]\tvalid's binary_logloss: 0.0358905\n",
      "[50]\tvalid's binary_logloss: 0.0285521\n",
      "[60]\tvalid's binary_logloss: 0.0238874\n",
      "[70]\tvalid's binary_logloss: 0.0206176\n",
      "[80]\tvalid's binary_logloss: 0.0186732\n",
      "[90]\tvalid's binary_logloss: 0.0173618\n",
      "[100]\tvalid's binary_logloss: 0.0162628\n",
      "[110]\tvalid's binary_logloss: 0.0155634\n",
      "[120]\tvalid's binary_logloss: 0.015166\n",
      "[130]\tvalid's binary_logloss: 0.0147343\n",
      "[140]\tvalid's binary_logloss: 0.0145307\n",
      "[150]\tvalid's binary_logloss: 0.0143597\n",
      "[160]\tvalid's binary_logloss: 0.0142849\n",
      "[170]\tvalid's binary_logloss: 0.0142546\n",
      "[180]\tvalid's binary_logloss: 0.0141815\n",
      "[190]\tvalid's binary_logloss: 0.0141349\n",
      "[200]\tvalid's binary_logloss: 0.0141874\n",
      "[210]\tvalid's binary_logloss: 0.0141289\n",
      "[220]\tvalid's binary_logloss: 0.0140992\n",
      "[230]\tvalid's binary_logloss: 0.0140921\n",
      "[240]\tvalid's binary_logloss: 0.0140917\n",
      "[250]\tvalid's binary_logloss: 0.0140785\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid's binary_logloss: 0.0140524\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  48.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.109522\n",
      "[20]\tvalid's binary_logloss: 0.0671603\n",
      "[30]\tvalid's binary_logloss: 0.0474713\n",
      "[40]\tvalid's binary_logloss: 0.0342015\n",
      "[50]\tvalid's binary_logloss: 0.0272037\n",
      "[60]\tvalid's binary_logloss: 0.0219822\n",
      "[70]\tvalid's binary_logloss: 0.0183311\n",
      "[80]\tvalid's binary_logloss: 0.0159279\n",
      "[90]\tvalid's binary_logloss: 0.0138834\n",
      "[100]\tvalid's binary_logloss: 0.012629\n",
      "[110]\tvalid's binary_logloss: 0.0115336\n",
      "[120]\tvalid's binary_logloss: 0.0108353\n",
      "[130]\tvalid's binary_logloss: 0.0102461\n",
      "[140]\tvalid's binary_logloss: 0.00976592\n",
      "[150]\tvalid's binary_logloss: 0.00940867\n",
      "[160]\tvalid's binary_logloss: 0.00918258\n",
      "[170]\tvalid's binary_logloss: 0.00898923\n",
      "[180]\tvalid's binary_logloss: 0.00877394\n",
      "[190]\tvalid's binary_logloss: 0.00856944\n",
      "[200]\tvalid's binary_logloss: 0.00846448\n",
      "[210]\tvalid's binary_logloss: 0.008384\n",
      "[220]\tvalid's binary_logloss: 0.00828709\n",
      "[230]\tvalid's binary_logloss: 0.00825024\n",
      "[240]\tvalid's binary_logloss: 0.00816006\n",
      "[250]\tvalid's binary_logloss: 0.00807546\n",
      "[260]\tvalid's binary_logloss: 0.00801484\n",
      "[270]\tvalid's binary_logloss: 0.0079977\n",
      "[280]\tvalid's binary_logloss: 0.00794342\n",
      "[290]\tvalid's binary_logloss: 0.0079156\n",
      "[300]\tvalid's binary_logloss: 0.00788788\n",
      "[310]\tvalid's binary_logloss: 0.00788281\n",
      "[320]\tvalid's binary_logloss: 0.00789617\n",
      "[330]\tvalid's binary_logloss: 0.00788187\n",
      "[340]\tvalid's binary_logloss: 0.00786271\n",
      "[350]\tvalid's binary_logloss: 0.00785412\n",
      "[360]\tvalid's binary_logloss: 0.0078348\n",
      "[370]\tvalid's binary_logloss: 0.00786589\n",
      "[380]\tvalid's binary_logloss: 0.00782069\n",
      "[390]\tvalid's binary_logloss: 0.00782201\n",
      "[400]\tvalid's binary_logloss: 0.00780834\n",
      "[410]\tvalid's binary_logloss: 0.00779289\n",
      "[420]\tvalid's binary_logloss: 0.00778177\n",
      "[430]\tvalid's binary_logloss: 0.00777264\n",
      "[440]\tvalid's binary_logloss: 0.00777648\n",
      "[450]\tvalid's binary_logloss: 0.00777648\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalid's binary_logloss: 0.00777112\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  53.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.104236\n",
      "[20]\tvalid's binary_logloss: 0.0621943\n",
      "[30]\tvalid's binary_logloss: 0.0421176\n",
      "[40]\tvalid's binary_logloss: 0.0314421\n",
      "[50]\tvalid's binary_logloss: 0.0240942\n",
      "[60]\tvalid's binary_logloss: 0.02\n",
      "[70]\tvalid's binary_logloss: 0.0171374\n",
      "[80]\tvalid's binary_logloss: 0.0152168\n",
      "[90]\tvalid's binary_logloss: 0.0140418\n",
      "[100]\tvalid's binary_logloss: 0.0129109\n",
      "[110]\tvalid's binary_logloss: 0.0123042\n",
      "[120]\tvalid's binary_logloss: 0.0118722\n",
      "[130]\tvalid's binary_logloss: 0.0114726\n",
      "[140]\tvalid's binary_logloss: 0.0108827\n",
      "[150]\tvalid's binary_logloss: 0.010671\n",
      "[160]\tvalid's binary_logloss: 0.010762\n",
      "[170]\tvalid's binary_logloss: 0.0106864\n",
      "[180]\tvalid's binary_logloss: 0.0105873\n",
      "[190]\tvalid's binary_logloss: 0.0105303\n",
      "[200]\tvalid's binary_logloss: 0.010483\n",
      "[210]\tvalid's binary_logloss: 0.0105228\n",
      "[220]\tvalid's binary_logloss: 0.010514\n",
      "[230]\tvalid's binary_logloss: 0.010544\n",
      "Early stopping, best iteration is:\n",
      "[202]\tvalid's binary_logloss: 0.0104714\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9243697478991596, total=  50.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105025\n",
      "[20]\tvalid's binary_logloss: 0.0624325\n",
      "[30]\tvalid's binary_logloss: 0.0429629\n",
      "[40]\tvalid's binary_logloss: 0.032295\n",
      "[50]\tvalid's binary_logloss: 0.0260916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60]\tvalid's binary_logloss: 0.0221975\n",
      "[70]\tvalid's binary_logloss: 0.0190747\n",
      "[80]\tvalid's binary_logloss: 0.0172067\n",
      "[90]\tvalid's binary_logloss: 0.0152815\n",
      "[100]\tvalid's binary_logloss: 0.0139322\n",
      "[110]\tvalid's binary_logloss: 0.0129463\n",
      "[120]\tvalid's binary_logloss: 0.0120926\n",
      "[130]\tvalid's binary_logloss: 0.011658\n",
      "[140]\tvalid's binary_logloss: 0.0112719\n",
      "[150]\tvalid's binary_logloss: 0.0110219\n",
      "[160]\tvalid's binary_logloss: 0.0107245\n",
      "[170]\tvalid's binary_logloss: 0.0105287\n",
      "[180]\tvalid's binary_logloss: 0.0105008\n",
      "[190]\tvalid's binary_logloss: 0.0103936\n",
      "[200]\tvalid's binary_logloss: 0.0103401\n",
      "[210]\tvalid's binary_logloss: 0.0103094\n",
      "[220]\tvalid's binary_logloss: 0.0102439\n",
      "[230]\tvalid's binary_logloss: 0.0102136\n",
      "[240]\tvalid's binary_logloss: 0.0101844\n",
      "[250]\tvalid's binary_logloss: 0.0101786\n",
      "[260]\tvalid's binary_logloss: 0.0101786\n",
      "Early stopping, best iteration is:\n",
      "[238]\tvalid's binary_logloss: 0.0101567\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  50.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105467\n",
      "[20]\tvalid's binary_logloss: 0.0639552\n",
      "[30]\tvalid's binary_logloss: 0.0441868\n",
      "[40]\tvalid's binary_logloss: 0.0336782\n",
      "[50]\tvalid's binary_logloss: 0.0261812\n",
      "[60]\tvalid's binary_logloss: 0.0219006\n",
      "[70]\tvalid's binary_logloss: 0.0189318\n",
      "[80]\tvalid's binary_logloss: 0.0170567\n",
      "[90]\tvalid's binary_logloss: 0.0157642\n",
      "[100]\tvalid's binary_logloss: 0.0149296\n",
      "[110]\tvalid's binary_logloss: 0.0143259\n",
      "[120]\tvalid's binary_logloss: 0.0141404\n",
      "[130]\tvalid's binary_logloss: 0.0139556\n",
      "[140]\tvalid's binary_logloss: 0.0139998\n",
      "[150]\tvalid's binary_logloss: 0.0138837\n",
      "[160]\tvalid's binary_logloss: 0.0138824\n",
      "[170]\tvalid's binary_logloss: 0.0138623\n",
      "[180]\tvalid's binary_logloss: 0.0139068\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid's binary_logloss: 0.0138295\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.8947368421052632, total=  42.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105214\n",
      "[20]\tvalid's binary_logloss: 0.0641103\n",
      "[30]\tvalid's binary_logloss: 0.0438163\n",
      "[40]\tvalid's binary_logloss: 0.0315665\n",
      "[50]\tvalid's binary_logloss: 0.0247837\n",
      "[60]\tvalid's binary_logloss: 0.0199875\n",
      "[70]\tvalid's binary_logloss: 0.0169589\n",
      "[80]\tvalid's binary_logloss: 0.0150651\n",
      "[90]\tvalid's binary_logloss: 0.0131812\n",
      "[100]\tvalid's binary_logloss: 0.0119113\n",
      "[110]\tvalid's binary_logloss: 0.0110215\n",
      "[120]\tvalid's binary_logloss: 0.0102952\n",
      "[130]\tvalid's binary_logloss: 0.00970525\n",
      "[140]\tvalid's binary_logloss: 0.00936956\n",
      "[150]\tvalid's binary_logloss: 0.0089747\n",
      "[160]\tvalid's binary_logloss: 0.00872207\n",
      "[170]\tvalid's binary_logloss: 0.00879511\n",
      "[180]\tvalid's binary_logloss: 0.00871814\n",
      "[190]\tvalid's binary_logloss: 0.00862583\n",
      "[200]\tvalid's binary_logloss: 0.00862792\n",
      "[210]\tvalid's binary_logloss: 0.00858166\n",
      "[220]\tvalid's binary_logloss: 0.00861449\n",
      "[230]\tvalid's binary_logloss: 0.0085606\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid's binary_logloss: 0.00853928\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.923076923076923, total=  44.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.103867\n",
      "[20]\tvalid's binary_logloss: 0.0621888\n",
      "[30]\tvalid's binary_logloss: 0.0421255\n",
      "[40]\tvalid's binary_logloss: 0.0313438\n",
      "[50]\tvalid's binary_logloss: 0.0249743\n",
      "[60]\tvalid's binary_logloss: 0.0207917\n",
      "[70]\tvalid's binary_logloss: 0.0180242\n",
      "[80]\tvalid's binary_logloss: 0.0156794\n",
      "[90]\tvalid's binary_logloss: 0.0141928\n",
      "[100]\tvalid's binary_logloss: 0.0130816\n",
      "[110]\tvalid's binary_logloss: 0.0122489\n",
      "[120]\tvalid's binary_logloss: 0.011709\n",
      "[130]\tvalid's binary_logloss: 0.0112775\n",
      "[140]\tvalid's binary_logloss: 0.0112097\n",
      "[150]\tvalid's binary_logloss: 0.0111554\n",
      "[160]\tvalid's binary_logloss: 0.0109929\n",
      "[170]\tvalid's binary_logloss: 0.0110387\n",
      "[180]\tvalid's binary_logloss: 0.0110157\n",
      "[190]\tvalid's binary_logloss: 0.011198\n",
      "[200]\tvalid's binary_logloss: 0.0111097\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid's binary_logloss: 0.0109535\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9152542372881356, total=  44.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105278\n",
      "[20]\tvalid's binary_logloss: 0.0625434\n",
      "[30]\tvalid's binary_logloss: 0.043132\n",
      "[40]\tvalid's binary_logloss: 0.0333392\n",
      "[50]\tvalid's binary_logloss: 0.0260549\n",
      "[60]\tvalid's binary_logloss: 0.0217457\n",
      "[70]\tvalid's binary_logloss: 0.0186995\n",
      "[80]\tvalid's binary_logloss: 0.0160965\n",
      "[90]\tvalid's binary_logloss: 0.0142165\n",
      "[100]\tvalid's binary_logloss: 0.0131518\n",
      "[110]\tvalid's binary_logloss: 0.0121506\n",
      "[120]\tvalid's binary_logloss: 0.011207\n",
      "[130]\tvalid's binary_logloss: 0.0107442\n",
      "[140]\tvalid's binary_logloss: 0.0105319\n",
      "[150]\tvalid's binary_logloss: 0.0102157\n",
      "[160]\tvalid's binary_logloss: 0.00996698\n",
      "[170]\tvalid's binary_logloss: 0.00991044\n",
      "[180]\tvalid's binary_logloss: 0.00971893\n",
      "[190]\tvalid's binary_logloss: 0.00973769\n",
      "[200]\tvalid's binary_logloss: 0.00962952\n",
      "[210]\tvalid's binary_logloss: 0.00954575\n",
      "[220]\tvalid's binary_logloss: 0.00950127\n",
      "[230]\tvalid's binary_logloss: 0.00949896\n",
      "[240]\tvalid's binary_logloss: 0.00949896\n",
      "Early stopping, best iteration is:\n",
      "[219]\tvalid's binary_logloss: 0.00949166\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9411764705882353, total=  44.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105628\n",
      "[20]\tvalid's binary_logloss: 0.0642127\n",
      "[30]\tvalid's binary_logloss: 0.0445031\n",
      "[40]\tvalid's binary_logloss: 0.0327512\n",
      "[50]\tvalid's binary_logloss: 0.025844\n",
      "[60]\tvalid's binary_logloss: 0.0213218\n",
      "[70]\tvalid's binary_logloss: 0.0185541\n",
      "[80]\tvalid's binary_logloss: 0.0167364\n",
      "[90]\tvalid's binary_logloss: 0.0152023\n",
      "[100]\tvalid's binary_logloss: 0.0143267\n",
      "[110]\tvalid's binary_logloss: 0.0139999\n",
      "[120]\tvalid's binary_logloss: 0.0137595\n",
      "[130]\tvalid's binary_logloss: 0.0134968\n",
      "[140]\tvalid's binary_logloss: 0.0133995\n",
      "[150]\tvalid's binary_logloss: 0.0133898\n",
      "[160]\tvalid's binary_logloss: 0.0134109\n",
      "[170]\tvalid's binary_logloss: 0.0135475\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid's binary_logloss: 0.0133567\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.8947368421052632, total=  42.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.103784\n",
      "[20]\tvalid's binary_logloss: 0.0628516\n",
      "[30]\tvalid's binary_logloss: 0.0433051\n",
      "[40]\tvalid's binary_logloss: 0.0311816\n",
      "[50]\tvalid's binary_logloss: 0.0243108\n",
      "[60]\tvalid's binary_logloss: 0.0197721\n",
      "[70]\tvalid's binary_logloss: 0.0165249\n",
      "[80]\tvalid's binary_logloss: 0.0144207\n",
      "[90]\tvalid's binary_logloss: 0.0128558\n",
      "[100]\tvalid's binary_logloss: 0.0116563\n",
      "[110]\tvalid's binary_logloss: 0.010876\n",
      "[120]\tvalid's binary_logloss: 0.0104144\n",
      "[130]\tvalid's binary_logloss: 0.00985106\n",
      "[140]\tvalid's binary_logloss: 0.00935447\n",
      "[150]\tvalid's binary_logloss: 0.00898322\n",
      "[160]\tvalid's binary_logloss: 0.00878228\n",
      "[170]\tvalid's binary_logloss: 0.00858587\n",
      "[180]\tvalid's binary_logloss: 0.00861058\n",
      "[190]\tvalid's binary_logloss: 0.00858813\n",
      "[200]\tvalid's binary_logloss: 0.00858622\n",
      "[210]\tvalid's binary_logloss: 0.00849273\n",
      "[220]\tvalid's binary_logloss: 0.00853743\n",
      "[230]\tvalid's binary_logloss: 0.00854105\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid's binary_logloss: 0.00848469\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  44.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.108773\n",
      "[20]\tvalid's binary_logloss: 0.0671537\n",
      "[30]\tvalid's binary_logloss: 0.0470154\n",
      "[40]\tvalid's binary_logloss: 0.0362742\n",
      "[50]\tvalid's binary_logloss: 0.0297902\n",
      "[60]\tvalid's binary_logloss: 0.0251553\n",
      "[70]\tvalid's binary_logloss: 0.0222214\n",
      "[80]\tvalid's binary_logloss: 0.0200956\n",
      "[90]\tvalid's binary_logloss: 0.0181506\n",
      "[100]\tvalid's binary_logloss: 0.016747\n",
      "[110]\tvalid's binary_logloss: 0.015682\n",
      "[120]\tvalid's binary_logloss: 0.0148516\n",
      "[130]\tvalid's binary_logloss: 0.0144012\n",
      "[140]\tvalid's binary_logloss: 0.0138481\n",
      "[150]\tvalid's binary_logloss: 0.0134804\n",
      "[160]\tvalid's binary_logloss: 0.0131931\n",
      "[170]\tvalid's binary_logloss: 0.0131664\n",
      "[180]\tvalid's binary_logloss: 0.0131664\n",
      "[190]\tvalid's binary_logloss: 0.0131664\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid's binary_logloss: 0.0131561\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.923076923076923, total=  42.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.110331\n",
      "[20]\tvalid's binary_logloss: 0.0677529\n",
      "[30]\tvalid's binary_logloss: 0.0480082\n",
      "[40]\tvalid's binary_logloss: 0.0377166\n",
      "[50]\tvalid's binary_logloss: 0.0311004\n",
      "[60]\tvalid's binary_logloss: 0.0263283\n",
      "[70]\tvalid's binary_logloss: 0.0230037\n",
      "[80]\tvalid's binary_logloss: 0.0203579\n",
      "[90]\tvalid's binary_logloss: 0.018337\n",
      "[100]\tvalid's binary_logloss: 0.0169335\n",
      "[110]\tvalid's binary_logloss: 0.0159892\n",
      "[120]\tvalid's binary_logloss: 0.0151811\n",
      "[130]\tvalid's binary_logloss: 0.0145804\n",
      "[140]\tvalid's binary_logloss: 0.0140676\n",
      "[150]\tvalid's binary_logloss: 0.0136231\n",
      "[160]\tvalid's binary_logloss: 0.0133057\n",
      "[170]\tvalid's binary_logloss: 0.0132803\n",
      "[180]\tvalid's binary_logloss: 0.0132803\n",
      "[190]\tvalid's binary_logloss: 0.0132803\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid's binary_logloss: 0.0132803\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9152542372881356, total=  41.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.110583\n",
      "[20]\tvalid's binary_logloss: 0.0687046\n",
      "[30]\tvalid's binary_logloss: 0.0497556\n",
      "[40]\tvalid's binary_logloss: 0.0381143\n",
      "[50]\tvalid's binary_logloss: 0.0311464\n",
      "[60]\tvalid's binary_logloss: 0.0265839\n",
      "[70]\tvalid's binary_logloss: 0.0236219\n",
      "[80]\tvalid's binary_logloss: 0.0214025\n",
      "[90]\tvalid's binary_logloss: 0.0196905\n",
      "[100]\tvalid's binary_logloss: 0.018568\n",
      "[110]\tvalid's binary_logloss: 0.0177502\n",
      "[120]\tvalid's binary_logloss: 0.0170255\n",
      "[130]\tvalid's binary_logloss: 0.0166075\n",
      "[140]\tvalid's binary_logloss: 0.0162486\n",
      "[150]\tvalid's binary_logloss: 0.0160535\n",
      "[160]\tvalid's binary_logloss: 0.0158476\n",
      "[170]\tvalid's binary_logloss: 0.0158476\n",
      "[180]\tvalid's binary_logloss: 0.0158476\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid's binary_logloss: 0.0158472\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  42.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.110713\n",
      "[20]\tvalid's binary_logloss: 0.0685966\n",
      "[30]\tvalid's binary_logloss: 0.0492621\n",
      "[40]\tvalid's binary_logloss: 0.0373524\n",
      "[50]\tvalid's binary_logloss: 0.0303208\n",
      "[60]\tvalid's binary_logloss: 0.0252708\n",
      "[70]\tvalid's binary_logloss: 0.0218126\n",
      "[80]\tvalid's binary_logloss: 0.0191783\n",
      "[90]\tvalid's binary_logloss: 0.0172794\n",
      "[100]\tvalid's binary_logloss: 0.0158893\n",
      "[110]\tvalid's binary_logloss: 0.0147904\n",
      "[120]\tvalid's binary_logloss: 0.0138817\n",
      "[130]\tvalid's binary_logloss: 0.0131382\n",
      "[140]\tvalid's binary_logloss: 0.0126031\n",
      "[150]\tvalid's binary_logloss: 0.0120814\n",
      "[160]\tvalid's binary_logloss: 0.011814\n",
      "[170]\tvalid's binary_logloss: 0.0116958\n",
      "[180]\tvalid's binary_logloss: 0.0116959\n",
      "[190]\tvalid's binary_logloss: 0.0116959\n",
      "Early stopping, best iteration is:\n",
      "[164]\tvalid's binary_logloss: 0.0116957\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9243697478991596, total=  42.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.104393\n",
      "[20]\tvalid's binary_logloss: 0.0638198\n",
      "[30]\tvalid's binary_logloss: 0.0438652\n",
      "[40]\tvalid's binary_logloss: 0.0339155\n",
      "[50]\tvalid's binary_logloss: 0.027732\n",
      "[60]\tvalid's binary_logloss: 0.0237592\n",
      "[70]\tvalid's binary_logloss: 0.0204336\n",
      "[80]\tvalid's binary_logloss: 0.0180311\n",
      "[90]\tvalid's binary_logloss: 0.0165189\n",
      "[100]\tvalid's binary_logloss: 0.0153091\n",
      "[110]\tvalid's binary_logloss: 0.0145361\n",
      "[120]\tvalid's binary_logloss: 0.0137665\n",
      "[130]\tvalid's binary_logloss: 0.0134122\n",
      "[140]\tvalid's binary_logloss: 0.0130873\n",
      "[150]\tvalid's binary_logloss: 0.0130873\n",
      "[160]\tvalid's binary_logloss: 0.0130873\n",
      "[170]\tvalid's binary_logloss: 0.0130873\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid's binary_logloss: 0.0130873\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  41.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.107846\n",
      "[20]\tvalid's binary_logloss: 0.0657116\n",
      "[30]\tvalid's binary_logloss: 0.0464698\n",
      "[40]\tvalid's binary_logloss: 0.0357557\n",
      "[50]\tvalid's binary_logloss: 0.0293894\n",
      "[60]\tvalid's binary_logloss: 0.0243697\n",
      "[70]\tvalid's binary_logloss: 0.0207867\n",
      "[80]\tvalid's binary_logloss: 0.0184801\n",
      "[90]\tvalid's binary_logloss: 0.0167337\n",
      "[100]\tvalid's binary_logloss: 0.0154642\n",
      "[110]\tvalid's binary_logloss: 0.0144626\n",
      "[120]\tvalid's binary_logloss: 0.0136353\n",
      "[130]\tvalid's binary_logloss: 0.0131235\n",
      "[140]\tvalid's binary_logloss: 0.0128886\n",
      "[150]\tvalid's binary_logloss: 0.0128628\n",
      "[160]\tvalid's binary_logloss: 0.0128628\n",
      "[170]\tvalid's binary_logloss: 0.0128628\n",
      "Early stopping, best iteration is:\n",
      "[141]\tvalid's binary_logloss: 0.0128613\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.923076923076923, total=  41.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.107223\n",
      "[20]\tvalid's binary_logloss: 0.0665617\n",
      "[30]\tvalid's binary_logloss: 0.0471261\n",
      "[40]\tvalid's binary_logloss: 0.0363795\n",
      "[50]\tvalid's binary_logloss: 0.0293577\n",
      "[60]\tvalid's binary_logloss: 0.02532\n",
      "[70]\tvalid's binary_logloss: 0.0221706\n",
      "[80]\tvalid's binary_logloss: 0.0199175\n",
      "[90]\tvalid's binary_logloss: 0.0182635\n",
      "[100]\tvalid's binary_logloss: 0.0173776\n",
      "[110]\tvalid's binary_logloss: 0.0165926\n",
      "[120]\tvalid's binary_logloss: 0.0160784\n",
      "[130]\tvalid's binary_logloss: 0.0156974\n",
      "[140]\tvalid's binary_logloss: 0.0155937\n",
      "[150]\tvalid's binary_logloss: 0.0155935\n",
      "[160]\tvalid's binary_logloss: 0.0155933\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid's binary_logloss: 0.0155874\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9043478260869565, total=  41.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.106563\n",
      "[20]\tvalid's binary_logloss: 0.066306\n",
      "[30]\tvalid's binary_logloss: 0.0465082\n",
      "[40]\tvalid's binary_logloss: 0.0348962\n",
      "[50]\tvalid's binary_logloss: 0.0283762\n",
      "[60]\tvalid's binary_logloss: 0.0236916\n",
      "[70]\tvalid's binary_logloss: 0.0202501\n",
      "[80]\tvalid's binary_logloss: 0.0177053\n",
      "[90]\tvalid's binary_logloss: 0.0160001\n",
      "[100]\tvalid's binary_logloss: 0.0145489\n",
      "[110]\tvalid's binary_logloss: 0.0135572\n",
      "[120]\tvalid's binary_logloss: 0.0127758\n",
      "[130]\tvalid's binary_logloss: 0.0122089\n",
      "[140]\tvalid's binary_logloss: 0.0119301\n",
      "[150]\tvalid's binary_logloss: 0.0119301\n",
      "[160]\tvalid's binary_logloss: 0.0119301\n",
      "[170]\tvalid's binary_logloss: 0.0119301\n",
      "Early stopping, best iteration is:\n",
      "[141]\tvalid's binary_logloss: 0.0119301\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  41.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.104553\n",
      "[20]\tvalid's binary_logloss: 0.0641223\n",
      "[30]\tvalid's binary_logloss: 0.0445775\n",
      "[40]\tvalid's binary_logloss: 0.0340985\n",
      "[50]\tvalid's binary_logloss: 0.0276539\n",
      "[60]\tvalid's binary_logloss: 0.0234721\n",
      "[70]\tvalid's binary_logloss: 0.0202487\n",
      "[80]\tvalid's binary_logloss: 0.0179621\n",
      "[90]\tvalid's binary_logloss: 0.0161029\n",
      "[100]\tvalid's binary_logloss: 0.0149027\n",
      "[110]\tvalid's binary_logloss: 0.0140906\n",
      "[120]\tvalid's binary_logloss: 0.0133135\n",
      "[130]\tvalid's binary_logloss: 0.012903\n",
      "[140]\tvalid's binary_logloss: 0.012645\n",
      "[150]\tvalid's binary_logloss: 0.012645\n",
      "[160]\tvalid's binary_logloss: 0.012645\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid's binary_logloss: 0.01263\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  42.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.107525\n",
      "[20]\tvalid's binary_logloss: 0.0654114\n",
      "[30]\tvalid's binary_logloss: 0.0449262\n",
      "[40]\tvalid's binary_logloss: 0.034442\n",
      "[50]\tvalid's binary_logloss: 0.0280405\n",
      "[60]\tvalid's binary_logloss: 0.0239741\n",
      "[70]\tvalid's binary_logloss: 0.0208156\n",
      "[80]\tvalid's binary_logloss: 0.0182959\n",
      "[90]\tvalid's binary_logloss: 0.0163328\n",
      "[100]\tvalid's binary_logloss: 0.0151082\n",
      "[110]\tvalid's binary_logloss: 0.0142365\n",
      "[120]\tvalid's binary_logloss: 0.0136786\n",
      "[130]\tvalid's binary_logloss: 0.0132103\n",
      "[140]\tvalid's binary_logloss: 0.0129845\n",
      "[150]\tvalid's binary_logloss: 0.0129851\n",
      "[160]\tvalid's binary_logloss: 0.0129853\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid's binary_logloss: 0.0129844\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  42.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.106788\n",
      "[20]\tvalid's binary_logloss: 0.0668509\n",
      "[30]\tvalid's binary_logloss: 0.047338\n",
      "[40]\tvalid's binary_logloss: 0.0361531\n",
      "[50]\tvalid's binary_logloss: 0.0287867\n",
      "[60]\tvalid's binary_logloss: 0.0245361\n",
      "[70]\tvalid's binary_logloss: 0.0217258\n",
      "[80]\tvalid's binary_logloss: 0.0197345\n",
      "[90]\tvalid's binary_logloss: 0.0181669\n",
      "[100]\tvalid's binary_logloss: 0.0170939\n",
      "[110]\tvalid's binary_logloss: 0.0165261\n",
      "[120]\tvalid's binary_logloss: 0.0159821\n",
      "[130]\tvalid's binary_logloss: 0.0156589\n",
      "[140]\tvalid's binary_logloss: 0.0155159\n",
      "[150]\tvalid's binary_logloss: 0.0155159\n",
      "[160]\tvalid's binary_logloss: 0.0155159\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid's binary_logloss: 0.0155159\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  42.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.106141\n",
      "[20]\tvalid's binary_logloss: 0.0659531\n",
      "[30]\tvalid's binary_logloss: 0.0464754\n",
      "[40]\tvalid's binary_logloss: 0.0355273\n",
      "[50]\tvalid's binary_logloss: 0.0287834\n",
      "[60]\tvalid's binary_logloss: 0.0238626\n",
      "[70]\tvalid's binary_logloss: 0.0203086\n",
      "[80]\tvalid's binary_logloss: 0.0175621\n",
      "[90]\tvalid's binary_logloss: 0.0157997\n",
      "[100]\tvalid's binary_logloss: 0.0145572\n",
      "[110]\tvalid's binary_logloss: 0.0135781\n",
      "[120]\tvalid's binary_logloss: 0.0128714\n",
      "[130]\tvalid's binary_logloss: 0.0122873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140]\tvalid's binary_logloss: 0.01204\n",
      "[150]\tvalid's binary_logloss: 0.01204\n",
      "[160]\tvalid's binary_logloss: 0.01204\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid's binary_logloss: 0.01204\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9137931034482758, total=  43.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105073\n",
      "[20]\tvalid's binary_logloss: 0.0634969\n",
      "[30]\tvalid's binary_logloss: 0.043598\n",
      "[40]\tvalid's binary_logloss: 0.0325657\n",
      "[50]\tvalid's binary_logloss: 0.0259775\n",
      "[60]\tvalid's binary_logloss: 0.0216568\n",
      "[70]\tvalid's binary_logloss: 0.0183566\n",
      "[80]\tvalid's binary_logloss: 0.0160596\n",
      "[90]\tvalid's binary_logloss: 0.014584\n",
      "[100]\tvalid's binary_logloss: 0.0135308\n",
      "[110]\tvalid's binary_logloss: 0.0127343\n",
      "[120]\tvalid's binary_logloss: 0.0122699\n",
      "[130]\tvalid's binary_logloss: 0.0118449\n",
      "[140]\tvalid's binary_logloss: 0.0114971\n",
      "[150]\tvalid's binary_logloss: 0.0112897\n",
      "[160]\tvalid's binary_logloss: 0.0110799\n",
      "[170]\tvalid's binary_logloss: 0.0109311\n",
      "[180]\tvalid's binary_logloss: 0.0107706\n",
      "[190]\tvalid's binary_logloss: 0.0105952\n",
      "[200]\tvalid's binary_logloss: 0.0106606\n",
      "[210]\tvalid's binary_logloss: 0.0106479\n",
      "[220]\tvalid's binary_logloss: 0.0106011\n",
      "[230]\tvalid's binary_logloss: 0.0105393\n",
      "[240]\tvalid's binary_logloss: 0.0105146\n",
      "[250]\tvalid's binary_logloss: 0.0104637\n",
      "[260]\tvalid's binary_logloss: 0.0105087\n",
      "[270]\tvalid's binary_logloss: 0.0104558\n",
      "[280]\tvalid's binary_logloss: 0.0105071\n",
      "[290]\tvalid's binary_logloss: 0.0105024\n",
      "Early stopping, best iteration is:\n",
      "[267]\tvalid's binary_logloss: 0.0104283\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  50.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.108514\n",
      "[20]\tvalid's binary_logloss: 0.0659098\n",
      "[30]\tvalid's binary_logloss: 0.0466444\n",
      "[40]\tvalid's binary_logloss: 0.0354487\n",
      "[50]\tvalid's binary_logloss: 0.0282533\n",
      "[60]\tvalid's binary_logloss: 0.0237543\n",
      "[70]\tvalid's binary_logloss: 0.0200102\n",
      "[80]\tvalid's binary_logloss: 0.0172922\n",
      "[90]\tvalid's binary_logloss: 0.0154309\n",
      "[100]\tvalid's binary_logloss: 0.0141211\n",
      "[110]\tvalid's binary_logloss: 0.0129775\n",
      "[120]\tvalid's binary_logloss: 0.0121296\n",
      "[130]\tvalid's binary_logloss: 0.0116095\n",
      "[140]\tvalid's binary_logloss: 0.0112809\n",
      "[150]\tvalid's binary_logloss: 0.0108221\n",
      "[160]\tvalid's binary_logloss: 0.0105229\n",
      "[170]\tvalid's binary_logloss: 0.0102484\n",
      "[180]\tvalid's binary_logloss: 0.0101004\n",
      "[190]\tvalid's binary_logloss: 0.00992344\n",
      "[200]\tvalid's binary_logloss: 0.00991031\n",
      "[210]\tvalid's binary_logloss: 0.00984618\n",
      "[220]\tvalid's binary_logloss: 0.0097944\n",
      "[230]\tvalid's binary_logloss: 0.00977616\n",
      "[240]\tvalid's binary_logloss: 0.00970219\n",
      "[250]\tvalid's binary_logloss: 0.00961072\n",
      "[260]\tvalid's binary_logloss: 0.00959773\n",
      "[270]\tvalid's binary_logloss: 0.00953399\n",
      "[280]\tvalid's binary_logloss: 0.00949878\n",
      "[290]\tvalid's binary_logloss: 0.00949019\n",
      "[300]\tvalid's binary_logloss: 0.00950097\n",
      "Early stopping, best iteration is:\n",
      "[279]\tvalid's binary_logloss: 0.00948442\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9411764705882353, total=  49.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.108911\n",
      "[20]\tvalid's binary_logloss: 0.0668806\n",
      "[30]\tvalid's binary_logloss: 0.0465771\n",
      "[40]\tvalid's binary_logloss: 0.0353627\n",
      "[50]\tvalid's binary_logloss: 0.0280265\n",
      "[60]\tvalid's binary_logloss: 0.0235674\n",
      "[70]\tvalid's binary_logloss: 0.0201826\n",
      "[80]\tvalid's binary_logloss: 0.0178578\n",
      "[90]\tvalid's binary_logloss: 0.0165988\n",
      "[100]\tvalid's binary_logloss: 0.0155225\n",
      "[110]\tvalid's binary_logloss: 0.0147456\n",
      "[120]\tvalid's binary_logloss: 0.014327\n",
      "[130]\tvalid's binary_logloss: 0.0139162\n",
      "[140]\tvalid's binary_logloss: 0.0138116\n",
      "[150]\tvalid's binary_logloss: 0.0136314\n",
      "[160]\tvalid's binary_logloss: 0.013631\n",
      "[170]\tvalid's binary_logloss: 0.0136222\n",
      "[180]\tvalid's binary_logloss: 0.0136329\n",
      "[190]\tvalid's binary_logloss: 0.0136771\n",
      "[200]\tvalid's binary_logloss: 0.0137022\n",
      "Early stopping, best iteration is:\n",
      "[173]\tvalid's binary_logloss: 0.0135902\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  45.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.109367\n",
      "[20]\tvalid's binary_logloss: 0.066934\n",
      "[30]\tvalid's binary_logloss: 0.0467975\n",
      "[40]\tvalid's binary_logloss: 0.0352754\n",
      "[50]\tvalid's binary_logloss: 0.0275737\n",
      "[60]\tvalid's binary_logloss: 0.0224697\n",
      "[70]\tvalid's binary_logloss: 0.0186838\n",
      "[80]\tvalid's binary_logloss: 0.0160403\n",
      "[90]\tvalid's binary_logloss: 0.0142614\n",
      "[100]\tvalid's binary_logloss: 0.0127755\n",
      "[110]\tvalid's binary_logloss: 0.0117657\n",
      "[120]\tvalid's binary_logloss: 0.0108088\n",
      "[130]\tvalid's binary_logloss: 0.0102188\n",
      "[140]\tvalid's binary_logloss: 0.0097797\n",
      "[150]\tvalid's binary_logloss: 0.00938666\n",
      "[160]\tvalid's binary_logloss: 0.00906686\n",
      "[170]\tvalid's binary_logloss: 0.00877187\n",
      "[180]\tvalid's binary_logloss: 0.00863268\n",
      "[190]\tvalid's binary_logloss: 0.0084416\n",
      "[200]\tvalid's binary_logloss: 0.00835473\n",
      "[210]\tvalid's binary_logloss: 0.00832613\n",
      "[220]\tvalid's binary_logloss: 0.0082392\n",
      "[230]\tvalid's binary_logloss: 0.008165\n",
      "[240]\tvalid's binary_logloss: 0.00813927\n",
      "[250]\tvalid's binary_logloss: 0.00811384\n",
      "[260]\tvalid's binary_logloss: 0.008079\n",
      "[270]\tvalid's binary_logloss: 0.00813439\n",
      "[280]\tvalid's binary_logloss: 0.00802023\n",
      "[290]\tvalid's binary_logloss: 0.00801244\n",
      "[300]\tvalid's binary_logloss: 0.00803158\n",
      "[310]\tvalid's binary_logloss: 0.00800437\n",
      "[320]\tvalid's binary_logloss: 0.00801984\n",
      "[330]\tvalid's binary_logloss: 0.00803649\n",
      "Early stopping, best iteration is:\n",
      "[308]\tvalid's binary_logloss: 0.00798337\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  51.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.10403\n",
      "[20]\tvalid's binary_logloss: 0.0619253\n",
      "[30]\tvalid's binary_logloss: 0.0412695\n",
      "[40]\tvalid's binary_logloss: 0.0305845\n",
      "[50]\tvalid's binary_logloss: 0.024381\n",
      "[60]\tvalid's binary_logloss: 0.020415\n",
      "[70]\tvalid's binary_logloss: 0.017838\n",
      "[80]\tvalid's binary_logloss: 0.0158273\n",
      "[90]\tvalid's binary_logloss: 0.0143852\n",
      "[100]\tvalid's binary_logloss: 0.0134324\n",
      "[110]\tvalid's binary_logloss: 0.0126329\n",
      "[120]\tvalid's binary_logloss: 0.0119836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130]\tvalid's binary_logloss: 0.0118424\n",
      "[140]\tvalid's binary_logloss: 0.0115534\n",
      "[150]\tvalid's binary_logloss: 0.0114087\n",
      "[160]\tvalid's binary_logloss: 0.0114295\n",
      "[170]\tvalid's binary_logloss: 0.0114933\n",
      "Early stopping, best iteration is:\n",
      "[146]\tvalid's binary_logloss: 0.0113581\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9152542372881356, total=  44.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.104828\n",
      "[20]\tvalid's binary_logloss: 0.0619801\n",
      "[30]\tvalid's binary_logloss: 0.042355\n",
      "[40]\tvalid's binary_logloss: 0.0317094\n",
      "[50]\tvalid's binary_logloss: 0.0253121\n",
      "[60]\tvalid's binary_logloss: 0.0212435\n",
      "[70]\tvalid's binary_logloss: 0.018234\n",
      "[80]\tvalid's binary_logloss: 0.0159163\n",
      "[90]\tvalid's binary_logloss: 0.0140125\n",
      "[100]\tvalid's binary_logloss: 0.0129025\n",
      "[110]\tvalid's binary_logloss: 0.0118438\n",
      "[120]\tvalid's binary_logloss: 0.0110394\n",
      "[130]\tvalid's binary_logloss: 0.010695\n",
      "[140]\tvalid's binary_logloss: 0.0102274\n",
      "[150]\tvalid's binary_logloss: 0.0100192\n",
      "[160]\tvalid's binary_logloss: 0.00994716\n",
      "[170]\tvalid's binary_logloss: 0.00977614\n",
      "[180]\tvalid's binary_logloss: 0.00965711\n",
      "[190]\tvalid's binary_logloss: 0.00966731\n",
      "[200]\tvalid's binary_logloss: 0.00966612\n",
      "[210]\tvalid's binary_logloss: 0.00967999\n",
      "[220]\tvalid's binary_logloss: 0.00960755\n",
      "Early stopping, best iteration is:\n",
      "[195]\tvalid's binary_logloss: 0.00958885\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9411764705882353, total=  46.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105891\n",
      "[20]\tvalid's binary_logloss: 0.064263\n",
      "[30]\tvalid's binary_logloss: 0.0444696\n",
      "[40]\tvalid's binary_logloss: 0.0329888\n",
      "[50]\tvalid's binary_logloss: 0.0257328\n",
      "[60]\tvalid's binary_logloss: 0.020946\n",
      "[70]\tvalid's binary_logloss: 0.0179026\n",
      "[80]\tvalid's binary_logloss: 0.0160167\n",
      "[90]\tvalid's binary_logloss: 0.014951\n",
      "[100]\tvalid's binary_logloss: 0.0139925\n",
      "[110]\tvalid's binary_logloss: 0.0135363\n",
      "[120]\tvalid's binary_logloss: 0.013268\n",
      "[130]\tvalid's binary_logloss: 0.0132706\n",
      "[140]\tvalid's binary_logloss: 0.0131513\n",
      "[150]\tvalid's binary_logloss: 0.013128\n",
      "[160]\tvalid's binary_logloss: 0.0132014\n",
      "[170]\tvalid's binary_logloss: 0.0133134\n",
      "[180]\tvalid's binary_logloss: 0.0135634\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid's binary_logloss: 0.0130475\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.8947368421052632, total=  44.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105213\n",
      "[20]\tvalid's binary_logloss: 0.063666\n",
      "[30]\tvalid's binary_logloss: 0.0439832\n",
      "[40]\tvalid's binary_logloss: 0.0312532\n",
      "[50]\tvalid's binary_logloss: 0.0244658\n",
      "[60]\tvalid's binary_logloss: 0.0196177\n",
      "[70]\tvalid's binary_logloss: 0.01662\n",
      "[80]\tvalid's binary_logloss: 0.0143578\n",
      "[90]\tvalid's binary_logloss: 0.0130189\n",
      "[100]\tvalid's binary_logloss: 0.0119034\n",
      "[110]\tvalid's binary_logloss: 0.011225\n",
      "[120]\tvalid's binary_logloss: 0.0106242\n",
      "[130]\tvalid's binary_logloss: 0.00997445\n",
      "[140]\tvalid's binary_logloss: 0.00969074\n",
      "[150]\tvalid's binary_logloss: 0.00944223\n",
      "[160]\tvalid's binary_logloss: 0.00947977\n",
      "[170]\tvalid's binary_logloss: 0.00919612\n",
      "[180]\tvalid's binary_logloss: 0.0091831\n",
      "[190]\tvalid's binary_logloss: 0.00918958\n",
      "[200]\tvalid's binary_logloss: 0.00915935\n",
      "[210]\tvalid's binary_logloss: 0.00913588\n",
      "[220]\tvalid's binary_logloss: 0.00916788\n",
      "[230]\tvalid's binary_logloss: 0.0092328\n",
      "Early stopping, best iteration is:\n",
      "[203]\tvalid's binary_logloss: 0.00907183\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  51.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.103311\n",
      "[20]\tvalid's binary_logloss: 0.0624206\n",
      "[30]\tvalid's binary_logloss: 0.0420341\n",
      "[40]\tvalid's binary_logloss: 0.0310373\n",
      "[50]\tvalid's binary_logloss: 0.0253095\n",
      "[60]\tvalid's binary_logloss: 0.0209138\n",
      "[70]\tvalid's binary_logloss: 0.0180477\n",
      "[80]\tvalid's binary_logloss: 0.0157355\n",
      "[90]\tvalid's binary_logloss: 0.0143605\n",
      "[100]\tvalid's binary_logloss: 0.013181\n",
      "[110]\tvalid's binary_logloss: 0.0123085\n",
      "[120]\tvalid's binary_logloss: 0.0118526\n",
      "[130]\tvalid's binary_logloss: 0.0115192\n",
      "[140]\tvalid's binary_logloss: 0.011445\n",
      "[150]\tvalid's binary_logloss: 0.0114706\n",
      "[160]\tvalid's binary_logloss: 0.0111286\n",
      "[170]\tvalid's binary_logloss: 0.0113165\n",
      "[180]\tvalid's binary_logloss: 0.0113453\n",
      "[190]\tvalid's binary_logloss: 0.0114473\n",
      "Early stopping, best iteration is:\n",
      "[160]\tvalid's binary_logloss: 0.0111286\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  53.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105185\n",
      "[20]\tvalid's binary_logloss: 0.0624537\n",
      "[30]\tvalid's binary_logloss: 0.0425726\n",
      "[40]\tvalid's binary_logloss: 0.0323354\n",
      "[50]\tvalid's binary_logloss: 0.0261086\n",
      "[60]\tvalid's binary_logloss: 0.0218966\n",
      "[70]\tvalid's binary_logloss: 0.0188421\n",
      "[80]\tvalid's binary_logloss: 0.0164398\n",
      "[90]\tvalid's binary_logloss: 0.0145015\n",
      "[100]\tvalid's binary_logloss: 0.0133203\n",
      "[110]\tvalid's binary_logloss: 0.0125799\n",
      "[120]\tvalid's binary_logloss: 0.0118984\n",
      "[130]\tvalid's binary_logloss: 0.0114483\n",
      "[140]\tvalid's binary_logloss: 0.0109922\n",
      "[150]\tvalid's binary_logloss: 0.0110174\n",
      "[160]\tvalid's binary_logloss: 0.0107704\n",
      "[170]\tvalid's binary_logloss: 0.0106354\n",
      "[180]\tvalid's binary_logloss: 0.0104663\n",
      "[190]\tvalid's binary_logloss: 0.0106085\n",
      "[200]\tvalid's binary_logloss: 0.0105117\n",
      "[210]\tvalid's binary_logloss: 0.0106258\n",
      "Early stopping, best iteration is:\n",
      "[180]\tvalid's binary_logloss: 0.0104663\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  49.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.106244\n",
      "[20]\tvalid's binary_logloss: 0.0641992\n",
      "[30]\tvalid's binary_logloss: 0.0444306\n",
      "[40]\tvalid's binary_logloss: 0.0328603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid's binary_logloss: 0.0258487\n",
      "[60]\tvalid's binary_logloss: 0.0219757\n",
      "[70]\tvalid's binary_logloss: 0.0192139\n",
      "[80]\tvalid's binary_logloss: 0.0170575\n",
      "[90]\tvalid's binary_logloss: 0.0155943\n",
      "[100]\tvalid's binary_logloss: 0.0146334\n",
      "[110]\tvalid's binary_logloss: 0.0140324\n",
      "[120]\tvalid's binary_logloss: 0.0136262\n",
      "[130]\tvalid's binary_logloss: 0.0135501\n",
      "[140]\tvalid's binary_logloss: 0.0136438\n",
      "[150]\tvalid's binary_logloss: 0.0137461\n",
      "[160]\tvalid's binary_logloss: 0.0140524\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid's binary_logloss: 0.0135288\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  44.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.103877\n",
      "[20]\tvalid's binary_logloss: 0.0626593\n",
      "[30]\tvalid's binary_logloss: 0.042861\n",
      "[40]\tvalid's binary_logloss: 0.0308303\n",
      "[50]\tvalid's binary_logloss: 0.0238472\n",
      "[60]\tvalid's binary_logloss: 0.0192712\n",
      "[70]\tvalid's binary_logloss: 0.0163761\n",
      "[80]\tvalid's binary_logloss: 0.0143597\n",
      "[90]\tvalid's binary_logloss: 0.0129875\n",
      "[100]\tvalid's binary_logloss: 0.0115306\n",
      "[110]\tvalid's binary_logloss: 0.0109007\n",
      "[120]\tvalid's binary_logloss: 0.0101833\n",
      "[130]\tvalid's binary_logloss: 0.00939737\n",
      "[140]\tvalid's binary_logloss: 0.00902941\n",
      "[150]\tvalid's binary_logloss: 0.00893926\n",
      "[160]\tvalid's binary_logloss: 0.00872862\n",
      "[170]\tvalid's binary_logloss: 0.00859869\n",
      "[180]\tvalid's binary_logloss: 0.00860693\n",
      "[190]\tvalid's binary_logloss: 0.00877435\n",
      "[200]\tvalid's binary_logloss: 0.00904169\n",
      "Early stopping, best iteration is:\n",
      "[170]\tvalid's binary_logloss: 0.00859869\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  44.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105402\n",
      "[20]\tvalid's binary_logloss: 0.0637107\n",
      "[30]\tvalid's binary_logloss: 0.0441163\n",
      "[40]\tvalid's binary_logloss: 0.0327957\n",
      "[50]\tvalid's binary_logloss: 0.0261293\n",
      "[60]\tvalid's binary_logloss: 0.0216882\n",
      "[70]\tvalid's binary_logloss: 0.0185559\n",
      "[80]\tvalid's binary_logloss: 0.0163256\n",
      "[90]\tvalid's binary_logloss: 0.0147613\n",
      "[100]\tvalid's binary_logloss: 0.013434\n",
      "[110]\tvalid's binary_logloss: 0.0125084\n",
      "[120]\tvalid's binary_logloss: 0.0119322\n",
      "[130]\tvalid's binary_logloss: 0.0114275\n",
      "[140]\tvalid's binary_logloss: 0.0108397\n",
      "[150]\tvalid's binary_logloss: 0.0107366\n",
      "[160]\tvalid's binary_logloss: 0.0104506\n",
      "[170]\tvalid's binary_logloss: 0.0102861\n",
      "[180]\tvalid's binary_logloss: 0.0102196\n",
      "[190]\tvalid's binary_logloss: 0.0101319\n",
      "[200]\tvalid's binary_logloss: 0.0100741\n",
      "[210]\tvalid's binary_logloss: 0.010062\n",
      "[220]\tvalid's binary_logloss: 0.00998625\n",
      "[230]\tvalid's binary_logloss: 0.0099825\n",
      "[240]\tvalid's binary_logloss: 0.00996906\n",
      "[250]\tvalid's binary_logloss: 0.00995443\n",
      "[260]\tvalid's binary_logloss: 0.00994827\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid's binary_logloss: 0.00993362\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  46.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.10869\n",
      "[20]\tvalid's binary_logloss: 0.0668116\n",
      "[30]\tvalid's binary_logloss: 0.0475331\n",
      "[40]\tvalid's binary_logloss: 0.0361981\n",
      "[50]\tvalid's binary_logloss: 0.0291257\n",
      "[60]\tvalid's binary_logloss: 0.0242729\n",
      "[70]\tvalid's binary_logloss: 0.021108\n",
      "[80]\tvalid's binary_logloss: 0.0181725\n",
      "[90]\tvalid's binary_logloss: 0.0159832\n",
      "[100]\tvalid's binary_logloss: 0.0146482\n",
      "[110]\tvalid's binary_logloss: 0.0134766\n",
      "[120]\tvalid's binary_logloss: 0.0125574\n",
      "[130]\tvalid's binary_logloss: 0.0119916\n",
      "[140]\tvalid's binary_logloss: 0.0114239\n",
      "[150]\tvalid's binary_logloss: 0.0110395\n",
      "[160]\tvalid's binary_logloss: 0.0108293\n",
      "[170]\tvalid's binary_logloss: 0.0105654\n",
      "[180]\tvalid's binary_logloss: 0.0103841\n",
      "[190]\tvalid's binary_logloss: 0.0102782\n",
      "[200]\tvalid's binary_logloss: 0.0101514\n",
      "[210]\tvalid's binary_logloss: 0.00996005\n",
      "[220]\tvalid's binary_logloss: 0.00986726\n",
      "[230]\tvalid's binary_logloss: 0.00974528\n",
      "[240]\tvalid's binary_logloss: 0.00974271\n",
      "[250]\tvalid's binary_logloss: 0.00967982\n",
      "[260]\tvalid's binary_logloss: 0.00959037\n",
      "[270]\tvalid's binary_logloss: 0.00947986\n",
      "[280]\tvalid's binary_logloss: 0.00943935\n",
      "[290]\tvalid's binary_logloss: 0.00939352\n",
      "[300]\tvalid's binary_logloss: 0.00938363\n",
      "[310]\tvalid's binary_logloss: 0.00932688\n",
      "[320]\tvalid's binary_logloss: 0.00929186\n",
      "[330]\tvalid's binary_logloss: 0.00929904\n",
      "[340]\tvalid's binary_logloss: 0.00925993\n",
      "[350]\tvalid's binary_logloss: 0.00920759\n",
      "[360]\tvalid's binary_logloss: 0.00920123\n",
      "[370]\tvalid's binary_logloss: 0.00919729\n",
      "[380]\tvalid's binary_logloss: 0.00917266\n",
      "[390]\tvalid's binary_logloss: 0.00917508\n",
      "[400]\tvalid's binary_logloss: 0.00913585\n",
      "[410]\tvalid's binary_logloss: 0.00912738\n",
      "[420]\tvalid's binary_logloss: 0.00912283\n",
      "[430]\tvalid's binary_logloss: 0.00910591\n",
      "[440]\tvalid's binary_logloss: 0.00910413\n",
      "[450]\tvalid's binary_logloss: 0.00910395\n",
      "[460]\tvalid's binary_logloss: 0.00910385\n",
      "Early stopping, best iteration is:\n",
      "[436]\tvalid's binary_logloss: 0.00910289\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  51.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.109109\n",
      "[20]\tvalid's binary_logloss: 0.0672676\n",
      "[30]\tvalid's binary_logloss: 0.0476093\n",
      "[40]\tvalid's binary_logloss: 0.0358905\n",
      "[50]\tvalid's binary_logloss: 0.0285521\n",
      "[60]\tvalid's binary_logloss: 0.0238874\n",
      "[70]\tvalid's binary_logloss: 0.0206176\n",
      "[80]\tvalid's binary_logloss: 0.0186732\n",
      "[90]\tvalid's binary_logloss: 0.0173618\n",
      "[100]\tvalid's binary_logloss: 0.0162628\n",
      "[110]\tvalid's binary_logloss: 0.0155634\n",
      "[120]\tvalid's binary_logloss: 0.015166\n",
      "[130]\tvalid's binary_logloss: 0.0147343\n",
      "[140]\tvalid's binary_logloss: 0.0145307\n",
      "[150]\tvalid's binary_logloss: 0.0143597\n",
      "[160]\tvalid's binary_logloss: 0.0142849\n",
      "[170]\tvalid's binary_logloss: 0.0142546\n",
      "[180]\tvalid's binary_logloss: 0.0141815\n",
      "[190]\tvalid's binary_logloss: 0.0141349\n",
      "[200]\tvalid's binary_logloss: 0.0141874\n",
      "[210]\tvalid's binary_logloss: 0.0141289\n",
      "[220]\tvalid's binary_logloss: 0.0140992\n",
      "[230]\tvalid's binary_logloss: 0.0140921\n",
      "[240]\tvalid's binary_logloss: 0.0140917\n",
      "[250]\tvalid's binary_logloss: 0.0140785\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid's binary_logloss: 0.0140524\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  45.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.109522\n",
      "[20]\tvalid's binary_logloss: 0.0671603\n",
      "[30]\tvalid's binary_logloss: 0.0474713\n",
      "[40]\tvalid's binary_logloss: 0.0342015\n",
      "[50]\tvalid's binary_logloss: 0.0272037\n",
      "[60]\tvalid's binary_logloss: 0.0219822\n",
      "[70]\tvalid's binary_logloss: 0.0183311\n",
      "[80]\tvalid's binary_logloss: 0.0159279\n",
      "[90]\tvalid's binary_logloss: 0.0138834\n",
      "[100]\tvalid's binary_logloss: 0.012629\n",
      "[110]\tvalid's binary_logloss: 0.0115336\n",
      "[120]\tvalid's binary_logloss: 0.0108353\n",
      "[130]\tvalid's binary_logloss: 0.0102461\n",
      "[140]\tvalid's binary_logloss: 0.00976592\n",
      "[150]\tvalid's binary_logloss: 0.00940867\n",
      "[160]\tvalid's binary_logloss: 0.00918258\n",
      "[170]\tvalid's binary_logloss: 0.00898923\n",
      "[180]\tvalid's binary_logloss: 0.00877394\n",
      "[190]\tvalid's binary_logloss: 0.00856944\n",
      "[200]\tvalid's binary_logloss: 0.00846448\n",
      "[210]\tvalid's binary_logloss: 0.008384\n",
      "[220]\tvalid's binary_logloss: 0.00828709\n",
      "[230]\tvalid's binary_logloss: 0.00825024\n",
      "[240]\tvalid's binary_logloss: 0.00816006\n",
      "[250]\tvalid's binary_logloss: 0.00807546\n",
      "[260]\tvalid's binary_logloss: 0.00801484\n",
      "[270]\tvalid's binary_logloss: 0.0079977\n",
      "[280]\tvalid's binary_logloss: 0.00794342\n",
      "[290]\tvalid's binary_logloss: 0.0079156\n",
      "[300]\tvalid's binary_logloss: 0.00788788\n",
      "[310]\tvalid's binary_logloss: 0.00788281\n",
      "[320]\tvalid's binary_logloss: 0.00789617\n",
      "[330]\tvalid's binary_logloss: 0.00788187\n",
      "[340]\tvalid's binary_logloss: 0.00786271\n",
      "[350]\tvalid's binary_logloss: 0.00785412\n",
      "[360]\tvalid's binary_logloss: 0.0078348\n",
      "[370]\tvalid's binary_logloss: 0.00786589\n",
      "[380]\tvalid's binary_logloss: 0.00782069\n",
      "[390]\tvalid's binary_logloss: 0.00782201\n",
      "[400]\tvalid's binary_logloss: 0.00780834\n",
      "[410]\tvalid's binary_logloss: 0.00779289\n",
      "[420]\tvalid's binary_logloss: 0.00778177\n",
      "[430]\tvalid's binary_logloss: 0.00777264\n",
      "[440]\tvalid's binary_logloss: 0.00777648\n",
      "[450]\tvalid's binary_logloss: 0.00777648\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalid's binary_logloss: 0.00777112\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  51.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.104236\n",
      "[20]\tvalid's binary_logloss: 0.0621943\n",
      "[30]\tvalid's binary_logloss: 0.0421176\n",
      "[40]\tvalid's binary_logloss: 0.0314421\n",
      "[50]\tvalid's binary_logloss: 0.0240942\n",
      "[60]\tvalid's binary_logloss: 0.02\n",
      "[70]\tvalid's binary_logloss: 0.0171374\n",
      "[80]\tvalid's binary_logloss: 0.0152168\n",
      "[90]\tvalid's binary_logloss: 0.0140418\n",
      "[100]\tvalid's binary_logloss: 0.0129109\n",
      "[110]\tvalid's binary_logloss: 0.0123042\n",
      "[120]\tvalid's binary_logloss: 0.0118722\n",
      "[130]\tvalid's binary_logloss: 0.0114726\n",
      "[140]\tvalid's binary_logloss: 0.0108827\n",
      "[150]\tvalid's binary_logloss: 0.010671\n",
      "[160]\tvalid's binary_logloss: 0.010762\n",
      "[170]\tvalid's binary_logloss: 0.0106864\n",
      "[180]\tvalid's binary_logloss: 0.0105873\n",
      "[190]\tvalid's binary_logloss: 0.0105303\n",
      "[200]\tvalid's binary_logloss: 0.010483\n",
      "[210]\tvalid's binary_logloss: 0.0105228\n",
      "[220]\tvalid's binary_logloss: 0.010514\n",
      "[230]\tvalid's binary_logloss: 0.010544\n",
      "Early stopping, best iteration is:\n",
      "[202]\tvalid's binary_logloss: 0.0104714\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9243697478991596, total=  44.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105025\n",
      "[20]\tvalid's binary_logloss: 0.0624325\n",
      "[30]\tvalid's binary_logloss: 0.0429629\n",
      "[40]\tvalid's binary_logloss: 0.032295\n",
      "[50]\tvalid's binary_logloss: 0.0260916\n",
      "[60]\tvalid's binary_logloss: 0.0221975\n",
      "[70]\tvalid's binary_logloss: 0.0190747\n",
      "[80]\tvalid's binary_logloss: 0.0172067\n",
      "[90]\tvalid's binary_logloss: 0.0152815\n",
      "[100]\tvalid's binary_logloss: 0.0139322\n",
      "[110]\tvalid's binary_logloss: 0.0129463\n",
      "[120]\tvalid's binary_logloss: 0.0120926\n",
      "[130]\tvalid's binary_logloss: 0.011658\n",
      "[140]\tvalid's binary_logloss: 0.0112719\n",
      "[150]\tvalid's binary_logloss: 0.0110219\n",
      "[160]\tvalid's binary_logloss: 0.0107245\n",
      "[170]\tvalid's binary_logloss: 0.0105287\n",
      "[180]\tvalid's binary_logloss: 0.0105008\n",
      "[190]\tvalid's binary_logloss: 0.0103936\n",
      "[200]\tvalid's binary_logloss: 0.0103401\n",
      "[210]\tvalid's binary_logloss: 0.0103094\n",
      "[220]\tvalid's binary_logloss: 0.0102439\n",
      "[230]\tvalid's binary_logloss: 0.0102136\n",
      "[240]\tvalid's binary_logloss: 0.0101844\n",
      "[250]\tvalid's binary_logloss: 0.0101786\n",
      "[260]\tvalid's binary_logloss: 0.0101786\n",
      "Early stopping, best iteration is:\n",
      "[238]\tvalid's binary_logloss: 0.0101567\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  45.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105467\n",
      "[20]\tvalid's binary_logloss: 0.0639552\n",
      "[30]\tvalid's binary_logloss: 0.0441868\n",
      "[40]\tvalid's binary_logloss: 0.0336782\n",
      "[50]\tvalid's binary_logloss: 0.0261812\n",
      "[60]\tvalid's binary_logloss: 0.0219006\n",
      "[70]\tvalid's binary_logloss: 0.0189318\n",
      "[80]\tvalid's binary_logloss: 0.0170567\n",
      "[90]\tvalid's binary_logloss: 0.0157642\n",
      "[100]\tvalid's binary_logloss: 0.0149296\n",
      "[110]\tvalid's binary_logloss: 0.0143259\n",
      "[120]\tvalid's binary_logloss: 0.0141404\n",
      "[130]\tvalid's binary_logloss: 0.0139556\n",
      "[140]\tvalid's binary_logloss: 0.0139998\n",
      "[150]\tvalid's binary_logloss: 0.0138837\n",
      "[160]\tvalid's binary_logloss: 0.0138824\n",
      "[170]\tvalid's binary_logloss: 0.0138623\n",
      "[180]\tvalid's binary_logloss: 0.0139068\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid's binary_logloss: 0.0138295\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.8947368421052632, total=  43.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105214\n",
      "[20]\tvalid's binary_logloss: 0.0641103\n",
      "[30]\tvalid's binary_logloss: 0.0438163\n",
      "[40]\tvalid's binary_logloss: 0.0315665\n",
      "[50]\tvalid's binary_logloss: 0.0247837\n",
      "[60]\tvalid's binary_logloss: 0.0199875\n",
      "[70]\tvalid's binary_logloss: 0.0169589\n",
      "[80]\tvalid's binary_logloss: 0.0150651\n",
      "[90]\tvalid's binary_logloss: 0.0131812\n",
      "[100]\tvalid's binary_logloss: 0.0119113\n",
      "[110]\tvalid's binary_logloss: 0.0110215\n",
      "[120]\tvalid's binary_logloss: 0.0102952\n",
      "[130]\tvalid's binary_logloss: 0.00970525\n",
      "[140]\tvalid's binary_logloss: 0.00936956\n",
      "[150]\tvalid's binary_logloss: 0.0089747\n",
      "[160]\tvalid's binary_logloss: 0.00872207\n",
      "[170]\tvalid's binary_logloss: 0.00879511\n",
      "[180]\tvalid's binary_logloss: 0.00871814\n",
      "[190]\tvalid's binary_logloss: 0.00862583\n",
      "[200]\tvalid's binary_logloss: 0.00862792\n",
      "[210]\tvalid's binary_logloss: 0.00858166\n",
      "[220]\tvalid's binary_logloss: 0.00861449\n",
      "[230]\tvalid's binary_logloss: 0.0085606\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid's binary_logloss: 0.00853928\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.923076923076923, total=  44.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.103867\n",
      "[20]\tvalid's binary_logloss: 0.0621888\n",
      "[30]\tvalid's binary_logloss: 0.0421255\n",
      "[40]\tvalid's binary_logloss: 0.0313438\n",
      "[50]\tvalid's binary_logloss: 0.0249743\n",
      "[60]\tvalid's binary_logloss: 0.0207917\n",
      "[70]\tvalid's binary_logloss: 0.0180242\n",
      "[80]\tvalid's binary_logloss: 0.0156794\n",
      "[90]\tvalid's binary_logloss: 0.0141928\n",
      "[100]\tvalid's binary_logloss: 0.0130816\n",
      "[110]\tvalid's binary_logloss: 0.0122489\n",
      "[120]\tvalid's binary_logloss: 0.011709\n",
      "[130]\tvalid's binary_logloss: 0.0112775\n",
      "[140]\tvalid's binary_logloss: 0.0112097\n",
      "[150]\tvalid's binary_logloss: 0.0111554\n",
      "[160]\tvalid's binary_logloss: 0.0109929\n",
      "[170]\tvalid's binary_logloss: 0.0110387\n",
      "[180]\tvalid's binary_logloss: 0.0110157\n",
      "[190]\tvalid's binary_logloss: 0.011198\n",
      "[200]\tvalid's binary_logloss: 0.0111097\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid's binary_logloss: 0.0109535\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9152542372881356, total=  44.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105278\n",
      "[20]\tvalid's binary_logloss: 0.0625434\n",
      "[30]\tvalid's binary_logloss: 0.043132\n",
      "[40]\tvalid's binary_logloss: 0.0333392\n",
      "[50]\tvalid's binary_logloss: 0.0260549\n",
      "[60]\tvalid's binary_logloss: 0.0217457\n",
      "[70]\tvalid's binary_logloss: 0.0186995\n",
      "[80]\tvalid's binary_logloss: 0.0160965\n",
      "[90]\tvalid's binary_logloss: 0.0142165\n",
      "[100]\tvalid's binary_logloss: 0.0131518\n",
      "[110]\tvalid's binary_logloss: 0.0121506\n",
      "[120]\tvalid's binary_logloss: 0.011207\n",
      "[130]\tvalid's binary_logloss: 0.0107442\n",
      "[140]\tvalid's binary_logloss: 0.0105319\n",
      "[150]\tvalid's binary_logloss: 0.0102157\n",
      "[160]\tvalid's binary_logloss: 0.00996698\n",
      "[170]\tvalid's binary_logloss: 0.00991044\n",
      "[180]\tvalid's binary_logloss: 0.00971893\n",
      "[190]\tvalid's binary_logloss: 0.00973769\n",
      "[200]\tvalid's binary_logloss: 0.00962952\n",
      "[210]\tvalid's binary_logloss: 0.00954575\n",
      "[220]\tvalid's binary_logloss: 0.00950127\n",
      "[230]\tvalid's binary_logloss: 0.00949896\n",
      "[240]\tvalid's binary_logloss: 0.00949896\n",
      "Early stopping, best iteration is:\n",
      "[219]\tvalid's binary_logloss: 0.00949166\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9411764705882353, total=  44.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105628\n",
      "[20]\tvalid's binary_logloss: 0.0642127\n",
      "[30]\tvalid's binary_logloss: 0.0445031\n",
      "[40]\tvalid's binary_logloss: 0.0327512\n",
      "[50]\tvalid's binary_logloss: 0.025844\n",
      "[60]\tvalid's binary_logloss: 0.0213218\n",
      "[70]\tvalid's binary_logloss: 0.0185541\n",
      "[80]\tvalid's binary_logloss: 0.0167364\n",
      "[90]\tvalid's binary_logloss: 0.0152023\n",
      "[100]\tvalid's binary_logloss: 0.0143267\n",
      "[110]\tvalid's binary_logloss: 0.0139999\n",
      "[120]\tvalid's binary_logloss: 0.0137595\n",
      "[130]\tvalid's binary_logloss: 0.0134968\n",
      "[140]\tvalid's binary_logloss: 0.0133995\n",
      "[150]\tvalid's binary_logloss: 0.0133898\n",
      "[160]\tvalid's binary_logloss: 0.0134109\n",
      "[170]\tvalid's binary_logloss: 0.0135475\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid's binary_logloss: 0.0133567\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.8947368421052632, total=  43.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.103784\n",
      "[20]\tvalid's binary_logloss: 0.0628516\n",
      "[30]\tvalid's binary_logloss: 0.0433051\n",
      "[40]\tvalid's binary_logloss: 0.0311816\n",
      "[50]\tvalid's binary_logloss: 0.0243108\n",
      "[60]\tvalid's binary_logloss: 0.0197721\n",
      "[70]\tvalid's binary_logloss: 0.0165249\n",
      "[80]\tvalid's binary_logloss: 0.0144207\n",
      "[90]\tvalid's binary_logloss: 0.0128558\n",
      "[100]\tvalid's binary_logloss: 0.0116563\n",
      "[110]\tvalid's binary_logloss: 0.010876\n",
      "[120]\tvalid's binary_logloss: 0.0104144\n",
      "[130]\tvalid's binary_logloss: 0.00985106\n",
      "[140]\tvalid's binary_logloss: 0.00935447\n",
      "[150]\tvalid's binary_logloss: 0.00898322\n",
      "[160]\tvalid's binary_logloss: 0.00878228\n",
      "[170]\tvalid's binary_logloss: 0.00858587\n",
      "[180]\tvalid's binary_logloss: 0.00861058\n",
      "[190]\tvalid's binary_logloss: 0.00858813\n",
      "[200]\tvalid's binary_logloss: 0.00858622\n",
      "[210]\tvalid's binary_logloss: 0.00849273\n",
      "[220]\tvalid's binary_logloss: 0.00853743\n",
      "[230]\tvalid's binary_logloss: 0.00854105\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid's binary_logloss: 0.00848469\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  53.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.108773\n",
      "[20]\tvalid's binary_logloss: 0.0671537\n",
      "[30]\tvalid's binary_logloss: 0.0470154\n",
      "[40]\tvalid's binary_logloss: 0.0362742\n",
      "[50]\tvalid's binary_logloss: 0.0297902\n",
      "[60]\tvalid's binary_logloss: 0.0251553\n",
      "[70]\tvalid's binary_logloss: 0.0222214\n",
      "[80]\tvalid's binary_logloss: 0.0200956\n",
      "[90]\tvalid's binary_logloss: 0.0181506\n",
      "[100]\tvalid's binary_logloss: 0.016747\n",
      "[110]\tvalid's binary_logloss: 0.015682\n",
      "[120]\tvalid's binary_logloss: 0.0148516\n",
      "[130]\tvalid's binary_logloss: 0.0144012\n",
      "[140]\tvalid's binary_logloss: 0.0138481\n",
      "[150]\tvalid's binary_logloss: 0.0134804\n",
      "[160]\tvalid's binary_logloss: 0.0131931\n",
      "[170]\tvalid's binary_logloss: 0.0131664\n",
      "[180]\tvalid's binary_logloss: 0.0131664\n",
      "[190]\tvalid's binary_logloss: 0.0131664\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid's binary_logloss: 0.0131561\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.923076923076923, total=  45.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.110331\n",
      "[20]\tvalid's binary_logloss: 0.0677529\n",
      "[30]\tvalid's binary_logloss: 0.0480082\n",
      "[40]\tvalid's binary_logloss: 0.0377166\n",
      "[50]\tvalid's binary_logloss: 0.0311004\n",
      "[60]\tvalid's binary_logloss: 0.0263283\n",
      "[70]\tvalid's binary_logloss: 0.0230037\n",
      "[80]\tvalid's binary_logloss: 0.0203579\n",
      "[90]\tvalid's binary_logloss: 0.018337\n",
      "[100]\tvalid's binary_logloss: 0.0169335\n",
      "[110]\tvalid's binary_logloss: 0.0159892\n",
      "[120]\tvalid's binary_logloss: 0.0151811\n",
      "[130]\tvalid's binary_logloss: 0.0145804\n",
      "[140]\tvalid's binary_logloss: 0.0140676\n",
      "[150]\tvalid's binary_logloss: 0.0136231\n",
      "[160]\tvalid's binary_logloss: 0.0133057\n",
      "[170]\tvalid's binary_logloss: 0.0132803\n",
      "[180]\tvalid's binary_logloss: 0.0132803\n",
      "[190]\tvalid's binary_logloss: 0.0132803\n",
      "Early stopping, best iteration is:\n",
      "[165]\tvalid's binary_logloss: 0.0132803\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9152542372881356, total=  42.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.110583\n",
      "[20]\tvalid's binary_logloss: 0.0687046\n",
      "[30]\tvalid's binary_logloss: 0.0497556\n",
      "[40]\tvalid's binary_logloss: 0.0381143\n",
      "[50]\tvalid's binary_logloss: 0.0311464\n",
      "[60]\tvalid's binary_logloss: 0.0265839\n",
      "[70]\tvalid's binary_logloss: 0.0236219\n",
      "[80]\tvalid's binary_logloss: 0.0214025\n",
      "[90]\tvalid's binary_logloss: 0.0196905\n",
      "[100]\tvalid's binary_logloss: 0.018568\n",
      "[110]\tvalid's binary_logloss: 0.0177502\n",
      "[120]\tvalid's binary_logloss: 0.0170255\n",
      "[130]\tvalid's binary_logloss: 0.0166075\n",
      "[140]\tvalid's binary_logloss: 0.0162486\n",
      "[150]\tvalid's binary_logloss: 0.0160535\n",
      "[160]\tvalid's binary_logloss: 0.0158476\n",
      "[170]\tvalid's binary_logloss: 0.0158476\n",
      "[180]\tvalid's binary_logloss: 0.0158476\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid's binary_logloss: 0.0158472\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  42.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.110713\n",
      "[20]\tvalid's binary_logloss: 0.0685966\n",
      "[30]\tvalid's binary_logloss: 0.0492621\n",
      "[40]\tvalid's binary_logloss: 0.0373524\n",
      "[50]\tvalid's binary_logloss: 0.0303208\n",
      "[60]\tvalid's binary_logloss: 0.0252708\n",
      "[70]\tvalid's binary_logloss: 0.0218126\n",
      "[80]\tvalid's binary_logloss: 0.0191783\n",
      "[90]\tvalid's binary_logloss: 0.0172794\n",
      "[100]\tvalid's binary_logloss: 0.0158893\n",
      "[110]\tvalid's binary_logloss: 0.0147904\n",
      "[120]\tvalid's binary_logloss: 0.0138817\n",
      "[130]\tvalid's binary_logloss: 0.0131382\n",
      "[140]\tvalid's binary_logloss: 0.0126031\n",
      "[150]\tvalid's binary_logloss: 0.0120814\n",
      "[160]\tvalid's binary_logloss: 0.011814\n",
      "[170]\tvalid's binary_logloss: 0.0116958\n",
      "[180]\tvalid's binary_logloss: 0.0116959\n",
      "[190]\tvalid's binary_logloss: 0.0116959\n",
      "Early stopping, best iteration is:\n",
      "[164]\tvalid's binary_logloss: 0.0116957\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9243697478991596, total=  42.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.104393\n",
      "[20]\tvalid's binary_logloss: 0.0638198\n",
      "[30]\tvalid's binary_logloss: 0.0438652\n",
      "[40]\tvalid's binary_logloss: 0.0339155\n",
      "[50]\tvalid's binary_logloss: 0.027732\n",
      "[60]\tvalid's binary_logloss: 0.0237592\n",
      "[70]\tvalid's binary_logloss: 0.0204336\n",
      "[80]\tvalid's binary_logloss: 0.0180311\n",
      "[90]\tvalid's binary_logloss: 0.0165189\n",
      "[100]\tvalid's binary_logloss: 0.0153091\n",
      "[110]\tvalid's binary_logloss: 0.0145361\n",
      "[120]\tvalid's binary_logloss: 0.0137665\n",
      "[130]\tvalid's binary_logloss: 0.0134122\n",
      "[140]\tvalid's binary_logloss: 0.0130873\n",
      "[150]\tvalid's binary_logloss: 0.0130873\n",
      "[160]\tvalid's binary_logloss: 0.0130873\n",
      "[170]\tvalid's binary_logloss: 0.0130873\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid's binary_logloss: 0.0130873\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  42.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.107846\n",
      "[20]\tvalid's binary_logloss: 0.0657116\n",
      "[30]\tvalid's binary_logloss: 0.0464698\n",
      "[40]\tvalid's binary_logloss: 0.0357557\n",
      "[50]\tvalid's binary_logloss: 0.0293894\n",
      "[60]\tvalid's binary_logloss: 0.0243697\n",
      "[70]\tvalid's binary_logloss: 0.0207867\n",
      "[80]\tvalid's binary_logloss: 0.0184801\n",
      "[90]\tvalid's binary_logloss: 0.0167337\n",
      "[100]\tvalid's binary_logloss: 0.0154642\n",
      "[110]\tvalid's binary_logloss: 0.0144626\n",
      "[120]\tvalid's binary_logloss: 0.0136353\n",
      "[130]\tvalid's binary_logloss: 0.0131235\n",
      "[140]\tvalid's binary_logloss: 0.0128886\n",
      "[150]\tvalid's binary_logloss: 0.0128628\n",
      "[160]\tvalid's binary_logloss: 0.0128628\n",
      "[170]\tvalid's binary_logloss: 0.0128628\n",
      "Early stopping, best iteration is:\n",
      "[141]\tvalid's binary_logloss: 0.0128613\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.923076923076923, total=  47.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.107223\n",
      "[20]\tvalid's binary_logloss: 0.0665617\n",
      "[30]\tvalid's binary_logloss: 0.0471261\n",
      "[40]\tvalid's binary_logloss: 0.0363795\n",
      "[50]\tvalid's binary_logloss: 0.0293577\n",
      "[60]\tvalid's binary_logloss: 0.02532\n",
      "[70]\tvalid's binary_logloss: 0.0221706\n",
      "[80]\tvalid's binary_logloss: 0.0199175\n",
      "[90]\tvalid's binary_logloss: 0.0182635\n",
      "[100]\tvalid's binary_logloss: 0.0173776\n",
      "[110]\tvalid's binary_logloss: 0.0165926\n",
      "[120]\tvalid's binary_logloss: 0.0160784\n",
      "[130]\tvalid's binary_logloss: 0.0156974\n",
      "[140]\tvalid's binary_logloss: 0.0155937\n",
      "[150]\tvalid's binary_logloss: 0.0155935\n",
      "[160]\tvalid's binary_logloss: 0.0155933\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid's binary_logloss: 0.0155874\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9043478260869565, total=  41.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.106563\n",
      "[20]\tvalid's binary_logloss: 0.066306\n",
      "[30]\tvalid's binary_logloss: 0.0465082\n",
      "[40]\tvalid's binary_logloss: 0.0348962\n",
      "[50]\tvalid's binary_logloss: 0.0283762\n",
      "[60]\tvalid's binary_logloss: 0.0236916\n",
      "[70]\tvalid's binary_logloss: 0.0202501\n",
      "[80]\tvalid's binary_logloss: 0.0177053\n",
      "[90]\tvalid's binary_logloss: 0.0160001\n",
      "[100]\tvalid's binary_logloss: 0.0145489\n",
      "[110]\tvalid's binary_logloss: 0.0135572\n",
      "[120]\tvalid's binary_logloss: 0.0127758\n",
      "[130]\tvalid's binary_logloss: 0.0122089\n",
      "[140]\tvalid's binary_logloss: 0.0119301\n",
      "[150]\tvalid's binary_logloss: 0.0119301\n",
      "[160]\tvalid's binary_logloss: 0.0119301\n",
      "[170]\tvalid's binary_logloss: 0.0119301\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid's binary_logloss: 0.0119301\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  41.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.104553\n",
      "[20]\tvalid's binary_logloss: 0.0641223\n",
      "[30]\tvalid's binary_logloss: 0.0445775\n",
      "[40]\tvalid's binary_logloss: 0.0340985\n",
      "[50]\tvalid's binary_logloss: 0.0276539\n",
      "[60]\tvalid's binary_logloss: 0.0234721\n",
      "[70]\tvalid's binary_logloss: 0.0202487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80]\tvalid's binary_logloss: 0.0179621\n",
      "[90]\tvalid's binary_logloss: 0.0161029\n",
      "[100]\tvalid's binary_logloss: 0.0149027\n",
      "[110]\tvalid's binary_logloss: 0.0140906\n",
      "[120]\tvalid's binary_logloss: 0.0133135\n",
      "[130]\tvalid's binary_logloss: 0.012903\n",
      "[140]\tvalid's binary_logloss: 0.012645\n",
      "[150]\tvalid's binary_logloss: 0.012645\n",
      "[160]\tvalid's binary_logloss: 0.012645\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid's binary_logloss: 0.01263\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  42.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.107525\n",
      "[20]\tvalid's binary_logloss: 0.0654114\n",
      "[30]\tvalid's binary_logloss: 0.0449262\n",
      "[40]\tvalid's binary_logloss: 0.034442\n",
      "[50]\tvalid's binary_logloss: 0.0280405\n",
      "[60]\tvalid's binary_logloss: 0.0239741\n",
      "[70]\tvalid's binary_logloss: 0.0208156\n",
      "[80]\tvalid's binary_logloss: 0.0182959\n",
      "[90]\tvalid's binary_logloss: 0.0163328\n",
      "[100]\tvalid's binary_logloss: 0.0151082\n",
      "[110]\tvalid's binary_logloss: 0.0142365\n",
      "[120]\tvalid's binary_logloss: 0.0136786\n",
      "[130]\tvalid's binary_logloss: 0.0132103\n",
      "[140]\tvalid's binary_logloss: 0.0129845\n",
      "[150]\tvalid's binary_logloss: 0.0129851\n",
      "[160]\tvalid's binary_logloss: 0.0129853\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid's binary_logloss: 0.0129844\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  41.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.106788\n",
      "[20]\tvalid's binary_logloss: 0.0668509\n",
      "[30]\tvalid's binary_logloss: 0.047338\n",
      "[40]\tvalid's binary_logloss: 0.0361531\n",
      "[50]\tvalid's binary_logloss: 0.0287867\n",
      "[60]\tvalid's binary_logloss: 0.0245361\n",
      "[70]\tvalid's binary_logloss: 0.0217258\n",
      "[80]\tvalid's binary_logloss: 0.0197345\n",
      "[90]\tvalid's binary_logloss: 0.0181669\n",
      "[100]\tvalid's binary_logloss: 0.0170939\n",
      "[110]\tvalid's binary_logloss: 0.0165261\n",
      "[120]\tvalid's binary_logloss: 0.0159821\n",
      "[130]\tvalid's binary_logloss: 0.0156589\n",
      "[140]\tvalid's binary_logloss: 0.0155159\n",
      "[150]\tvalid's binary_logloss: 0.0155159\n",
      "[160]\tvalid's binary_logloss: 0.0155159\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid's binary_logloss: 0.0155159\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  42.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.106141\n",
      "[20]\tvalid's binary_logloss: 0.0659531\n",
      "[30]\tvalid's binary_logloss: 0.0464754\n",
      "[40]\tvalid's binary_logloss: 0.0355273\n",
      "[50]\tvalid's binary_logloss: 0.0287834\n",
      "[60]\tvalid's binary_logloss: 0.0238626\n",
      "[70]\tvalid's binary_logloss: 0.0203086\n",
      "[80]\tvalid's binary_logloss: 0.0175621\n",
      "[90]\tvalid's binary_logloss: 0.0157997\n",
      "[100]\tvalid's binary_logloss: 0.0145572\n",
      "[110]\tvalid's binary_logloss: 0.0135781\n",
      "[120]\tvalid's binary_logloss: 0.0128714\n",
      "[130]\tvalid's binary_logloss: 0.0122873\n",
      "[140]\tvalid's binary_logloss: 0.01204\n",
      "[150]\tvalid's binary_logloss: 0.01204\n",
      "[160]\tvalid's binary_logloss: 0.01204\n",
      "[170]\tvalid's binary_logloss: 0.01204\n",
      "Early stopping, best iteration is:\n",
      "[142]\tvalid's binary_logloss: 0.01204\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9137931034482758, total=  45.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105073\n",
      "[20]\tvalid's binary_logloss: 0.0634969\n",
      "[30]\tvalid's binary_logloss: 0.043598\n",
      "[40]\tvalid's binary_logloss: 0.0325657\n",
      "[50]\tvalid's binary_logloss: 0.0259775\n",
      "[60]\tvalid's binary_logloss: 0.0216568\n",
      "[70]\tvalid's binary_logloss: 0.0183566\n",
      "[80]\tvalid's binary_logloss: 0.0160596\n",
      "[90]\tvalid's binary_logloss: 0.014584\n",
      "[100]\tvalid's binary_logloss: 0.0135308\n",
      "[110]\tvalid's binary_logloss: 0.0127343\n",
      "[120]\tvalid's binary_logloss: 0.0122699\n",
      "[130]\tvalid's binary_logloss: 0.0118449\n",
      "[140]\tvalid's binary_logloss: 0.0114971\n",
      "[150]\tvalid's binary_logloss: 0.0112897\n",
      "[160]\tvalid's binary_logloss: 0.0110799\n",
      "[170]\tvalid's binary_logloss: 0.0109311\n",
      "[180]\tvalid's binary_logloss: 0.0107706\n",
      "[190]\tvalid's binary_logloss: 0.0105952\n",
      "[200]\tvalid's binary_logloss: 0.0106606\n",
      "[210]\tvalid's binary_logloss: 0.0106479\n",
      "[220]\tvalid's binary_logloss: 0.0106011\n",
      "[230]\tvalid's binary_logloss: 0.0105393\n",
      "[240]\tvalid's binary_logloss: 0.0105146\n",
      "[250]\tvalid's binary_logloss: 0.0104637\n",
      "[260]\tvalid's binary_logloss: 0.0105087\n",
      "[270]\tvalid's binary_logloss: 0.0104558\n",
      "[280]\tvalid's binary_logloss: 0.0105071\n",
      "[290]\tvalid's binary_logloss: 0.0105024\n",
      "Early stopping, best iteration is:\n",
      "[267]\tvalid's binary_logloss: 0.0104283\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  51.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.108514\n",
      "[20]\tvalid's binary_logloss: 0.0659098\n",
      "[30]\tvalid's binary_logloss: 0.0466444\n",
      "[40]\tvalid's binary_logloss: 0.0354487\n",
      "[50]\tvalid's binary_logloss: 0.0282533\n",
      "[60]\tvalid's binary_logloss: 0.0237543\n",
      "[70]\tvalid's binary_logloss: 0.0200102\n",
      "[80]\tvalid's binary_logloss: 0.0172922\n",
      "[90]\tvalid's binary_logloss: 0.0154309\n",
      "[100]\tvalid's binary_logloss: 0.0141211\n",
      "[110]\tvalid's binary_logloss: 0.0129775\n",
      "[120]\tvalid's binary_logloss: 0.0121296\n",
      "[130]\tvalid's binary_logloss: 0.0116095\n",
      "[140]\tvalid's binary_logloss: 0.0112809\n",
      "[150]\tvalid's binary_logloss: 0.0108221\n",
      "[160]\tvalid's binary_logloss: 0.0105229\n",
      "[170]\tvalid's binary_logloss: 0.0102484\n",
      "[180]\tvalid's binary_logloss: 0.0101004\n",
      "[190]\tvalid's binary_logloss: 0.00992344\n",
      "[200]\tvalid's binary_logloss: 0.00991031\n",
      "[210]\tvalid's binary_logloss: 0.00984618\n",
      "[220]\tvalid's binary_logloss: 0.0097944\n",
      "[230]\tvalid's binary_logloss: 0.00977616\n",
      "[240]\tvalid's binary_logloss: 0.00970219\n",
      "[250]\tvalid's binary_logloss: 0.00961072\n",
      "[260]\tvalid's binary_logloss: 0.00959773\n",
      "[270]\tvalid's binary_logloss: 0.00953399\n",
      "[280]\tvalid's binary_logloss: 0.00949878\n",
      "[290]\tvalid's binary_logloss: 0.00949019\n",
      "[300]\tvalid's binary_logloss: 0.00950097\n",
      "Early stopping, best iteration is:\n",
      "[279]\tvalid's binary_logloss: 0.00948442\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9411764705882353, total=  49.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.108911\n",
      "[20]\tvalid's binary_logloss: 0.0668806\n",
      "[30]\tvalid's binary_logloss: 0.0465771\n",
      "[40]\tvalid's binary_logloss: 0.0353627\n",
      "[50]\tvalid's binary_logloss: 0.0280265\n",
      "[60]\tvalid's binary_logloss: 0.0235674\n",
      "[70]\tvalid's binary_logloss: 0.0201826\n",
      "[80]\tvalid's binary_logloss: 0.0178578\n",
      "[90]\tvalid's binary_logloss: 0.0165988\n",
      "[100]\tvalid's binary_logloss: 0.0155225\n",
      "[110]\tvalid's binary_logloss: 0.0147456\n",
      "[120]\tvalid's binary_logloss: 0.014327\n",
      "[130]\tvalid's binary_logloss: 0.0139162\n",
      "[140]\tvalid's binary_logloss: 0.0138116\n",
      "[150]\tvalid's binary_logloss: 0.0136314\n",
      "[160]\tvalid's binary_logloss: 0.013631\n",
      "[170]\tvalid's binary_logloss: 0.0136222\n",
      "[180]\tvalid's binary_logloss: 0.0136329\n",
      "[190]\tvalid's binary_logloss: 0.0136771\n",
      "[200]\tvalid's binary_logloss: 0.0137022\n",
      "Early stopping, best iteration is:\n",
      "[173]\tvalid's binary_logloss: 0.0135902\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  44.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.109367\n",
      "[20]\tvalid's binary_logloss: 0.066934\n",
      "[30]\tvalid's binary_logloss: 0.0467975\n",
      "[40]\tvalid's binary_logloss: 0.0352754\n",
      "[50]\tvalid's binary_logloss: 0.0275737\n",
      "[60]\tvalid's binary_logloss: 0.0224697\n",
      "[70]\tvalid's binary_logloss: 0.0186838\n",
      "[80]\tvalid's binary_logloss: 0.0160403\n",
      "[90]\tvalid's binary_logloss: 0.0142614\n",
      "[100]\tvalid's binary_logloss: 0.0127755\n",
      "[110]\tvalid's binary_logloss: 0.0117657\n",
      "[120]\tvalid's binary_logloss: 0.0108088\n",
      "[130]\tvalid's binary_logloss: 0.0102188\n",
      "[140]\tvalid's binary_logloss: 0.0097797\n",
      "[150]\tvalid's binary_logloss: 0.00938666\n",
      "[160]\tvalid's binary_logloss: 0.00906686\n",
      "[170]\tvalid's binary_logloss: 0.00877187\n",
      "[180]\tvalid's binary_logloss: 0.00863268\n",
      "[190]\tvalid's binary_logloss: 0.0084416\n",
      "[200]\tvalid's binary_logloss: 0.00835473\n",
      "[210]\tvalid's binary_logloss: 0.00832613\n",
      "[220]\tvalid's binary_logloss: 0.0082392\n",
      "[230]\tvalid's binary_logloss: 0.008165\n",
      "[240]\tvalid's binary_logloss: 0.00813927\n",
      "[250]\tvalid's binary_logloss: 0.00811384\n",
      "[260]\tvalid's binary_logloss: 0.008079\n",
      "[270]\tvalid's binary_logloss: 0.00813439\n",
      "[280]\tvalid's binary_logloss: 0.00802023\n",
      "[290]\tvalid's binary_logloss: 0.00801244\n",
      "[300]\tvalid's binary_logloss: 0.00803158\n",
      "[310]\tvalid's binary_logloss: 0.00800437\n",
      "[320]\tvalid's binary_logloss: 0.00801984\n",
      "[330]\tvalid's binary_logloss: 0.00803649\n",
      "Early stopping, best iteration is:\n",
      "[308]\tvalid's binary_logloss: 0.00798337\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  50.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.10403\n",
      "[20]\tvalid's binary_logloss: 0.0619253\n",
      "[30]\tvalid's binary_logloss: 0.0412695\n",
      "[40]\tvalid's binary_logloss: 0.0305845\n",
      "[50]\tvalid's binary_logloss: 0.024381\n",
      "[60]\tvalid's binary_logloss: 0.020415\n",
      "[70]\tvalid's binary_logloss: 0.017838\n",
      "[80]\tvalid's binary_logloss: 0.0158273\n",
      "[90]\tvalid's binary_logloss: 0.0143852\n",
      "[100]\tvalid's binary_logloss: 0.0134324\n",
      "[110]\tvalid's binary_logloss: 0.0126329\n",
      "[120]\tvalid's binary_logloss: 0.0119836\n",
      "[130]\tvalid's binary_logloss: 0.0118424\n",
      "[140]\tvalid's binary_logloss: 0.0115534\n",
      "[150]\tvalid's binary_logloss: 0.0114087\n",
      "[160]\tvalid's binary_logloss: 0.0114295\n",
      "[170]\tvalid's binary_logloss: 0.0114933\n",
      "Early stopping, best iteration is:\n",
      "[146]\tvalid's binary_logloss: 0.0113581\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9152542372881356, total=  45.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.104828\n",
      "[20]\tvalid's binary_logloss: 0.0619801\n",
      "[30]\tvalid's binary_logloss: 0.042355\n",
      "[40]\tvalid's binary_logloss: 0.0317094\n",
      "[50]\tvalid's binary_logloss: 0.0253121\n",
      "[60]\tvalid's binary_logloss: 0.0212435\n",
      "[70]\tvalid's binary_logloss: 0.018234\n",
      "[80]\tvalid's binary_logloss: 0.0159163\n",
      "[90]\tvalid's binary_logloss: 0.0140125\n",
      "[100]\tvalid's binary_logloss: 0.0129025\n",
      "[110]\tvalid's binary_logloss: 0.0118438\n",
      "[120]\tvalid's binary_logloss: 0.0110394\n",
      "[130]\tvalid's binary_logloss: 0.010695\n",
      "[140]\tvalid's binary_logloss: 0.0102274\n",
      "[150]\tvalid's binary_logloss: 0.0100192\n",
      "[160]\tvalid's binary_logloss: 0.00994716\n",
      "[170]\tvalid's binary_logloss: 0.00977614\n",
      "[180]\tvalid's binary_logloss: 0.00965711\n",
      "[190]\tvalid's binary_logloss: 0.00966731\n",
      "[200]\tvalid's binary_logloss: 0.00966612\n",
      "[210]\tvalid's binary_logloss: 0.00967999\n",
      "[220]\tvalid's binary_logloss: 0.00960755\n",
      "Early stopping, best iteration is:\n",
      "[195]\tvalid's binary_logloss: 0.00958885\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9411764705882353, total=  46.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105891\n",
      "[20]\tvalid's binary_logloss: 0.064263\n",
      "[30]\tvalid's binary_logloss: 0.0444696\n",
      "[40]\tvalid's binary_logloss: 0.0329888\n",
      "[50]\tvalid's binary_logloss: 0.0257328\n",
      "[60]\tvalid's binary_logloss: 0.020946\n",
      "[70]\tvalid's binary_logloss: 0.0179026\n",
      "[80]\tvalid's binary_logloss: 0.0160167\n",
      "[90]\tvalid's binary_logloss: 0.014951\n",
      "[100]\tvalid's binary_logloss: 0.0139925\n",
      "[110]\tvalid's binary_logloss: 0.0135363\n",
      "[120]\tvalid's binary_logloss: 0.013268\n",
      "[130]\tvalid's binary_logloss: 0.0132706\n",
      "[140]\tvalid's binary_logloss: 0.0131513\n",
      "[150]\tvalid's binary_logloss: 0.013128\n",
      "[160]\tvalid's binary_logloss: 0.0132014\n",
      "[170]\tvalid's binary_logloss: 0.0133134\n",
      "[180]\tvalid's binary_logloss: 0.0135634\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid's binary_logloss: 0.0130475\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.8947368421052632, total=  44.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105213\n",
      "[20]\tvalid's binary_logloss: 0.063666\n",
      "[30]\tvalid's binary_logloss: 0.0439832\n",
      "[40]\tvalid's binary_logloss: 0.0312532\n",
      "[50]\tvalid's binary_logloss: 0.0244658\n",
      "[60]\tvalid's binary_logloss: 0.0196177\n",
      "[70]\tvalid's binary_logloss: 0.01662\n",
      "[80]\tvalid's binary_logloss: 0.0143578\n",
      "[90]\tvalid's binary_logloss: 0.0130189\n",
      "[100]\tvalid's binary_logloss: 0.0119034\n",
      "[110]\tvalid's binary_logloss: 0.011225\n",
      "[120]\tvalid's binary_logloss: 0.0106242\n",
      "[130]\tvalid's binary_logloss: 0.00997445\n",
      "[140]\tvalid's binary_logloss: 0.00969074\n",
      "[150]\tvalid's binary_logloss: 0.00944223\n",
      "[160]\tvalid's binary_logloss: 0.00947977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170]\tvalid's binary_logloss: 0.00919612\n",
      "[180]\tvalid's binary_logloss: 0.0091831\n",
      "[190]\tvalid's binary_logloss: 0.00918958\n",
      "[200]\tvalid's binary_logloss: 0.00915935\n",
      "[210]\tvalid's binary_logloss: 0.00913588\n",
      "[220]\tvalid's binary_logloss: 0.00916788\n",
      "[230]\tvalid's binary_logloss: 0.0092328\n",
      "Early stopping, best iteration is:\n",
      "[203]\tvalid's binary_logloss: 0.00907183\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  46.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.103311\n",
      "[20]\tvalid's binary_logloss: 0.0624206\n",
      "[30]\tvalid's binary_logloss: 0.0420341\n",
      "[40]\tvalid's binary_logloss: 0.0310373\n",
      "[50]\tvalid's binary_logloss: 0.0253095\n",
      "[60]\tvalid's binary_logloss: 0.0209138\n",
      "[70]\tvalid's binary_logloss: 0.0180477\n",
      "[80]\tvalid's binary_logloss: 0.0157355\n",
      "[90]\tvalid's binary_logloss: 0.0143605\n",
      "[100]\tvalid's binary_logloss: 0.013181\n",
      "[110]\tvalid's binary_logloss: 0.0123085\n",
      "[120]\tvalid's binary_logloss: 0.0118526\n",
      "[130]\tvalid's binary_logloss: 0.0115192\n",
      "[140]\tvalid's binary_logloss: 0.011445\n",
      "[150]\tvalid's binary_logloss: 0.0114706\n",
      "[160]\tvalid's binary_logloss: 0.0111286\n",
      "[170]\tvalid's binary_logloss: 0.0113165\n",
      "[180]\tvalid's binary_logloss: 0.0113453\n",
      "[190]\tvalid's binary_logloss: 0.0114473\n",
      "Early stopping, best iteration is:\n",
      "[160]\tvalid's binary_logloss: 0.0111286\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  45.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105185\n",
      "[20]\tvalid's binary_logloss: 0.0624537\n",
      "[30]\tvalid's binary_logloss: 0.0425726\n",
      "[40]\tvalid's binary_logloss: 0.0323354\n",
      "[50]\tvalid's binary_logloss: 0.0261086\n",
      "[60]\tvalid's binary_logloss: 0.0218966\n",
      "[70]\tvalid's binary_logloss: 0.0188421\n",
      "[80]\tvalid's binary_logloss: 0.0164398\n",
      "[90]\tvalid's binary_logloss: 0.0145015\n",
      "[100]\tvalid's binary_logloss: 0.0133203\n",
      "[110]\tvalid's binary_logloss: 0.0125799\n",
      "[120]\tvalid's binary_logloss: 0.0118984\n",
      "[130]\tvalid's binary_logloss: 0.0114483\n",
      "[140]\tvalid's binary_logloss: 0.0109922\n",
      "[150]\tvalid's binary_logloss: 0.0110174\n",
      "[160]\tvalid's binary_logloss: 0.0107704\n",
      "[170]\tvalid's binary_logloss: 0.0106354\n",
      "[180]\tvalid's binary_logloss: 0.0104663\n",
      "[190]\tvalid's binary_logloss: 0.0106085\n",
      "[200]\tvalid's binary_logloss: 0.0105117\n",
      "[210]\tvalid's binary_logloss: 0.0106258\n",
      "Early stopping, best iteration is:\n",
      "[180]\tvalid's binary_logloss: 0.0104663\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  45.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.106244\n",
      "[20]\tvalid's binary_logloss: 0.0641992\n",
      "[30]\tvalid's binary_logloss: 0.0444306\n",
      "[40]\tvalid's binary_logloss: 0.0328603\n",
      "[50]\tvalid's binary_logloss: 0.0258487\n",
      "[60]\tvalid's binary_logloss: 0.0219757\n",
      "[70]\tvalid's binary_logloss: 0.0192139\n",
      "[80]\tvalid's binary_logloss: 0.0170575\n",
      "[90]\tvalid's binary_logloss: 0.0155943\n",
      "[100]\tvalid's binary_logloss: 0.0146334\n",
      "[110]\tvalid's binary_logloss: 0.0140324\n",
      "[120]\tvalid's binary_logloss: 0.0136262\n",
      "[130]\tvalid's binary_logloss: 0.0135501\n",
      "[140]\tvalid's binary_logloss: 0.0136438\n",
      "[150]\tvalid's binary_logloss: 0.0137461\n",
      "[160]\tvalid's binary_logloss: 0.0140524\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid's binary_logloss: 0.0135288\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  44.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.103877\n",
      "[20]\tvalid's binary_logloss: 0.0626593\n",
      "[30]\tvalid's binary_logloss: 0.042861\n",
      "[40]\tvalid's binary_logloss: 0.0308303\n",
      "[50]\tvalid's binary_logloss: 0.0238472\n",
      "[60]\tvalid's binary_logloss: 0.0192712\n",
      "[70]\tvalid's binary_logloss: 0.0163761\n",
      "[80]\tvalid's binary_logloss: 0.0143597\n",
      "[90]\tvalid's binary_logloss: 0.0129875\n",
      "[100]\tvalid's binary_logloss: 0.0115306\n",
      "[110]\tvalid's binary_logloss: 0.0109007\n",
      "[120]\tvalid's binary_logloss: 0.0101833\n",
      "[130]\tvalid's binary_logloss: 0.00939737\n",
      "[140]\tvalid's binary_logloss: 0.00902941\n",
      "[150]\tvalid's binary_logloss: 0.00893926\n",
      "[160]\tvalid's binary_logloss: 0.00872862\n",
      "[170]\tvalid's binary_logloss: 0.00859869\n",
      "[180]\tvalid's binary_logloss: 0.00860693\n",
      "[190]\tvalid's binary_logloss: 0.00877435\n",
      "[200]\tvalid's binary_logloss: 0.00904169\n",
      "Early stopping, best iteration is:\n",
      "[170]\tvalid's binary_logloss: 0.00859869\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  45.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105402\n",
      "[20]\tvalid's binary_logloss: 0.0637107\n",
      "[30]\tvalid's binary_logloss: 0.0441163\n",
      "[40]\tvalid's binary_logloss: 0.0327957\n",
      "[50]\tvalid's binary_logloss: 0.0261293\n",
      "[60]\tvalid's binary_logloss: 0.0216882\n",
      "[70]\tvalid's binary_logloss: 0.0185559\n",
      "[80]\tvalid's binary_logloss: 0.0163256\n",
      "[90]\tvalid's binary_logloss: 0.0147613\n",
      "[100]\tvalid's binary_logloss: 0.013434\n",
      "[110]\tvalid's binary_logloss: 0.0125084\n",
      "[120]\tvalid's binary_logloss: 0.0119322\n",
      "[130]\tvalid's binary_logloss: 0.0114275\n",
      "[140]\tvalid's binary_logloss: 0.0108397\n",
      "[150]\tvalid's binary_logloss: 0.0107366\n",
      "[160]\tvalid's binary_logloss: 0.0104506\n",
      "[170]\tvalid's binary_logloss: 0.0102861\n",
      "[180]\tvalid's binary_logloss: 0.0102196\n",
      "[190]\tvalid's binary_logloss: 0.0101319\n",
      "[200]\tvalid's binary_logloss: 0.0100741\n",
      "[210]\tvalid's binary_logloss: 0.010062\n",
      "[220]\tvalid's binary_logloss: 0.00998625\n",
      "[230]\tvalid's binary_logloss: 0.0099825\n",
      "[240]\tvalid's binary_logloss: 0.00996906\n",
      "[250]\tvalid's binary_logloss: 0.00995443\n",
      "[260]\tvalid's binary_logloss: 0.00994827\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid's binary_logloss: 0.00993362\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  46.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.10869\n",
      "[20]\tvalid's binary_logloss: 0.0668116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalid's binary_logloss: 0.0475331\n",
      "[40]\tvalid's binary_logloss: 0.0361981\n",
      "[50]\tvalid's binary_logloss: 0.0291257\n",
      "[60]\tvalid's binary_logloss: 0.0242729\n",
      "[70]\tvalid's binary_logloss: 0.021108\n",
      "[80]\tvalid's binary_logloss: 0.0181725\n",
      "[90]\tvalid's binary_logloss: 0.0159832\n",
      "[100]\tvalid's binary_logloss: 0.0146482\n",
      "[110]\tvalid's binary_logloss: 0.0134766\n",
      "[120]\tvalid's binary_logloss: 0.0125574\n",
      "[130]\tvalid's binary_logloss: 0.0119916\n",
      "[140]\tvalid's binary_logloss: 0.0114239\n",
      "[150]\tvalid's binary_logloss: 0.0110395\n",
      "[160]\tvalid's binary_logloss: 0.0108293\n",
      "[170]\tvalid's binary_logloss: 0.0105654\n",
      "[180]\tvalid's binary_logloss: 0.0103841\n",
      "[190]\tvalid's binary_logloss: 0.0102782\n",
      "[200]\tvalid's binary_logloss: 0.0101514\n",
      "[210]\tvalid's binary_logloss: 0.00996005\n",
      "[220]\tvalid's binary_logloss: 0.00986726\n",
      "[230]\tvalid's binary_logloss: 0.00974528\n",
      "[240]\tvalid's binary_logloss: 0.00974271\n",
      "[250]\tvalid's binary_logloss: 0.00967982\n",
      "[260]\tvalid's binary_logloss: 0.00959037\n",
      "[270]\tvalid's binary_logloss: 0.00947986\n",
      "[280]\tvalid's binary_logloss: 0.00943935\n",
      "[290]\tvalid's binary_logloss: 0.00939352\n",
      "[300]\tvalid's binary_logloss: 0.00938363\n",
      "[310]\tvalid's binary_logloss: 0.00932688\n",
      "[320]\tvalid's binary_logloss: 0.00929186\n",
      "[330]\tvalid's binary_logloss: 0.00929904\n",
      "[340]\tvalid's binary_logloss: 0.00925993\n",
      "[350]\tvalid's binary_logloss: 0.00920759\n",
      "[360]\tvalid's binary_logloss: 0.00920123\n",
      "[370]\tvalid's binary_logloss: 0.00919729\n",
      "[380]\tvalid's binary_logloss: 0.00917266\n",
      "[390]\tvalid's binary_logloss: 0.00917508\n",
      "[400]\tvalid's binary_logloss: 0.00913585\n",
      "[410]\tvalid's binary_logloss: 0.00912738\n",
      "[420]\tvalid's binary_logloss: 0.00912283\n",
      "[430]\tvalid's binary_logloss: 0.00910591\n",
      "[440]\tvalid's binary_logloss: 0.00910413\n",
      "[450]\tvalid's binary_logloss: 0.00910395\n",
      "[460]\tvalid's binary_logloss: 0.00910385\n",
      "Early stopping, best iteration is:\n",
      "[436]\tvalid's binary_logloss: 0.00910289\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  51.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.109109\n",
      "[20]\tvalid's binary_logloss: 0.0672676\n",
      "[30]\tvalid's binary_logloss: 0.0476093\n",
      "[40]\tvalid's binary_logloss: 0.0358905\n",
      "[50]\tvalid's binary_logloss: 0.0285521\n",
      "[60]\tvalid's binary_logloss: 0.0238874\n",
      "[70]\tvalid's binary_logloss: 0.0206176\n",
      "[80]\tvalid's binary_logloss: 0.0186732\n",
      "[90]\tvalid's binary_logloss: 0.0173618\n",
      "[100]\tvalid's binary_logloss: 0.0162628\n",
      "[110]\tvalid's binary_logloss: 0.0155634\n",
      "[120]\tvalid's binary_logloss: 0.015166\n",
      "[130]\tvalid's binary_logloss: 0.0147343\n",
      "[140]\tvalid's binary_logloss: 0.0145307\n",
      "[150]\tvalid's binary_logloss: 0.0143597\n",
      "[160]\tvalid's binary_logloss: 0.0142849\n",
      "[170]\tvalid's binary_logloss: 0.0142546\n",
      "[180]\tvalid's binary_logloss: 0.0141815\n",
      "[190]\tvalid's binary_logloss: 0.0141349\n",
      "[200]\tvalid's binary_logloss: 0.0141874\n",
      "[210]\tvalid's binary_logloss: 0.0141289\n",
      "[220]\tvalid's binary_logloss: 0.0140992\n",
      "[230]\tvalid's binary_logloss: 0.0140921\n",
      "[240]\tvalid's binary_logloss: 0.0140917\n",
      "[250]\tvalid's binary_logloss: 0.0140785\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid's binary_logloss: 0.0140524\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  45.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.109522\n",
      "[20]\tvalid's binary_logloss: 0.0671603\n",
      "[30]\tvalid's binary_logloss: 0.0474713\n",
      "[40]\tvalid's binary_logloss: 0.0342015\n",
      "[50]\tvalid's binary_logloss: 0.0272037\n",
      "[60]\tvalid's binary_logloss: 0.0219822\n",
      "[70]\tvalid's binary_logloss: 0.0183311\n",
      "[80]\tvalid's binary_logloss: 0.0159279\n",
      "[90]\tvalid's binary_logloss: 0.0138834\n",
      "[100]\tvalid's binary_logloss: 0.012629\n",
      "[110]\tvalid's binary_logloss: 0.0115336\n",
      "[120]\tvalid's binary_logloss: 0.0108353\n",
      "[130]\tvalid's binary_logloss: 0.0102461\n",
      "[140]\tvalid's binary_logloss: 0.00976592\n",
      "[150]\tvalid's binary_logloss: 0.00940867\n",
      "[160]\tvalid's binary_logloss: 0.00918258\n",
      "[170]\tvalid's binary_logloss: 0.00898923\n",
      "[180]\tvalid's binary_logloss: 0.00877394\n",
      "[190]\tvalid's binary_logloss: 0.00856944\n",
      "[200]\tvalid's binary_logloss: 0.00846448\n",
      "[210]\tvalid's binary_logloss: 0.008384\n",
      "[220]\tvalid's binary_logloss: 0.00828709\n",
      "[230]\tvalid's binary_logloss: 0.00825024\n",
      "[240]\tvalid's binary_logloss: 0.00816006\n",
      "[250]\tvalid's binary_logloss: 0.00807546\n",
      "[260]\tvalid's binary_logloss: 0.00801484\n",
      "[270]\tvalid's binary_logloss: 0.0079977\n",
      "[280]\tvalid's binary_logloss: 0.00794342\n",
      "[290]\tvalid's binary_logloss: 0.0079156\n",
      "[300]\tvalid's binary_logloss: 0.00788788\n",
      "[310]\tvalid's binary_logloss: 0.00788281\n",
      "[320]\tvalid's binary_logloss: 0.00789617\n",
      "[330]\tvalid's binary_logloss: 0.00788187\n",
      "[340]\tvalid's binary_logloss: 0.00786271\n",
      "[350]\tvalid's binary_logloss: 0.00785412\n",
      "[360]\tvalid's binary_logloss: 0.0078348\n",
      "[370]\tvalid's binary_logloss: 0.00786589\n",
      "[380]\tvalid's binary_logloss: 0.00782069\n",
      "[390]\tvalid's binary_logloss: 0.00782201\n",
      "[400]\tvalid's binary_logloss: 0.00780834\n",
      "[410]\tvalid's binary_logloss: 0.00779289\n",
      "[420]\tvalid's binary_logloss: 0.00778177\n",
      "[430]\tvalid's binary_logloss: 0.00777264\n",
      "[440]\tvalid's binary_logloss: 0.00777648\n",
      "[450]\tvalid's binary_logloss: 0.00777648\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalid's binary_logloss: 0.00777112\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  51.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.104236\n",
      "[20]\tvalid's binary_logloss: 0.0621943\n",
      "[30]\tvalid's binary_logloss: 0.0421176\n",
      "[40]\tvalid's binary_logloss: 0.0314421\n",
      "[50]\tvalid's binary_logloss: 0.0240942\n",
      "[60]\tvalid's binary_logloss: 0.02\n",
      "[70]\tvalid's binary_logloss: 0.0171374\n",
      "[80]\tvalid's binary_logloss: 0.0152168\n",
      "[90]\tvalid's binary_logloss: 0.0140418\n",
      "[100]\tvalid's binary_logloss: 0.0129109\n",
      "[110]\tvalid's binary_logloss: 0.0123042\n",
      "[120]\tvalid's binary_logloss: 0.0118722\n",
      "[130]\tvalid's binary_logloss: 0.0114726\n",
      "[140]\tvalid's binary_logloss: 0.0108827\n",
      "[150]\tvalid's binary_logloss: 0.010671\n",
      "[160]\tvalid's binary_logloss: 0.010762\n",
      "[170]\tvalid's binary_logloss: 0.0106864\n",
      "[180]\tvalid's binary_logloss: 0.0105873\n",
      "[190]\tvalid's binary_logloss: 0.0105303\n",
      "[200]\tvalid's binary_logloss: 0.010483\n",
      "[210]\tvalid's binary_logloss: 0.0105228\n",
      "[220]\tvalid's binary_logloss: 0.010514\n",
      "[230]\tvalid's binary_logloss: 0.010544\n",
      "Early stopping, best iteration is:\n",
      "[202]\tvalid's binary_logloss: 0.0104714\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9243697478991596, total=  43.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105025\n",
      "[20]\tvalid's binary_logloss: 0.0624325\n",
      "[30]\tvalid's binary_logloss: 0.0429629\n",
      "[40]\tvalid's binary_logloss: 0.032295\n",
      "[50]\tvalid's binary_logloss: 0.0260916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60]\tvalid's binary_logloss: 0.0221975\n",
      "[70]\tvalid's binary_logloss: 0.0190747\n",
      "[80]\tvalid's binary_logloss: 0.0172067\n",
      "[90]\tvalid's binary_logloss: 0.0152815\n",
      "[100]\tvalid's binary_logloss: 0.0139322\n",
      "[110]\tvalid's binary_logloss: 0.0129463\n",
      "[120]\tvalid's binary_logloss: 0.0120926\n",
      "[130]\tvalid's binary_logloss: 0.011658\n",
      "[140]\tvalid's binary_logloss: 0.0112719\n",
      "[150]\tvalid's binary_logloss: 0.0110219\n",
      "[160]\tvalid's binary_logloss: 0.0107245\n",
      "[170]\tvalid's binary_logloss: 0.0105287\n",
      "[180]\tvalid's binary_logloss: 0.0105008\n",
      "[190]\tvalid's binary_logloss: 0.0103936\n",
      "[200]\tvalid's binary_logloss: 0.0103401\n",
      "[210]\tvalid's binary_logloss: 0.0103094\n",
      "[220]\tvalid's binary_logloss: 0.0102439\n",
      "[230]\tvalid's binary_logloss: 0.0102136\n",
      "[240]\tvalid's binary_logloss: 0.0101844\n",
      "[250]\tvalid's binary_logloss: 0.0101786\n",
      "[260]\tvalid's binary_logloss: 0.0101786\n",
      "Early stopping, best iteration is:\n",
      "[238]\tvalid's binary_logloss: 0.0101567\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  42.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105467\n",
      "[20]\tvalid's binary_logloss: 0.0639552\n",
      "[30]\tvalid's binary_logloss: 0.0441868\n",
      "[40]\tvalid's binary_logloss: 0.0336782\n",
      "[50]\tvalid's binary_logloss: 0.0261812\n",
      "[60]\tvalid's binary_logloss: 0.0219006\n",
      "[70]\tvalid's binary_logloss: 0.0189318\n",
      "[80]\tvalid's binary_logloss: 0.0170567\n",
      "[90]\tvalid's binary_logloss: 0.0157642\n",
      "[100]\tvalid's binary_logloss: 0.0149296\n",
      "[110]\tvalid's binary_logloss: 0.0143259\n",
      "[120]\tvalid's binary_logloss: 0.0141404\n",
      "[130]\tvalid's binary_logloss: 0.0139556\n",
      "[140]\tvalid's binary_logloss: 0.0139998\n",
      "[150]\tvalid's binary_logloss: 0.0138837\n",
      "[160]\tvalid's binary_logloss: 0.0138824\n",
      "[170]\tvalid's binary_logloss: 0.0138623\n",
      "[180]\tvalid's binary_logloss: 0.0139068\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid's binary_logloss: 0.0138295\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.8947368421052632, total=  40.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105214\n",
      "[20]\tvalid's binary_logloss: 0.0641103\n",
      "[30]\tvalid's binary_logloss: 0.0438163\n",
      "[40]\tvalid's binary_logloss: 0.0315665\n",
      "[50]\tvalid's binary_logloss: 0.0247837\n",
      "[60]\tvalid's binary_logloss: 0.0199875\n",
      "[70]\tvalid's binary_logloss: 0.0169589\n",
      "[80]\tvalid's binary_logloss: 0.0150651\n",
      "[90]\tvalid's binary_logloss: 0.0131812\n",
      "[100]\tvalid's binary_logloss: 0.0119113\n",
      "[110]\tvalid's binary_logloss: 0.0110215\n",
      "[120]\tvalid's binary_logloss: 0.0102952\n",
      "[130]\tvalid's binary_logloss: 0.00970525\n",
      "[140]\tvalid's binary_logloss: 0.00936956\n",
      "[150]\tvalid's binary_logloss: 0.0089747\n",
      "[160]\tvalid's binary_logloss: 0.00872207\n",
      "[170]\tvalid's binary_logloss: 0.00879511\n",
      "[180]\tvalid's binary_logloss: 0.00871814\n",
      "[190]\tvalid's binary_logloss: 0.00862583\n",
      "[200]\tvalid's binary_logloss: 0.00862792\n",
      "[210]\tvalid's binary_logloss: 0.00858166\n",
      "[220]\tvalid's binary_logloss: 0.00861449\n",
      "[230]\tvalid's binary_logloss: 0.0085606\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid's binary_logloss: 0.00853928\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.923076923076923, total=  42.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.103867\n",
      "[20]\tvalid's binary_logloss: 0.0621888\n",
      "[30]\tvalid's binary_logloss: 0.0421255\n",
      "[40]\tvalid's binary_logloss: 0.0313438\n",
      "[50]\tvalid's binary_logloss: 0.0249743\n",
      "[60]\tvalid's binary_logloss: 0.0207917\n",
      "[70]\tvalid's binary_logloss: 0.0180242\n",
      "[80]\tvalid's binary_logloss: 0.0156794\n",
      "[90]\tvalid's binary_logloss: 0.0141928\n",
      "[100]\tvalid's binary_logloss: 0.0130816\n",
      "[110]\tvalid's binary_logloss: 0.0122489\n",
      "[120]\tvalid's binary_logloss: 0.011709\n",
      "[130]\tvalid's binary_logloss: 0.0112775\n",
      "[140]\tvalid's binary_logloss: 0.0112097\n",
      "[150]\tvalid's binary_logloss: 0.0111554\n",
      "[160]\tvalid's binary_logloss: 0.0109929\n",
      "[170]\tvalid's binary_logloss: 0.0110387\n",
      "[180]\tvalid's binary_logloss: 0.0110157\n",
      "[190]\tvalid's binary_logloss: 0.011198\n",
      "[200]\tvalid's binary_logloss: 0.0111097\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid's binary_logloss: 0.0109535\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9152542372881356, total=  44.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105278\n",
      "[20]\tvalid's binary_logloss: 0.0625434\n",
      "[30]\tvalid's binary_logloss: 0.043132\n",
      "[40]\tvalid's binary_logloss: 0.0333392\n",
      "[50]\tvalid's binary_logloss: 0.0260549\n",
      "[60]\tvalid's binary_logloss: 0.0217457\n",
      "[70]\tvalid's binary_logloss: 0.0186995\n",
      "[80]\tvalid's binary_logloss: 0.0160965\n",
      "[90]\tvalid's binary_logloss: 0.0142165\n",
      "[100]\tvalid's binary_logloss: 0.0131518\n",
      "[110]\tvalid's binary_logloss: 0.0121506\n",
      "[120]\tvalid's binary_logloss: 0.011207\n",
      "[130]\tvalid's binary_logloss: 0.0107442\n",
      "[140]\tvalid's binary_logloss: 0.0105319\n",
      "[150]\tvalid's binary_logloss: 0.0102157\n",
      "[160]\tvalid's binary_logloss: 0.00996698\n",
      "[170]\tvalid's binary_logloss: 0.00991044\n",
      "[180]\tvalid's binary_logloss: 0.00971893\n",
      "[190]\tvalid's binary_logloss: 0.00973769\n",
      "[200]\tvalid's binary_logloss: 0.00962952\n",
      "[210]\tvalid's binary_logloss: 0.00954575\n",
      "[220]\tvalid's binary_logloss: 0.00950127\n",
      "[230]\tvalid's binary_logloss: 0.00949896\n",
      "[240]\tvalid's binary_logloss: 0.00949896\n",
      "Early stopping, best iteration is:\n",
      "[219]\tvalid's binary_logloss: 0.00949166\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9411764705882353, total=  44.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105628\n",
      "[20]\tvalid's binary_logloss: 0.0642127\n",
      "[30]\tvalid's binary_logloss: 0.0445031\n",
      "[40]\tvalid's binary_logloss: 0.0327512\n",
      "[50]\tvalid's binary_logloss: 0.025844\n",
      "[60]\tvalid's binary_logloss: 0.0213218\n",
      "[70]\tvalid's binary_logloss: 0.0185541\n",
      "[80]\tvalid's binary_logloss: 0.0167364\n",
      "[90]\tvalid's binary_logloss: 0.0152023\n",
      "[100]\tvalid's binary_logloss: 0.0143267\n",
      "[110]\tvalid's binary_logloss: 0.0139999\n",
      "[120]\tvalid's binary_logloss: 0.0137595\n",
      "[130]\tvalid's binary_logloss: 0.0134968\n",
      "[140]\tvalid's binary_logloss: 0.0133995\n",
      "[150]\tvalid's binary_logloss: 0.0133898\n",
      "[160]\tvalid's binary_logloss: 0.0134109\n",
      "[170]\tvalid's binary_logloss: 0.0135475\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid's binary_logloss: 0.0133567\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.8947368421052632, total=  43.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.103784\n",
      "[20]\tvalid's binary_logloss: 0.0628516\n",
      "[30]\tvalid's binary_logloss: 0.0433051\n",
      "[40]\tvalid's binary_logloss: 0.0311816\n",
      "[50]\tvalid's binary_logloss: 0.0243108\n",
      "[60]\tvalid's binary_logloss: 0.0197721\n",
      "[70]\tvalid's binary_logloss: 0.0165249\n",
      "[80]\tvalid's binary_logloss: 0.0144207\n",
      "[90]\tvalid's binary_logloss: 0.0128558\n",
      "[100]\tvalid's binary_logloss: 0.0116563\n",
      "[110]\tvalid's binary_logloss: 0.010876\n",
      "[120]\tvalid's binary_logloss: 0.0104144\n",
      "[130]\tvalid's binary_logloss: 0.00985106\n",
      "[140]\tvalid's binary_logloss: 0.00935447\n",
      "[150]\tvalid's binary_logloss: 0.00898322\n",
      "[160]\tvalid's binary_logloss: 0.00878228\n",
      "[170]\tvalid's binary_logloss: 0.00858587\n",
      "[180]\tvalid's binary_logloss: 0.00861058\n",
      "[190]\tvalid's binary_logloss: 0.00858813\n",
      "[200]\tvalid's binary_logloss: 0.00858622\n",
      "[210]\tvalid's binary_logloss: 0.00849273\n",
      "[220]\tvalid's binary_logloss: 0.00853743\n",
      "[230]\tvalid's binary_logloss: 0.00854105\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid's binary_logloss: 0.00848469\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  44.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.108773\n",
      "[20]\tvalid's binary_logloss: 0.0671537\n",
      "[30]\tvalid's binary_logloss: 0.0470154\n",
      "[40]\tvalid's binary_logloss: 0.0362742\n",
      "[50]\tvalid's binary_logloss: 0.0297902\n",
      "[60]\tvalid's binary_logloss: 0.0251553\n",
      "[70]\tvalid's binary_logloss: 0.0222214\n",
      "[80]\tvalid's binary_logloss: 0.0200956\n",
      "[90]\tvalid's binary_logloss: 0.0181506\n",
      "[100]\tvalid's binary_logloss: 0.016747\n",
      "[110]\tvalid's binary_logloss: 0.015682\n",
      "[120]\tvalid's binary_logloss: 0.0148516\n",
      "[130]\tvalid's binary_logloss: 0.0144012\n",
      "[140]\tvalid's binary_logloss: 0.0138481\n",
      "[150]\tvalid's binary_logloss: 0.0134804\n",
      "[160]\tvalid's binary_logloss: 0.0131931\n",
      "[170]\tvalid's binary_logloss: 0.0131664\n",
      "[180]\tvalid's binary_logloss: 0.0131664\n",
      "[190]\tvalid's binary_logloss: 0.0131664\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid's binary_logloss: 0.0131561\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.923076923076923, total=  42.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.110331\n",
      "[20]\tvalid's binary_logloss: 0.0677529\n",
      "[30]\tvalid's binary_logloss: 0.0480082\n",
      "[40]\tvalid's binary_logloss: 0.0377166\n",
      "[50]\tvalid's binary_logloss: 0.0311004\n",
      "[60]\tvalid's binary_logloss: 0.0263283\n",
      "[70]\tvalid's binary_logloss: 0.0230037\n",
      "[80]\tvalid's binary_logloss: 0.0203579\n",
      "[90]\tvalid's binary_logloss: 0.018337\n",
      "[100]\tvalid's binary_logloss: 0.0169335\n",
      "[110]\tvalid's binary_logloss: 0.0159892\n",
      "[120]\tvalid's binary_logloss: 0.0151811\n",
      "[130]\tvalid's binary_logloss: 0.0145804\n",
      "[140]\tvalid's binary_logloss: 0.0140676\n",
      "[150]\tvalid's binary_logloss: 0.0136231\n",
      "[160]\tvalid's binary_logloss: 0.0133057\n",
      "[170]\tvalid's binary_logloss: 0.0132803\n",
      "[180]\tvalid's binary_logloss: 0.0132803\n",
      "[190]\tvalid's binary_logloss: 0.0132803\n",
      "Early stopping, best iteration is:\n",
      "[165]\tvalid's binary_logloss: 0.0132803\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9152542372881356, total=  42.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.110583\n",
      "[20]\tvalid's binary_logloss: 0.0687046\n",
      "[30]\tvalid's binary_logloss: 0.0497556\n",
      "[40]\tvalid's binary_logloss: 0.0381143\n",
      "[50]\tvalid's binary_logloss: 0.0311464\n",
      "[60]\tvalid's binary_logloss: 0.0265839\n",
      "[70]\tvalid's binary_logloss: 0.0236219\n",
      "[80]\tvalid's binary_logloss: 0.0214025\n",
      "[90]\tvalid's binary_logloss: 0.0196905\n",
      "[100]\tvalid's binary_logloss: 0.018568\n",
      "[110]\tvalid's binary_logloss: 0.0177502\n",
      "[120]\tvalid's binary_logloss: 0.0170255\n",
      "[130]\tvalid's binary_logloss: 0.0166075\n",
      "[140]\tvalid's binary_logloss: 0.0162486\n",
      "[150]\tvalid's binary_logloss: 0.0160535\n",
      "[160]\tvalid's binary_logloss: 0.0158476\n",
      "[170]\tvalid's binary_logloss: 0.0158476\n",
      "[180]\tvalid's binary_logloss: 0.0158476\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid's binary_logloss: 0.0158472\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  41.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.110713\n",
      "[20]\tvalid's binary_logloss: 0.0685966\n",
      "[30]\tvalid's binary_logloss: 0.0492621\n",
      "[40]\tvalid's binary_logloss: 0.0373524\n",
      "[50]\tvalid's binary_logloss: 0.0303208\n",
      "[60]\tvalid's binary_logloss: 0.0252708\n",
      "[70]\tvalid's binary_logloss: 0.0218126\n",
      "[80]\tvalid's binary_logloss: 0.0191783\n",
      "[90]\tvalid's binary_logloss: 0.0172794\n",
      "[100]\tvalid's binary_logloss: 0.0158893\n",
      "[110]\tvalid's binary_logloss: 0.0147904\n",
      "[120]\tvalid's binary_logloss: 0.0138817\n",
      "[130]\tvalid's binary_logloss: 0.0131382\n",
      "[140]\tvalid's binary_logloss: 0.0126031\n",
      "[150]\tvalid's binary_logloss: 0.0120814\n",
      "[160]\tvalid's binary_logloss: 0.011814\n",
      "[170]\tvalid's binary_logloss: 0.0116958\n",
      "[180]\tvalid's binary_logloss: 0.0116959\n",
      "[190]\tvalid's binary_logloss: 0.0116959\n",
      "Early stopping, best iteration is:\n",
      "[164]\tvalid's binary_logloss: 0.0116957\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9243697478991596, total=  41.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.104393\n",
      "[20]\tvalid's binary_logloss: 0.0638198\n",
      "[30]\tvalid's binary_logloss: 0.0438652\n",
      "[40]\tvalid's binary_logloss: 0.0339155\n",
      "[50]\tvalid's binary_logloss: 0.027732\n",
      "[60]\tvalid's binary_logloss: 0.0237592\n",
      "[70]\tvalid's binary_logloss: 0.0204336\n",
      "[80]\tvalid's binary_logloss: 0.0180311\n",
      "[90]\tvalid's binary_logloss: 0.0165189\n",
      "[100]\tvalid's binary_logloss: 0.0153091\n",
      "[110]\tvalid's binary_logloss: 0.0145361\n",
      "[120]\tvalid's binary_logloss: 0.0137665\n",
      "[130]\tvalid's binary_logloss: 0.0134122\n",
      "[140]\tvalid's binary_logloss: 0.0130873\n",
      "[150]\tvalid's binary_logloss: 0.0130873\n",
      "[160]\tvalid's binary_logloss: 0.0130873\n",
      "[170]\tvalid's binary_logloss: 0.0130873\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid's binary_logloss: 0.0130873\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  42.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.107846\n",
      "[20]\tvalid's binary_logloss: 0.0657116\n",
      "[30]\tvalid's binary_logloss: 0.0464698\n",
      "[40]\tvalid's binary_logloss: 0.0357557\n",
      "[50]\tvalid's binary_logloss: 0.0293894\n",
      "[60]\tvalid's binary_logloss: 0.0243697\n",
      "[70]\tvalid's binary_logloss: 0.0207867\n",
      "[80]\tvalid's binary_logloss: 0.0184801\n",
      "[90]\tvalid's binary_logloss: 0.0167337\n",
      "[100]\tvalid's binary_logloss: 0.0154642\n",
      "[110]\tvalid's binary_logloss: 0.0144626\n",
      "[120]\tvalid's binary_logloss: 0.0136353\n",
      "[130]\tvalid's binary_logloss: 0.0131235\n",
      "[140]\tvalid's binary_logloss: 0.0128886\n",
      "[150]\tvalid's binary_logloss: 0.0128628\n",
      "[160]\tvalid's binary_logloss: 0.0128628\n",
      "[170]\tvalid's binary_logloss: 0.0128628\n",
      "Early stopping, best iteration is:\n",
      "[141]\tvalid's binary_logloss: 0.0128613\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.923076923076923, total=  41.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.107223\n",
      "[20]\tvalid's binary_logloss: 0.0665617\n",
      "[30]\tvalid's binary_logloss: 0.0471261\n",
      "[40]\tvalid's binary_logloss: 0.0363795\n",
      "[50]\tvalid's binary_logloss: 0.0293577\n",
      "[60]\tvalid's binary_logloss: 0.02532\n",
      "[70]\tvalid's binary_logloss: 0.0221706\n",
      "[80]\tvalid's binary_logloss: 0.0199175\n",
      "[90]\tvalid's binary_logloss: 0.0182635\n",
      "[100]\tvalid's binary_logloss: 0.0173776\n",
      "[110]\tvalid's binary_logloss: 0.0165926\n",
      "[120]\tvalid's binary_logloss: 0.0160784\n",
      "[130]\tvalid's binary_logloss: 0.0156974\n",
      "[140]\tvalid's binary_logloss: 0.0155937\n",
      "[150]\tvalid's binary_logloss: 0.0155935\n",
      "[160]\tvalid's binary_logloss: 0.0155933\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid's binary_logloss: 0.0155874\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9043478260869565, total=  40.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.106563\n",
      "[20]\tvalid's binary_logloss: 0.066306\n",
      "[30]\tvalid's binary_logloss: 0.0465082\n",
      "[40]\tvalid's binary_logloss: 0.0348962\n",
      "[50]\tvalid's binary_logloss: 0.0283762\n",
      "[60]\tvalid's binary_logloss: 0.0236916\n",
      "[70]\tvalid's binary_logloss: 0.0202501\n",
      "[80]\tvalid's binary_logloss: 0.0177053\n",
      "[90]\tvalid's binary_logloss: 0.0160001\n",
      "[100]\tvalid's binary_logloss: 0.0145489\n",
      "[110]\tvalid's binary_logloss: 0.0135572\n",
      "[120]\tvalid's binary_logloss: 0.0127758\n",
      "[130]\tvalid's binary_logloss: 0.0122089\n",
      "[140]\tvalid's binary_logloss: 0.0119301\n",
      "[150]\tvalid's binary_logloss: 0.0119301\n",
      "[160]\tvalid's binary_logloss: 0.0119301\n",
      "[170]\tvalid's binary_logloss: 0.0119301\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid's binary_logloss: 0.0119301\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  41.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.104553\n",
      "[20]\tvalid's binary_logloss: 0.0641223\n",
      "[30]\tvalid's binary_logloss: 0.0445775\n",
      "[40]\tvalid's binary_logloss: 0.0340985\n",
      "[50]\tvalid's binary_logloss: 0.0276539\n",
      "[60]\tvalid's binary_logloss: 0.0234721\n",
      "[70]\tvalid's binary_logloss: 0.0202487\n",
      "[80]\tvalid's binary_logloss: 0.0179621\n",
      "[90]\tvalid's binary_logloss: 0.0161029\n",
      "[100]\tvalid's binary_logloss: 0.0149027\n",
      "[110]\tvalid's binary_logloss: 0.0140906\n",
      "[120]\tvalid's binary_logloss: 0.0133135\n",
      "[130]\tvalid's binary_logloss: 0.012903\n",
      "[140]\tvalid's binary_logloss: 0.012645\n",
      "[150]\tvalid's binary_logloss: 0.012645\n",
      "[160]\tvalid's binary_logloss: 0.012645\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid's binary_logloss: 0.01263\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  41.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.107525\n",
      "[20]\tvalid's binary_logloss: 0.0654114\n",
      "[30]\tvalid's binary_logloss: 0.0449262\n",
      "[40]\tvalid's binary_logloss: 0.034442\n",
      "[50]\tvalid's binary_logloss: 0.0280405\n",
      "[60]\tvalid's binary_logloss: 0.0239741\n",
      "[70]\tvalid's binary_logloss: 0.0208156\n",
      "[80]\tvalid's binary_logloss: 0.0182959\n",
      "[90]\tvalid's binary_logloss: 0.0163328\n",
      "[100]\tvalid's binary_logloss: 0.0151082\n",
      "[110]\tvalid's binary_logloss: 0.0142365\n",
      "[120]\tvalid's binary_logloss: 0.0136786\n",
      "[130]\tvalid's binary_logloss: 0.0132103\n",
      "[140]\tvalid's binary_logloss: 0.0129845\n",
      "[150]\tvalid's binary_logloss: 0.0129851\n",
      "[160]\tvalid's binary_logloss: 0.0129853\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid's binary_logloss: 0.0129844\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  41.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.106788\n",
      "[20]\tvalid's binary_logloss: 0.0668509\n",
      "[30]\tvalid's binary_logloss: 0.047338\n",
      "[40]\tvalid's binary_logloss: 0.0361531\n",
      "[50]\tvalid's binary_logloss: 0.0287867\n",
      "[60]\tvalid's binary_logloss: 0.0245361\n",
      "[70]\tvalid's binary_logloss: 0.0217258\n",
      "[80]\tvalid's binary_logloss: 0.0197345\n",
      "[90]\tvalid's binary_logloss: 0.0181669\n",
      "[100]\tvalid's binary_logloss: 0.0170939\n",
      "[110]\tvalid's binary_logloss: 0.0165261\n",
      "[120]\tvalid's binary_logloss: 0.0159821\n",
      "[130]\tvalid's binary_logloss: 0.0156589\n",
      "[140]\tvalid's binary_logloss: 0.0155159\n",
      "[150]\tvalid's binary_logloss: 0.0155159\n",
      "[160]\tvalid's binary_logloss: 0.0155159\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid's binary_logloss: 0.0155159\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  41.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.106141\n",
      "[20]\tvalid's binary_logloss: 0.0659531\n",
      "[30]\tvalid's binary_logloss: 0.0464754\n",
      "[40]\tvalid's binary_logloss: 0.0355273\n",
      "[50]\tvalid's binary_logloss: 0.0287834\n",
      "[60]\tvalid's binary_logloss: 0.0238626\n",
      "[70]\tvalid's binary_logloss: 0.0203086\n",
      "[80]\tvalid's binary_logloss: 0.0175621\n",
      "[90]\tvalid's binary_logloss: 0.0157997\n",
      "[100]\tvalid's binary_logloss: 0.0145572\n",
      "[110]\tvalid's binary_logloss: 0.0135781\n",
      "[120]\tvalid's binary_logloss: 0.0128714\n",
      "[130]\tvalid's binary_logloss: 0.0122873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140]\tvalid's binary_logloss: 0.01204\n",
      "[150]\tvalid's binary_logloss: 0.01204\n",
      "[160]\tvalid's binary_logloss: 0.01204\n",
      "[170]\tvalid's binary_logloss: 0.01204\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid's binary_logloss: 0.01204\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=3, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9137931034482758, total=  42.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0932861\n",
      "[20]\tvalid's binary_logloss: 0.0494427\n",
      "[30]\tvalid's binary_logloss: 0.030484\n",
      "[40]\tvalid's binary_logloss: 0.0215331\n",
      "[50]\tvalid's binary_logloss: 0.0168609\n",
      "[60]\tvalid's binary_logloss: 0.0144694\n",
      "[70]\tvalid's binary_logloss: 0.0130997\n",
      "[80]\tvalid's binary_logloss: 0.0121557\n",
      "[90]\tvalid's binary_logloss: 0.0117383\n",
      "[100]\tvalid's binary_logloss: 0.0114453\n",
      "[110]\tvalid's binary_logloss: 0.0111692\n",
      "[120]\tvalid's binary_logloss: 0.0111912\n",
      "[130]\tvalid's binary_logloss: 0.0110719\n",
      "[140]\tvalid's binary_logloss: 0.0111251\n",
      "[150]\tvalid's binary_logloss: 0.0110881\n",
      "[160]\tvalid's binary_logloss: 0.0111294\n",
      "[170]\tvalid's binary_logloss: 0.0111105\n",
      "[180]\tvalid's binary_logloss: 0.0110834\n",
      "Early stopping, best iteration is:\n",
      "[153]\tvalid's binary_logloss: 0.0110287\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  48.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0946626\n",
      "[20]\tvalid's binary_logloss: 0.0505159\n",
      "[30]\tvalid's binary_logloss: 0.0320799\n",
      "[40]\tvalid's binary_logloss: 0.0229103\n",
      "[50]\tvalid's binary_logloss: 0.0180791\n",
      "[60]\tvalid's binary_logloss: 0.015314\n",
      "[70]\tvalid's binary_logloss: 0.0136881\n",
      "[80]\tvalid's binary_logloss: 0.0125998\n",
      "[90]\tvalid's binary_logloss: 0.0119304\n",
      "[100]\tvalid's binary_logloss: 0.011545\n",
      "[110]\tvalid's binary_logloss: 0.011284\n",
      "[120]\tvalid's binary_logloss: 0.0110281\n",
      "[130]\tvalid's binary_logloss: 0.0108555\n",
      "[140]\tvalid's binary_logloss: 0.010759\n",
      "[150]\tvalid's binary_logloss: 0.01058\n",
      "[160]\tvalid's binary_logloss: 0.0105441\n",
      "[170]\tvalid's binary_logloss: 0.0104514\n",
      "[180]\tvalid's binary_logloss: 0.0104233\n",
      "[190]\tvalid's binary_logloss: 0.0103581\n",
      "[200]\tvalid's binary_logloss: 0.0103231\n",
      "[210]\tvalid's binary_logloss: 0.0103633\n",
      "[220]\tvalid's binary_logloss: 0.0103388\n",
      "Early stopping, best iteration is:\n",
      "[194]\tvalid's binary_logloss: 0.010314\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  50.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0963031\n",
      "[20]\tvalid's binary_logloss: 0.0524098\n",
      "[30]\tvalid's binary_logloss: 0.0336716\n",
      "[40]\tvalid's binary_logloss: 0.0248655\n",
      "[50]\tvalid's binary_logloss: 0.0201402\n",
      "[60]\tvalid's binary_logloss: 0.0175498\n",
      "[70]\tvalid's binary_logloss: 0.0160841\n",
      "[80]\tvalid's binary_logloss: 0.0152615\n",
      "[90]\tvalid's binary_logloss: 0.0149374\n",
      "[100]\tvalid's binary_logloss: 0.0148011\n",
      "[110]\tvalid's binary_logloss: 0.01468\n",
      "[120]\tvalid's binary_logloss: 0.014623\n",
      "[130]\tvalid's binary_logloss: 0.0146829\n",
      "[140]\tvalid's binary_logloss: 0.014762\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid's binary_logloss: 0.0146224\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  45.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0928242\n",
      "[20]\tvalid's binary_logloss: 0.0505185\n",
      "[30]\tvalid's binary_logloss: 0.0321786\n",
      "[40]\tvalid's binary_logloss: 0.0231276\n",
      "[50]\tvalid's binary_logloss: 0.0179645\n",
      "[60]\tvalid's binary_logloss: 0.0150084\n",
      "[70]\tvalid's binary_logloss: 0.0133792\n",
      "[80]\tvalid's binary_logloss: 0.0122213\n",
      "[90]\tvalid's binary_logloss: 0.0116883\n",
      "[100]\tvalid's binary_logloss: 0.0112682\n",
      "[110]\tvalid's binary_logloss: 0.0108952\n",
      "[120]\tvalid's binary_logloss: 0.0107977\n",
      "[130]\tvalid's binary_logloss: 0.010545\n",
      "[140]\tvalid's binary_logloss: 0.0104192\n",
      "[150]\tvalid's binary_logloss: 0.0103666\n",
      "[160]\tvalid's binary_logloss: 0.0103276\n",
      "[170]\tvalid's binary_logloss: 0.0102812\n",
      "[180]\tvalid's binary_logloss: 0.010208\n",
      "[190]\tvalid's binary_logloss: 0.0101912\n",
      "[200]\tvalid's binary_logloss: 0.0101918\n",
      "[210]\tvalid's binary_logloss: 0.0101885\n",
      "[220]\tvalid's binary_logloss: 0.010175\n",
      "[230]\tvalid's binary_logloss: 0.0101671\n",
      "[240]\tvalid's binary_logloss: 0.0101581\n",
      "[250]\tvalid's binary_logloss: 0.0101782\n",
      "[260]\tvalid's binary_logloss: 0.0101941\n",
      "Early stopping, best iteration is:\n",
      "[238]\tvalid's binary_logloss: 0.0101456\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  53.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0892818\n",
      "[20]\tvalid's binary_logloss: 0.046298\n",
      "[30]\tvalid's binary_logloss: 0.0280209\n",
      "[40]\tvalid's binary_logloss: 0.0193231\n",
      "[50]\tvalid's binary_logloss: 0.0150211\n",
      "[60]\tvalid's binary_logloss: 0.0129917\n",
      "[70]\tvalid's binary_logloss: 0.0119025\n",
      "[80]\tvalid's binary_logloss: 0.0114321\n",
      "[90]\tvalid's binary_logloss: 0.0113186\n",
      "[100]\tvalid's binary_logloss: 0.0114139\n",
      "[110]\tvalid's binary_logloss: 0.0115949\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's binary_logloss: 0.0112269\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  44.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0882476\n",
      "[20]\tvalid's binary_logloss: 0.0457284\n",
      "[30]\tvalid's binary_logloss: 0.0282226\n",
      "[40]\tvalid's binary_logloss: 0.0201653\n",
      "[50]\tvalid's binary_logloss: 0.0158068\n",
      "[60]\tvalid's binary_logloss: 0.0134284\n",
      "[70]\tvalid's binary_logloss: 0.0123694\n",
      "[80]\tvalid's binary_logloss: 0.0117893\n",
      "[90]\tvalid's binary_logloss: 0.0114529\n",
      "[100]\tvalid's binary_logloss: 0.0112781\n",
      "[110]\tvalid's binary_logloss: 0.0113205\n",
      "[120]\tvalid's binary_logloss: 0.0112988\n",
      "[130]\tvalid's binary_logloss: 0.0112718\n",
      "Early stopping, best iteration is:\n",
      "[102]\tvalid's binary_logloss: 0.0112063\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  45.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0917121\n",
      "[20]\tvalid's binary_logloss: 0.0489891\n",
      "[30]\tvalid's binary_logloss: 0.0305924\n",
      "[40]\tvalid's binary_logloss: 0.0225551\n",
      "[50]\tvalid's binary_logloss: 0.0179916\n",
      "[60]\tvalid's binary_logloss: 0.015685\n",
      "[70]\tvalid's binary_logloss: 0.0147658\n",
      "[80]\tvalid's binary_logloss: 0.014674\n",
      "[90]\tvalid's binary_logloss: 0.0147104\n",
      "[100]\tvalid's binary_logloss: 0.0147717\n",
      "[110]\tvalid's binary_logloss: 0.0151304\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid's binary_logloss: 0.0146255\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  44.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0889095\n",
      "[20]\tvalid's binary_logloss: 0.0470462\n",
      "[30]\tvalid's binary_logloss: 0.0287452\n",
      "[40]\tvalid's binary_logloss: 0.0203001\n",
      "[50]\tvalid's binary_logloss: 0.0158666\n",
      "[60]\tvalid's binary_logloss: 0.0130501\n",
      "[70]\tvalid's binary_logloss: 0.0118776\n",
      "[80]\tvalid's binary_logloss: 0.0111444\n",
      "[90]\tvalid's binary_logloss: 0.010991\n",
      "[100]\tvalid's binary_logloss: 0.0108112\n",
      "[110]\tvalid's binary_logloss: 0.0108054\n",
      "[120]\tvalid's binary_logloss: 0.0107751\n",
      "[130]\tvalid's binary_logloss: 0.0107998\n",
      "[140]\tvalid's binary_logloss: 0.010829\n",
      "[150]\tvalid's binary_logloss: 0.0108735\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid's binary_logloss: 0.0107254\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.923076923076923, total=  44.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0882275\n",
      "[20]\tvalid's binary_logloss: 0.0454328\n",
      "[30]\tvalid's binary_logloss: 0.0268276\n",
      "[40]\tvalid's binary_logloss: 0.0186009\n",
      "[50]\tvalid's binary_logloss: 0.014414\n",
      "[60]\tvalid's binary_logloss: 0.0124107\n",
      "[70]\tvalid's binary_logloss: 0.0116009\n",
      "[80]\tvalid's binary_logloss: 0.0114113\n",
      "[90]\tvalid's binary_logloss: 0.0117779\n",
      "[100]\tvalid's binary_logloss: 0.0120357\n",
      "[110]\tvalid's binary_logloss: 0.0126673\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's binary_logloss: 0.0114113\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  44.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0880298\n",
      "[20]\tvalid's binary_logloss: 0.0463705\n",
      "[30]\tvalid's binary_logloss: 0.0286364\n",
      "[40]\tvalid's binary_logloss: 0.0197203\n",
      "[50]\tvalid's binary_logloss: 0.0155504\n",
      "[60]\tvalid's binary_logloss: 0.0132114\n",
      "[70]\tvalid's binary_logloss: 0.0121348\n",
      "[80]\tvalid's binary_logloss: 0.0113077\n",
      "[90]\tvalid's binary_logloss: 0.0108484\n",
      "[100]\tvalid's binary_logloss: 0.0109067\n",
      "[110]\tvalid's binary_logloss: 0.011098\n",
      "[120]\tvalid's binary_logloss: 0.0115504\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid's binary_logloss: 0.0107769\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  45.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.092875\n",
      "[20]\tvalid's binary_logloss: 0.04969\n",
      "[30]\tvalid's binary_logloss: 0.0308121\n",
      "[40]\tvalid's binary_logloss: 0.0218861\n",
      "[50]\tvalid's binary_logloss: 0.0176439\n",
      "[60]\tvalid's binary_logloss: 0.0160177\n",
      "[70]\tvalid's binary_logloss: 0.0149733\n",
      "[80]\tvalid's binary_logloss: 0.014672\n",
      "[90]\tvalid's binary_logloss: 0.0146037\n",
      "[100]\tvalid's binary_logloss: 0.0149315\n",
      "[110]\tvalid's binary_logloss: 0.0155006\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid's binary_logloss: 0.014467\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.8750000000000001, total=  44.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0880636\n",
      "[20]\tvalid's binary_logloss: 0.0471637\n",
      "[30]\tvalid's binary_logloss: 0.0298009\n",
      "[40]\tvalid's binary_logloss: 0.0214684\n",
      "[50]\tvalid's binary_logloss: 0.0164837\n",
      "[60]\tvalid's binary_logloss: 0.0138193\n",
      "[70]\tvalid's binary_logloss: 0.0122176\n",
      "[80]\tvalid's binary_logloss: 0.0113611\n",
      "[90]\tvalid's binary_logloss: 0.0110642\n",
      "[100]\tvalid's binary_logloss: 0.0110197\n",
      "[110]\tvalid's binary_logloss: 0.0110424\n",
      "[120]\tvalid's binary_logloss: 0.0112557\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's binary_logloss: 0.01095\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  44.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0937044\n",
      "[20]\tvalid's binary_logloss: 0.0497217\n",
      "[30]\tvalid's binary_logloss: 0.030912\n",
      "[40]\tvalid's binary_logloss: 0.0217292\n",
      "[50]\tvalid's binary_logloss: 0.0169949\n",
      "[60]\tvalid's binary_logloss: 0.0145588\n",
      "[70]\tvalid's binary_logloss: 0.0128792\n",
      "[80]\tvalid's binary_logloss: 0.0121728\n",
      "[90]\tvalid's binary_logloss: 0.0116287\n",
      "[100]\tvalid's binary_logloss: 0.0111217\n",
      "[110]\tvalid's binary_logloss: 0.0108263\n",
      "[120]\tvalid's binary_logloss: 0.0105962\n",
      "[130]\tvalid's binary_logloss: 0.0104914\n",
      "[140]\tvalid's binary_logloss: 0.0103481\n",
      "[150]\tvalid's binary_logloss: 0.0103342\n",
      "[160]\tvalid's binary_logloss: 0.0102945\n",
      "[170]\tvalid's binary_logloss: 0.0103768\n",
      "[180]\tvalid's binary_logloss: 0.0103042\n",
      "[190]\tvalid's binary_logloss: 0.0103281\n",
      "Early stopping, best iteration is:\n",
      "[161]\tvalid's binary_logloss: 0.010259\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  44.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0949254\n",
      "[20]\tvalid's binary_logloss: 0.0507696\n",
      "[30]\tvalid's binary_logloss: 0.0321696\n",
      "[40]\tvalid's binary_logloss: 0.0229974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid's binary_logloss: 0.0179217\n",
      "[60]\tvalid's binary_logloss: 0.0151909\n",
      "[70]\tvalid's binary_logloss: 0.0136189\n",
      "[80]\tvalid's binary_logloss: 0.0126355\n",
      "[90]\tvalid's binary_logloss: 0.0119766\n",
      "[100]\tvalid's binary_logloss: 0.011525\n",
      "[110]\tvalid's binary_logloss: 0.0113012\n",
      "[120]\tvalid's binary_logloss: 0.0109943\n",
      "[130]\tvalid's binary_logloss: 0.010832\n",
      "[140]\tvalid's binary_logloss: 0.0106638\n",
      "[150]\tvalid's binary_logloss: 0.0106097\n",
      "[160]\tvalid's binary_logloss: 0.010593\n",
      "[170]\tvalid's binary_logloss: 0.0105199\n",
      "[180]\tvalid's binary_logloss: 0.0105059\n",
      "[190]\tvalid's binary_logloss: 0.0104609\n",
      "[200]\tvalid's binary_logloss: 0.0103981\n",
      "[210]\tvalid's binary_logloss: 0.0103406\n",
      "[220]\tvalid's binary_logloss: 0.0102811\n",
      "[230]\tvalid's binary_logloss: 0.0102539\n",
      "[240]\tvalid's binary_logloss: 0.0102355\n",
      "[250]\tvalid's binary_logloss: 0.0102003\n",
      "[260]\tvalid's binary_logloss: 0.0101717\n",
      "[270]\tvalid's binary_logloss: 0.0101537\n",
      "[280]\tvalid's binary_logloss: 0.0101574\n",
      "[290]\tvalid's binary_logloss: 0.0101574\n",
      "[300]\tvalid's binary_logloss: 0.0101574\n",
      "Early stopping, best iteration is:\n",
      "[270]\tvalid's binary_logloss: 0.0101537\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9411764705882353, total=  46.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0966835\n",
      "[20]\tvalid's binary_logloss: 0.0528896\n",
      "[30]\tvalid's binary_logloss: 0.0337677\n",
      "[40]\tvalid's binary_logloss: 0.0253112\n",
      "[50]\tvalid's binary_logloss: 0.0204368\n",
      "[60]\tvalid's binary_logloss: 0.0178434\n",
      "[70]\tvalid's binary_logloss: 0.0162092\n",
      "[80]\tvalid's binary_logloss: 0.0153297\n",
      "[90]\tvalid's binary_logloss: 0.0148954\n",
      "[100]\tvalid's binary_logloss: 0.0147025\n",
      "[110]\tvalid's binary_logloss: 0.0145803\n",
      "[120]\tvalid's binary_logloss: 0.0145211\n",
      "[130]\tvalid's binary_logloss: 0.0143316\n",
      "[140]\tvalid's binary_logloss: 0.0143501\n",
      "[150]\tvalid's binary_logloss: 0.0144278\n",
      "[160]\tvalid's binary_logloss: 0.014363\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid's binary_logloss: 0.0142884\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  42.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0931404\n",
      "[20]\tvalid's binary_logloss: 0.0505119\n",
      "[30]\tvalid's binary_logloss: 0.0318603\n",
      "[40]\tvalid's binary_logloss: 0.0225813\n",
      "[50]\tvalid's binary_logloss: 0.0179433\n",
      "[60]\tvalid's binary_logloss: 0.0150724\n",
      "[70]\tvalid's binary_logloss: 0.0133308\n",
      "[80]\tvalid's binary_logloss: 0.0123336\n",
      "[90]\tvalid's binary_logloss: 0.0117268\n",
      "[100]\tvalid's binary_logloss: 0.0112184\n",
      "[110]\tvalid's binary_logloss: 0.0109891\n",
      "[120]\tvalid's binary_logloss: 0.0108244\n",
      "[130]\tvalid's binary_logloss: 0.0106484\n",
      "[140]\tvalid's binary_logloss: 0.010516\n",
      "[150]\tvalid's binary_logloss: 0.0104619\n",
      "[160]\tvalid's binary_logloss: 0.010387\n",
      "[170]\tvalid's binary_logloss: 0.0104418\n",
      "[180]\tvalid's binary_logloss: 0.0103515\n",
      "[190]\tvalid's binary_logloss: 0.0102759\n",
      "[200]\tvalid's binary_logloss: 0.0102188\n",
      "[210]\tvalid's binary_logloss: 0.0101652\n",
      "[220]\tvalid's binary_logloss: 0.0101382\n",
      "[230]\tvalid's binary_logloss: 0.0101466\n",
      "[240]\tvalid's binary_logloss: 0.0100996\n",
      "[250]\tvalid's binary_logloss: 0.010081\n",
      "[260]\tvalid's binary_logloss: 0.010083\n",
      "[270]\tvalid's binary_logloss: 0.0100519\n",
      "[280]\tvalid's binary_logloss: 0.010055\n",
      "[290]\tvalid's binary_logloss: 0.0100552\n",
      "Early stopping, best iteration is:\n",
      "[267]\tvalid's binary_logloss: 0.0100369\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  47.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0894271\n",
      "[20]\tvalid's binary_logloss: 0.046392\n",
      "[30]\tvalid's binary_logloss: 0.0279622\n",
      "[40]\tvalid's binary_logloss: 0.0188404\n",
      "[50]\tvalid's binary_logloss: 0.0149669\n",
      "[60]\tvalid's binary_logloss: 0.0126106\n",
      "[70]\tvalid's binary_logloss: 0.0116314\n",
      "[80]\tvalid's binary_logloss: 0.0110977\n",
      "[90]\tvalid's binary_logloss: 0.0108233\n",
      "[100]\tvalid's binary_logloss: 0.0107336\n",
      "[110]\tvalid's binary_logloss: 0.0107446\n",
      "[120]\tvalid's binary_logloss: 0.0106143\n",
      "[130]\tvalid's binary_logloss: 0.010645\n",
      "[140]\tvalid's binary_logloss: 0.0106804\n",
      "[150]\tvalid's binary_logloss: 0.01067\n",
      "Early stopping, best iteration is:\n",
      "[128]\tvalid's binary_logloss: 0.0105462\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  42.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0892669\n",
      "[20]\tvalid's binary_logloss: 0.0467747\n",
      "[30]\tvalid's binary_logloss: 0.0289609\n",
      "[40]\tvalid's binary_logloss: 0.020937\n",
      "[50]\tvalid's binary_logloss: 0.0164452\n",
      "[60]\tvalid's binary_logloss: 0.0141998\n",
      "[70]\tvalid's binary_logloss: 0.0126585\n",
      "[80]\tvalid's binary_logloss: 0.0120905\n",
      "[90]\tvalid's binary_logloss: 0.0115252\n",
      "[100]\tvalid's binary_logloss: 0.0112103\n",
      "[110]\tvalid's binary_logloss: 0.0109107\n",
      "[120]\tvalid's binary_logloss: 0.0108825\n",
      "[130]\tvalid's binary_logloss: 0.0108354\n",
      "[140]\tvalid's binary_logloss: 0.010798\n",
      "[150]\tvalid's binary_logloss: 0.0107995\n",
      "[160]\tvalid's binary_logloss: 0.0107995\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid's binary_logloss: 0.0107729\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9411764705882353, total=  42.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0920661\n",
      "[20]\tvalid's binary_logloss: 0.0492056\n",
      "[30]\tvalid's binary_logloss: 0.0303852\n",
      "[40]\tvalid's binary_logloss: 0.0221752\n",
      "[50]\tvalid's binary_logloss: 0.0177464\n",
      "[60]\tvalid's binary_logloss: 0.015693\n",
      "[70]\tvalid's binary_logloss: 0.0148287\n",
      "[80]\tvalid's binary_logloss: 0.0141402\n",
      "[90]\tvalid's binary_logloss: 0.0141063\n",
      "[100]\tvalid's binary_logloss: 0.0141627\n",
      "[110]\tvalid's binary_logloss: 0.0142716\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid's binary_logloss: 0.0140366\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9043478260869565, total=  40.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0895059\n",
      "[20]\tvalid's binary_logloss: 0.0474749\n",
      "[30]\tvalid's binary_logloss: 0.0296643\n",
      "[40]\tvalid's binary_logloss: 0.0204995\n",
      "[50]\tvalid's binary_logloss: 0.0159638\n",
      "[60]\tvalid's binary_logloss: 0.0134468\n",
      "[70]\tvalid's binary_logloss: 0.0121038\n",
      "[80]\tvalid's binary_logloss: 0.0112485\n",
      "[90]\tvalid's binary_logloss: 0.0107695\n",
      "[100]\tvalid's binary_logloss: 0.0106069\n",
      "[110]\tvalid's binary_logloss: 0.0104948\n",
      "[120]\tvalid's binary_logloss: 0.0106048\n",
      "[130]\tvalid's binary_logloss: 0.010454\n",
      "[140]\tvalid's binary_logloss: 0.0104387\n",
      "[150]\tvalid's binary_logloss: 0.0104325\n",
      "[160]\tvalid's binary_logloss: 0.0104325\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid's binary_logloss: 0.0104233\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  43.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0885512\n",
      "[20]\tvalid's binary_logloss: 0.0454268\n",
      "[30]\tvalid's binary_logloss: 0.0268005\n",
      "[40]\tvalid's binary_logloss: 0.0185255\n",
      "[50]\tvalid's binary_logloss: 0.0144257\n",
      "[60]\tvalid's binary_logloss: 0.012088\n",
      "[70]\tvalid's binary_logloss: 0.0113445\n",
      "[80]\tvalid's binary_logloss: 0.0107797\n",
      "[90]\tvalid's binary_logloss: 0.0106465\n",
      "[100]\tvalid's binary_logloss: 0.0106836\n",
      "[110]\tvalid's binary_logloss: 0.0105844\n",
      "[120]\tvalid's binary_logloss: 0.0106635\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid's binary_logloss: 0.0105745\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  41.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0887213\n",
      "[20]\tvalid's binary_logloss: 0.046018\n",
      "[30]\tvalid's binary_logloss: 0.0281731\n",
      "[40]\tvalid's binary_logloss: 0.0200342\n",
      "[50]\tvalid's binary_logloss: 0.0160884\n",
      "[60]\tvalid's binary_logloss: 0.0139571\n",
      "[70]\tvalid's binary_logloss: 0.0123012\n",
      "[80]\tvalid's binary_logloss: 0.0119813\n",
      "[90]\tvalid's binary_logloss: 0.0116471\n",
      "[100]\tvalid's binary_logloss: 0.0114315\n",
      "[110]\tvalid's binary_logloss: 0.0112918\n",
      "[120]\tvalid's binary_logloss: 0.0111936\n",
      "[130]\tvalid's binary_logloss: 0.0111672\n",
      "[140]\tvalid's binary_logloss: 0.0111635\n",
      "[150]\tvalid's binary_logloss: 0.0111637\n",
      "Early stopping, best iteration is:\n",
      "[126]\tvalid's binary_logloss: 0.0111584\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  41.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0917201\n",
      "[20]\tvalid's binary_logloss: 0.049831\n",
      "[30]\tvalid's binary_logloss: 0.0309396\n",
      "[40]\tvalid's binary_logloss: 0.0227365\n",
      "[50]\tvalid's binary_logloss: 0.0184573\n",
      "[60]\tvalid's binary_logloss: 0.0163485\n",
      "[70]\tvalid's binary_logloss: 0.0154468\n",
      "[80]\tvalid's binary_logloss: 0.0150323\n",
      "[90]\tvalid's binary_logloss: 0.0148628\n",
      "[100]\tvalid's binary_logloss: 0.0147125\n",
      "[110]\tvalid's binary_logloss: 0.0149581\n",
      "[120]\tvalid's binary_logloss: 0.0150126\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid's binary_logloss: 0.0146756\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.8849557522123894, total=  41.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0885198\n",
      "[20]\tvalid's binary_logloss: 0.0468039\n",
      "[30]\tvalid's binary_logloss: 0.0289959\n",
      "[40]\tvalid's binary_logloss: 0.0199027\n",
      "[50]\tvalid's binary_logloss: 0.0150565\n",
      "[60]\tvalid's binary_logloss: 0.0125777\n",
      "[70]\tvalid's binary_logloss: 0.0114792\n",
      "[80]\tvalid's binary_logloss: 0.0106992\n",
      "[90]\tvalid's binary_logloss: 0.0105407\n",
      "[100]\tvalid's binary_logloss: 0.0102309\n",
      "[110]\tvalid's binary_logloss: 0.0100766\n",
      "[120]\tvalid's binary_logloss: 0.00994738\n",
      "[130]\tvalid's binary_logloss: 0.00992805\n",
      "[140]\tvalid's binary_logloss: 0.00992805\n",
      "[150]\tvalid's binary_logloss: 0.00992805\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid's binary_logloss: 0.00987015\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9137931034482758, total=  41.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0972043\n",
      "[20]\tvalid's binary_logloss: 0.0532205\n",
      "[30]\tvalid's binary_logloss: 0.0342323\n",
      "[40]\tvalid's binary_logloss: 0.0253504\n",
      "[50]\tvalid's binary_logloss: 0.0203567\n",
      "[60]\tvalid's binary_logloss: 0.0174739\n",
      "[70]\tvalid's binary_logloss: 0.0153818\n",
      "[80]\tvalid's binary_logloss: 0.0142184\n",
      "[90]\tvalid's binary_logloss: 0.0134061\n",
      "[100]\tvalid's binary_logloss: 0.0129318\n",
      "[110]\tvalid's binary_logloss: 0.0128083\n",
      "[120]\tvalid's binary_logloss: 0.0128083\n",
      "[130]\tvalid's binary_logloss: 0.0128083\n",
      "[140]\tvalid's binary_logloss: 0.0128083\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's binary_logloss: 0.0128083\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.923076923076923, total=  40.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0963368\n",
      "[20]\tvalid's binary_logloss: 0.0535315\n",
      "[30]\tvalid's binary_logloss: 0.0358251\n",
      "[40]\tvalid's binary_logloss: 0.0267178\n",
      "[50]\tvalid's binary_logloss: 0.0217226\n",
      "[60]\tvalid's binary_logloss: 0.0186848\n",
      "[70]\tvalid's binary_logloss: 0.0166997\n",
      "[80]\tvalid's binary_logloss: 0.0154774\n",
      "[90]\tvalid's binary_logloss: 0.0145658\n",
      "[100]\tvalid's binary_logloss: 0.0141197\n",
      "[110]\tvalid's binary_logloss: 0.0139665\n",
      "[120]\tvalid's binary_logloss: 0.0139665\n",
      "[130]\tvalid's binary_logloss: 0.0139665\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid's binary_logloss: 0.0139649\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.8869565217391304, total=  39.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0994751\n",
      "[20]\tvalid's binary_logloss: 0.0560104\n",
      "[30]\tvalid's binary_logloss: 0.0373047\n",
      "[40]\tvalid's binary_logloss: 0.0276327\n",
      "[50]\tvalid's binary_logloss: 0.0225099\n",
      "[60]\tvalid's binary_logloss: 0.0198617\n",
      "[70]\tvalid's binary_logloss: 0.0178898\n",
      "[80]\tvalid's binary_logloss: 0.016894\n",
      "[90]\tvalid's binary_logloss: 0.0161831\n",
      "[100]\tvalid's binary_logloss: 0.0159072\n",
      "[110]\tvalid's binary_logloss: 0.0157341\n",
      "[120]\tvalid's binary_logloss: 0.0157448\n",
      "[130]\tvalid's binary_logloss: 0.0157448\n",
      "[140]\tvalid's binary_logloss: 0.0157448\n",
      "Early stopping, best iteration is:\n",
      "[114]\tvalid's binary_logloss: 0.0157327\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.8849557522123894, total=  39.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0940511\n",
      "[20]\tvalid's binary_logloss: 0.0533288\n",
      "[30]\tvalid's binary_logloss: 0.0353715\n",
      "[40]\tvalid's binary_logloss: 0.025506\n",
      "[50]\tvalid's binary_logloss: 0.0202613\n",
      "[60]\tvalid's binary_logloss: 0.0172773\n",
      "[70]\tvalid's binary_logloss: 0.0155311\n",
      "[80]\tvalid's binary_logloss: 0.0143423\n",
      "[90]\tvalid's binary_logloss: 0.0135816\n",
      "[100]\tvalid's binary_logloss: 0.0129965\n",
      "[110]\tvalid's binary_logloss: 0.0127825\n",
      "[120]\tvalid's binary_logloss: 0.0127649\n",
      "[130]\tvalid's binary_logloss: 0.0127653\n",
      "[140]\tvalid's binary_logloss: 0.0127655\n",
      "[150]\tvalid's binary_logloss: 0.0127655\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid's binary_logloss: 0.0127649\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.8965517241379309, total=  40.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0929964\n",
      "[20]\tvalid's binary_logloss: 0.050258\n",
      "[30]\tvalid's binary_logloss: 0.0320953\n",
      "[40]\tvalid's binary_logloss: 0.0234356\n",
      "[50]\tvalid's binary_logloss: 0.0187439\n",
      "[60]\tvalid's binary_logloss: 0.0158936\n",
      "[70]\tvalid's binary_logloss: 0.0145914\n",
      "[80]\tvalid's binary_logloss: 0.0134666\n",
      "[90]\tvalid's binary_logloss: 0.0131404\n",
      "[100]\tvalid's binary_logloss: 0.0131404\n",
      "[110]\tvalid's binary_logloss: 0.0131404\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's binary_logloss: 0.0131404\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  39.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0927376\n",
      "[20]\tvalid's binary_logloss: 0.0505692\n",
      "[30]\tvalid's binary_logloss: 0.0328199\n",
      "[40]\tvalid's binary_logloss: 0.0243927\n",
      "[50]\tvalid's binary_logloss: 0.0199319\n",
      "[60]\tvalid's binary_logloss: 0.0172986\n",
      "[70]\tvalid's binary_logloss: 0.0156947\n",
      "[80]\tvalid's binary_logloss: 0.0147481\n",
      "[90]\tvalid's binary_logloss: 0.014129\n",
      "[100]\tvalid's binary_logloss: 0.0140889\n",
      "[110]\tvalid's binary_logloss: 0.014089\n",
      "[120]\tvalid's binary_logloss: 0.0140891\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid's binary_logloss: 0.0140888\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.923076923076923, total=  39.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0945958\n",
      "[20]\tvalid's binary_logloss: 0.0524931\n",
      "[30]\tvalid's binary_logloss: 0.0347845\n",
      "[40]\tvalid's binary_logloss: 0.0259523\n",
      "[50]\tvalid's binary_logloss: 0.021439\n",
      "[60]\tvalid's binary_logloss: 0.0188937\n",
      "[70]\tvalid's binary_logloss: 0.0174084\n",
      "[80]\tvalid's binary_logloss: 0.0166219\n",
      "[90]\tvalid's binary_logloss: 0.01631\n",
      "[100]\tvalid's binary_logloss: 0.01631\n",
      "[110]\tvalid's binary_logloss: 0.01631\n",
      "[120]\tvalid's binary_logloss: 0.01631\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's binary_logloss: 0.01631\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.8947368421052632, total=  39.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0927203\n",
      "[20]\tvalid's binary_logloss: 0.0515046\n",
      "[30]\tvalid's binary_logloss: 0.0332084\n",
      "[40]\tvalid's binary_logloss: 0.0235106\n",
      "[50]\tvalid's binary_logloss: 0.0185106\n",
      "[60]\tvalid's binary_logloss: 0.0156931\n",
      "[70]\tvalid's binary_logloss: 0.0140772\n",
      "[80]\tvalid's binary_logloss: 0.0130383\n",
      "[90]\tvalid's binary_logloss: 0.0125328\n",
      "[100]\tvalid's binary_logloss: 0.0125415\n",
      "[110]\tvalid's binary_logloss: 0.0125415\n",
      "[120]\tvalid's binary_logloss: 0.0125415\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's binary_logloss: 0.0125328\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.8869565217391304, total=  39.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0907469\n",
      "[20]\tvalid's binary_logloss: 0.0497535\n",
      "[30]\tvalid's binary_logloss: 0.0309537\n",
      "[40]\tvalid's binary_logloss: 0.0228444\n",
      "[50]\tvalid's binary_logloss: 0.0184286\n",
      "[60]\tvalid's binary_logloss: 0.0156789\n",
      "[70]\tvalid's binary_logloss: 0.0140555\n",
      "[80]\tvalid's binary_logloss: 0.0133474\n",
      "[90]\tvalid's binary_logloss: 0.0130514\n",
      "[100]\tvalid's binary_logloss: 0.0130515\n",
      "[110]\tvalid's binary_logloss: 0.0130515\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid's binary_logloss: 0.0130514\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9152542372881356, total=  39.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0921967\n",
      "[20]\tvalid's binary_logloss: 0.0500064\n",
      "[30]\tvalid's binary_logloss: 0.0322213\n",
      "[40]\tvalid's binary_logloss: 0.0239782\n",
      "[50]\tvalid's binary_logloss: 0.0196709\n",
      "[60]\tvalid's binary_logloss: 0.0170768\n",
      "[70]\tvalid's binary_logloss: 0.0155735\n",
      "[80]\tvalid's binary_logloss: 0.0147334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90]\tvalid's binary_logloss: 0.0144171\n",
      "[100]\tvalid's binary_logloss: 0.0143845\n",
      "[110]\tvalid's binary_logloss: 0.0143845\n",
      "[120]\tvalid's binary_logloss: 0.0143845\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid's binary_logloss: 0.0143784\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.8869565217391304, total=  39.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0940942\n",
      "[20]\tvalid's binary_logloss: 0.0516834\n",
      "[30]\tvalid's binary_logloss: 0.0342663\n",
      "[40]\tvalid's binary_logloss: 0.0258426\n",
      "[50]\tvalid's binary_logloss: 0.0214021\n",
      "[60]\tvalid's binary_logloss: 0.0188203\n",
      "[70]\tvalid's binary_logloss: 0.0172914\n",
      "[80]\tvalid's binary_logloss: 0.0167219\n",
      "[90]\tvalid's binary_logloss: 0.0164802\n",
      "[100]\tvalid's binary_logloss: 0.0164805\n",
      "[110]\tvalid's binary_logloss: 0.0164806\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid's binary_logloss: 0.01648\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.8947368421052632, total=  39.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0923976\n",
      "[20]\tvalid's binary_logloss: 0.0505755\n",
      "[30]\tvalid's binary_logloss: 0.0324094\n",
      "[40]\tvalid's binary_logloss: 0.022707\n",
      "[50]\tvalid's binary_logloss: 0.0176931\n",
      "[60]\tvalid's binary_logloss: 0.015294\n",
      "[70]\tvalid's binary_logloss: 0.0138752\n",
      "[80]\tvalid's binary_logloss: 0.0128335\n",
      "[90]\tvalid's binary_logloss: 0.0124402\n",
      "[100]\tvalid's binary_logloss: 0.0124293\n",
      "[110]\tvalid's binary_logloss: 0.0124293\n",
      "[120]\tvalid's binary_logloss: 0.0124293\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid's binary_logloss: 0.0124293\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  38.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0932861\n",
      "[20]\tvalid's binary_logloss: 0.0494427\n",
      "[30]\tvalid's binary_logloss: 0.030484\n",
      "[40]\tvalid's binary_logloss: 0.0215331\n",
      "[50]\tvalid's binary_logloss: 0.0168609\n",
      "[60]\tvalid's binary_logloss: 0.0144694\n",
      "[70]\tvalid's binary_logloss: 0.0130997\n",
      "[80]\tvalid's binary_logloss: 0.0121557\n",
      "[90]\tvalid's binary_logloss: 0.0117383\n",
      "[100]\tvalid's binary_logloss: 0.0114453\n",
      "[110]\tvalid's binary_logloss: 0.0111692\n",
      "[120]\tvalid's binary_logloss: 0.0111912\n",
      "[130]\tvalid's binary_logloss: 0.0110719\n",
      "[140]\tvalid's binary_logloss: 0.0111251\n",
      "[150]\tvalid's binary_logloss: 0.0110881\n",
      "[160]\tvalid's binary_logloss: 0.0111294\n",
      "[170]\tvalid's binary_logloss: 0.0111105\n",
      "[180]\tvalid's binary_logloss: 0.0110834\n",
      "Early stopping, best iteration is:\n",
      "[153]\tvalid's binary_logloss: 0.0110287\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  45.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0946626\n",
      "[20]\tvalid's binary_logloss: 0.0505159\n",
      "[30]\tvalid's binary_logloss: 0.0320799\n",
      "[40]\tvalid's binary_logloss: 0.0229103\n",
      "[50]\tvalid's binary_logloss: 0.0180791\n",
      "[60]\tvalid's binary_logloss: 0.015314\n",
      "[70]\tvalid's binary_logloss: 0.0136881\n",
      "[80]\tvalid's binary_logloss: 0.0125998\n",
      "[90]\tvalid's binary_logloss: 0.0119304\n",
      "[100]\tvalid's binary_logloss: 0.011545\n",
      "[110]\tvalid's binary_logloss: 0.011284\n",
      "[120]\tvalid's binary_logloss: 0.0110281\n",
      "[130]\tvalid's binary_logloss: 0.0108555\n",
      "[140]\tvalid's binary_logloss: 0.010759\n",
      "[150]\tvalid's binary_logloss: 0.01058\n",
      "[160]\tvalid's binary_logloss: 0.0105441\n",
      "[170]\tvalid's binary_logloss: 0.0104514\n",
      "[180]\tvalid's binary_logloss: 0.0104233\n",
      "[190]\tvalid's binary_logloss: 0.0103581\n",
      "[200]\tvalid's binary_logloss: 0.0103231\n",
      "[210]\tvalid's binary_logloss: 0.0103633\n",
      "[220]\tvalid's binary_logloss: 0.0103388\n",
      "Early stopping, best iteration is:\n",
      "[194]\tvalid's binary_logloss: 0.010314\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  47.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0963031\n",
      "[20]\tvalid's binary_logloss: 0.0524098\n",
      "[30]\tvalid's binary_logloss: 0.0336716\n",
      "[40]\tvalid's binary_logloss: 0.0248655\n",
      "[50]\tvalid's binary_logloss: 0.0201402\n",
      "[60]\tvalid's binary_logloss: 0.0175498\n",
      "[70]\tvalid's binary_logloss: 0.0160841\n",
      "[80]\tvalid's binary_logloss: 0.0152615\n",
      "[90]\tvalid's binary_logloss: 0.0149374\n",
      "[100]\tvalid's binary_logloss: 0.0148011\n",
      "[110]\tvalid's binary_logloss: 0.01468\n",
      "[120]\tvalid's binary_logloss: 0.014623\n",
      "[130]\tvalid's binary_logloss: 0.0146829\n",
      "[140]\tvalid's binary_logloss: 0.014762\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid's binary_logloss: 0.0146224\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  42.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0928242\n",
      "[20]\tvalid's binary_logloss: 0.0505185\n",
      "[30]\tvalid's binary_logloss: 0.0321786\n",
      "[40]\tvalid's binary_logloss: 0.0231276\n",
      "[50]\tvalid's binary_logloss: 0.0179645\n",
      "[60]\tvalid's binary_logloss: 0.0150084\n",
      "[70]\tvalid's binary_logloss: 0.0133792\n",
      "[80]\tvalid's binary_logloss: 0.0122213\n",
      "[90]\tvalid's binary_logloss: 0.0116883\n",
      "[100]\tvalid's binary_logloss: 0.0112682\n",
      "[110]\tvalid's binary_logloss: 0.0108952\n",
      "[120]\tvalid's binary_logloss: 0.0107977\n",
      "[130]\tvalid's binary_logloss: 0.010545\n",
      "[140]\tvalid's binary_logloss: 0.0104192\n",
      "[150]\tvalid's binary_logloss: 0.0103666\n",
      "[160]\tvalid's binary_logloss: 0.0103276\n",
      "[170]\tvalid's binary_logloss: 0.0102812\n",
      "[180]\tvalid's binary_logloss: 0.010208\n",
      "[190]\tvalid's binary_logloss: 0.0101912\n",
      "[200]\tvalid's binary_logloss: 0.0101918\n",
      "[210]\tvalid's binary_logloss: 0.0101885\n",
      "[220]\tvalid's binary_logloss: 0.010175\n",
      "[230]\tvalid's binary_logloss: 0.0101671\n",
      "[240]\tvalid's binary_logloss: 0.0101581\n",
      "[250]\tvalid's binary_logloss: 0.0101782\n",
      "[260]\tvalid's binary_logloss: 0.0101941\n",
      "Early stopping, best iteration is:\n",
      "[238]\tvalid's binary_logloss: 0.0101456\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  49.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0892818\n",
      "[20]\tvalid's binary_logloss: 0.046298\n",
      "[30]\tvalid's binary_logloss: 0.0280209\n",
      "[40]\tvalid's binary_logloss: 0.0193231\n",
      "[50]\tvalid's binary_logloss: 0.0150211\n",
      "[60]\tvalid's binary_logloss: 0.0129917\n",
      "[70]\tvalid's binary_logloss: 0.0119025\n",
      "[80]\tvalid's binary_logloss: 0.0114321\n",
      "[90]\tvalid's binary_logloss: 0.0113186\n",
      "[100]\tvalid's binary_logloss: 0.0114139\n",
      "[110]\tvalid's binary_logloss: 0.0115949\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's binary_logloss: 0.0112269\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  54.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0882476\n",
      "[20]\tvalid's binary_logloss: 0.0457284\n",
      "[30]\tvalid's binary_logloss: 0.0282226\n",
      "[40]\tvalid's binary_logloss: 0.0201653\n",
      "[50]\tvalid's binary_logloss: 0.0158068\n",
      "[60]\tvalid's binary_logloss: 0.0134284\n",
      "[70]\tvalid's binary_logloss: 0.0123694\n",
      "[80]\tvalid's binary_logloss: 0.0117893\n",
      "[90]\tvalid's binary_logloss: 0.0114529\n",
      "[100]\tvalid's binary_logloss: 0.0112781\n",
      "[110]\tvalid's binary_logloss: 0.0113205\n",
      "[120]\tvalid's binary_logloss: 0.0112988\n",
      "[130]\tvalid's binary_logloss: 0.0112718\n",
      "Early stopping, best iteration is:\n",
      "[102]\tvalid's binary_logloss: 0.0112063\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  46.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0917121\n",
      "[20]\tvalid's binary_logloss: 0.0489891\n",
      "[30]\tvalid's binary_logloss: 0.0305924\n",
      "[40]\tvalid's binary_logloss: 0.0225551\n",
      "[50]\tvalid's binary_logloss: 0.0179916\n",
      "[60]\tvalid's binary_logloss: 0.015685\n",
      "[70]\tvalid's binary_logloss: 0.0147658\n",
      "[80]\tvalid's binary_logloss: 0.014674\n",
      "[90]\tvalid's binary_logloss: 0.0147104\n",
      "[100]\tvalid's binary_logloss: 0.0147717\n",
      "[110]\tvalid's binary_logloss: 0.0151304\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid's binary_logloss: 0.0146255\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  44.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0889095\n",
      "[20]\tvalid's binary_logloss: 0.0470462\n",
      "[30]\tvalid's binary_logloss: 0.0287452\n",
      "[40]\tvalid's binary_logloss: 0.0203001\n",
      "[50]\tvalid's binary_logloss: 0.0158666\n",
      "[60]\tvalid's binary_logloss: 0.0130501\n",
      "[70]\tvalid's binary_logloss: 0.0118776\n",
      "[80]\tvalid's binary_logloss: 0.0111444\n",
      "[90]\tvalid's binary_logloss: 0.010991\n",
      "[100]\tvalid's binary_logloss: 0.0108112\n",
      "[110]\tvalid's binary_logloss: 0.0108054\n",
      "[120]\tvalid's binary_logloss: 0.0107751\n",
      "[130]\tvalid's binary_logloss: 0.0107998\n",
      "[140]\tvalid's binary_logloss: 0.010829\n",
      "[150]\tvalid's binary_logloss: 0.0108735\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid's binary_logloss: 0.0107254\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.923076923076923, total=  46.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0882275\n",
      "[20]\tvalid's binary_logloss: 0.0454328\n",
      "[30]\tvalid's binary_logloss: 0.0268276\n",
      "[40]\tvalid's binary_logloss: 0.0186009\n",
      "[50]\tvalid's binary_logloss: 0.014414\n",
      "[60]\tvalid's binary_logloss: 0.0124107\n",
      "[70]\tvalid's binary_logloss: 0.0116009\n",
      "[80]\tvalid's binary_logloss: 0.0114113\n",
      "[90]\tvalid's binary_logloss: 0.0117779\n",
      "[100]\tvalid's binary_logloss: 0.0120357\n",
      "[110]\tvalid's binary_logloss: 0.0126673\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's binary_logloss: 0.0114113\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  46.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0880298\n",
      "[20]\tvalid's binary_logloss: 0.0463705\n",
      "[30]\tvalid's binary_logloss: 0.0286364\n",
      "[40]\tvalid's binary_logloss: 0.0197203\n",
      "[50]\tvalid's binary_logloss: 0.0155504\n",
      "[60]\tvalid's binary_logloss: 0.0132114\n",
      "[70]\tvalid's binary_logloss: 0.0121348\n",
      "[80]\tvalid's binary_logloss: 0.0113077\n",
      "[90]\tvalid's binary_logloss: 0.0108484\n",
      "[100]\tvalid's binary_logloss: 0.0109067\n",
      "[110]\tvalid's binary_logloss: 0.011098\n",
      "[120]\tvalid's binary_logloss: 0.0115504\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid's binary_logloss: 0.0107769\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  47.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.092875\n",
      "[20]\tvalid's binary_logloss: 0.04969\n",
      "[30]\tvalid's binary_logloss: 0.0308121\n",
      "[40]\tvalid's binary_logloss: 0.0218861\n",
      "[50]\tvalid's binary_logloss: 0.0176439\n",
      "[60]\tvalid's binary_logloss: 0.0160177\n",
      "[70]\tvalid's binary_logloss: 0.0149733\n",
      "[80]\tvalid's binary_logloss: 0.014672\n",
      "[90]\tvalid's binary_logloss: 0.0146037\n",
      "[100]\tvalid's binary_logloss: 0.0149315\n",
      "[110]\tvalid's binary_logloss: 0.0155006\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid's binary_logloss: 0.014467\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.8750000000000001, total=  46.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0880636\n",
      "[20]\tvalid's binary_logloss: 0.0471637\n",
      "[30]\tvalid's binary_logloss: 0.0298009\n",
      "[40]\tvalid's binary_logloss: 0.0214684\n",
      "[50]\tvalid's binary_logloss: 0.0164837\n",
      "[60]\tvalid's binary_logloss: 0.0138193\n",
      "[70]\tvalid's binary_logloss: 0.0122176\n",
      "[80]\tvalid's binary_logloss: 0.0113611\n",
      "[90]\tvalid's binary_logloss: 0.0110642\n",
      "[100]\tvalid's binary_logloss: 0.0110197\n",
      "[110]\tvalid's binary_logloss: 0.0110424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120]\tvalid's binary_logloss: 0.0112557\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's binary_logloss: 0.01095\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  47.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0937044\n",
      "[20]\tvalid's binary_logloss: 0.0497217\n",
      "[30]\tvalid's binary_logloss: 0.030912\n",
      "[40]\tvalid's binary_logloss: 0.0217292\n",
      "[50]\tvalid's binary_logloss: 0.0169949\n",
      "[60]\tvalid's binary_logloss: 0.0145588\n",
      "[70]\tvalid's binary_logloss: 0.0128792\n",
      "[80]\tvalid's binary_logloss: 0.0121728\n",
      "[90]\tvalid's binary_logloss: 0.0116287\n",
      "[100]\tvalid's binary_logloss: 0.0111217\n",
      "[110]\tvalid's binary_logloss: 0.0108263\n",
      "[120]\tvalid's binary_logloss: 0.0105962\n",
      "[130]\tvalid's binary_logloss: 0.0104914\n",
      "[140]\tvalid's binary_logloss: 0.0103481\n",
      "[150]\tvalid's binary_logloss: 0.0103342\n",
      "[160]\tvalid's binary_logloss: 0.0102945\n",
      "[170]\tvalid's binary_logloss: 0.0103768\n",
      "[180]\tvalid's binary_logloss: 0.0103042\n",
      "[190]\tvalid's binary_logloss: 0.0103281\n",
      "Early stopping, best iteration is:\n",
      "[161]\tvalid's binary_logloss: 0.010259\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  46.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0949254\n",
      "[20]\tvalid's binary_logloss: 0.0507696\n",
      "[30]\tvalid's binary_logloss: 0.0321696\n",
      "[40]\tvalid's binary_logloss: 0.0229974\n",
      "[50]\tvalid's binary_logloss: 0.0179217\n",
      "[60]\tvalid's binary_logloss: 0.0151909\n",
      "[70]\tvalid's binary_logloss: 0.0136189\n",
      "[80]\tvalid's binary_logloss: 0.0126355\n",
      "[90]\tvalid's binary_logloss: 0.0119766\n",
      "[100]\tvalid's binary_logloss: 0.011525\n",
      "[110]\tvalid's binary_logloss: 0.0113012\n",
      "[120]\tvalid's binary_logloss: 0.0109943\n",
      "[130]\tvalid's binary_logloss: 0.010832\n",
      "[140]\tvalid's binary_logloss: 0.0106638\n",
      "[150]\tvalid's binary_logloss: 0.0106097\n",
      "[160]\tvalid's binary_logloss: 0.010593\n",
      "[170]\tvalid's binary_logloss: 0.0105199\n",
      "[180]\tvalid's binary_logloss: 0.0105059\n",
      "[190]\tvalid's binary_logloss: 0.0104609\n",
      "[200]\tvalid's binary_logloss: 0.0103981\n",
      "[210]\tvalid's binary_logloss: 0.0103406\n",
      "[220]\tvalid's binary_logloss: 0.0102811\n",
      "[230]\tvalid's binary_logloss: 0.0102539\n",
      "[240]\tvalid's binary_logloss: 0.0102355\n",
      "[250]\tvalid's binary_logloss: 0.0102003\n",
      "[260]\tvalid's binary_logloss: 0.0101717\n",
      "[270]\tvalid's binary_logloss: 0.0101537\n",
      "[280]\tvalid's binary_logloss: 0.0101574\n",
      "[290]\tvalid's binary_logloss: 0.0101574\n",
      "[300]\tvalid's binary_logloss: 0.0101574\n",
      "Early stopping, best iteration is:\n",
      "[270]\tvalid's binary_logloss: 0.0101537\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9411764705882353, total=  50.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0966835\n",
      "[20]\tvalid's binary_logloss: 0.0528896\n",
      "[30]\tvalid's binary_logloss: 0.0337677\n",
      "[40]\tvalid's binary_logloss: 0.0253112\n",
      "[50]\tvalid's binary_logloss: 0.0204368\n",
      "[60]\tvalid's binary_logloss: 0.0178434\n",
      "[70]\tvalid's binary_logloss: 0.0162092\n",
      "[80]\tvalid's binary_logloss: 0.0153297\n",
      "[90]\tvalid's binary_logloss: 0.0148954\n",
      "[100]\tvalid's binary_logloss: 0.0147025\n",
      "[110]\tvalid's binary_logloss: 0.0145803\n",
      "[120]\tvalid's binary_logloss: 0.0145211\n",
      "[130]\tvalid's binary_logloss: 0.0143316\n",
      "[140]\tvalid's binary_logloss: 0.0143501\n",
      "[150]\tvalid's binary_logloss: 0.0144278\n",
      "[160]\tvalid's binary_logloss: 0.014363\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid's binary_logloss: 0.0142884\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  51.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0931404\n",
      "[20]\tvalid's binary_logloss: 0.0505119\n",
      "[30]\tvalid's binary_logloss: 0.0318603\n",
      "[40]\tvalid's binary_logloss: 0.0225813\n",
      "[50]\tvalid's binary_logloss: 0.0179433\n",
      "[60]\tvalid's binary_logloss: 0.0150724\n",
      "[70]\tvalid's binary_logloss: 0.0133308\n",
      "[80]\tvalid's binary_logloss: 0.0123336\n",
      "[90]\tvalid's binary_logloss: 0.0117268\n",
      "[100]\tvalid's binary_logloss: 0.0112184\n",
      "[110]\tvalid's binary_logloss: 0.0109891\n",
      "[120]\tvalid's binary_logloss: 0.0108244\n",
      "[130]\tvalid's binary_logloss: 0.0106484\n",
      "[140]\tvalid's binary_logloss: 0.010516\n",
      "[150]\tvalid's binary_logloss: 0.0104619\n",
      "[160]\tvalid's binary_logloss: 0.010387\n",
      "[170]\tvalid's binary_logloss: 0.0104418\n",
      "[180]\tvalid's binary_logloss: 0.0103515\n",
      "[190]\tvalid's binary_logloss: 0.0102759\n",
      "[200]\tvalid's binary_logloss: 0.0102188\n",
      "[210]\tvalid's binary_logloss: 0.0101652\n",
      "[220]\tvalid's binary_logloss: 0.0101382\n",
      "[230]\tvalid's binary_logloss: 0.0101466\n",
      "[240]\tvalid's binary_logloss: 0.0100996\n",
      "[250]\tvalid's binary_logloss: 0.010081\n",
      "[260]\tvalid's binary_logloss: 0.010083\n",
      "[270]\tvalid's binary_logloss: 0.0100519\n",
      "[280]\tvalid's binary_logloss: 0.010055\n",
      "[290]\tvalid's binary_logloss: 0.0100552\n",
      "Early stopping, best iteration is:\n",
      "[267]\tvalid's binary_logloss: 0.0100369\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  55.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0894271\n",
      "[20]\tvalid's binary_logloss: 0.046392\n",
      "[30]\tvalid's binary_logloss: 0.0279622\n",
      "[40]\tvalid's binary_logloss: 0.0188404\n",
      "[50]\tvalid's binary_logloss: 0.0149669\n",
      "[60]\tvalid's binary_logloss: 0.0126106\n",
      "[70]\tvalid's binary_logloss: 0.0116314\n",
      "[80]\tvalid's binary_logloss: 0.0110977\n",
      "[90]\tvalid's binary_logloss: 0.0108233\n",
      "[100]\tvalid's binary_logloss: 0.0107336\n",
      "[110]\tvalid's binary_logloss: 0.0107446\n",
      "[120]\tvalid's binary_logloss: 0.0106143\n",
      "[130]\tvalid's binary_logloss: 0.010645\n",
      "[140]\tvalid's binary_logloss: 0.0106804\n",
      "[150]\tvalid's binary_logloss: 0.01067\n",
      "Early stopping, best iteration is:\n",
      "[128]\tvalid's binary_logloss: 0.0105462\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  54.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0892669\n",
      "[20]\tvalid's binary_logloss: 0.0467747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalid's binary_logloss: 0.0289609\n",
      "[40]\tvalid's binary_logloss: 0.020937\n",
      "[50]\tvalid's binary_logloss: 0.0164452\n",
      "[60]\tvalid's binary_logloss: 0.0141998\n",
      "[70]\tvalid's binary_logloss: 0.0126585\n",
      "[80]\tvalid's binary_logloss: 0.0120905\n",
      "[90]\tvalid's binary_logloss: 0.0115252\n",
      "[100]\tvalid's binary_logloss: 0.0112103\n",
      "[110]\tvalid's binary_logloss: 0.0109107\n",
      "[120]\tvalid's binary_logloss: 0.0108825\n",
      "[130]\tvalid's binary_logloss: 0.0108354\n",
      "[140]\tvalid's binary_logloss: 0.010798\n",
      "[150]\tvalid's binary_logloss: 0.0107995\n",
      "[160]\tvalid's binary_logloss: 0.0107995\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid's binary_logloss: 0.0107729\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9411764705882353, total=  56.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0920661\n",
      "[20]\tvalid's binary_logloss: 0.0492056\n",
      "[30]\tvalid's binary_logloss: 0.0303852\n",
      "[40]\tvalid's binary_logloss: 0.0221752\n",
      "[50]\tvalid's binary_logloss: 0.0177464\n",
      "[60]\tvalid's binary_logloss: 0.015693\n",
      "[70]\tvalid's binary_logloss: 0.0148287\n",
      "[80]\tvalid's binary_logloss: 0.0141402\n",
      "[90]\tvalid's binary_logloss: 0.0141063\n",
      "[100]\tvalid's binary_logloss: 0.0141627\n",
      "[110]\tvalid's binary_logloss: 0.0142716\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid's binary_logloss: 0.0140366\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9043478260869565, total=  58.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0895059\n",
      "[20]\tvalid's binary_logloss: 0.0474749\n",
      "[30]\tvalid's binary_logloss: 0.0296643\n",
      "[40]\tvalid's binary_logloss: 0.0204995\n",
      "[50]\tvalid's binary_logloss: 0.0159638\n",
      "[60]\tvalid's binary_logloss: 0.0134468\n",
      "[70]\tvalid's binary_logloss: 0.0121038\n",
      "[80]\tvalid's binary_logloss: 0.0112485\n",
      "[90]\tvalid's binary_logloss: 0.0107695\n",
      "[100]\tvalid's binary_logloss: 0.0106069\n",
      "[110]\tvalid's binary_logloss: 0.0104948\n",
      "[120]\tvalid's binary_logloss: 0.0106048\n",
      "[130]\tvalid's binary_logloss: 0.010454\n",
      "[140]\tvalid's binary_logloss: 0.0104387\n",
      "[150]\tvalid's binary_logloss: 0.0104325\n",
      "[160]\tvalid's binary_logloss: 0.0104325\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid's binary_logloss: 0.0104233\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total= 1.0min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0885512\n",
      "[20]\tvalid's binary_logloss: 0.0454268\n",
      "[30]\tvalid's binary_logloss: 0.0268005\n",
      "[40]\tvalid's binary_logloss: 0.0185255\n",
      "[50]\tvalid's binary_logloss: 0.0144257\n",
      "[60]\tvalid's binary_logloss: 0.012088\n",
      "[70]\tvalid's binary_logloss: 0.0113445\n",
      "[80]\tvalid's binary_logloss: 0.0107797\n",
      "[90]\tvalid's binary_logloss: 0.0106465\n",
      "[100]\tvalid's binary_logloss: 0.0106836\n",
      "[110]\tvalid's binary_logloss: 0.0105844\n",
      "[120]\tvalid's binary_logloss: 0.0106635\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid's binary_logloss: 0.0105745\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total= 1.0min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0887213\n",
      "[20]\tvalid's binary_logloss: 0.046018\n",
      "[30]\tvalid's binary_logloss: 0.0281731\n",
      "[40]\tvalid's binary_logloss: 0.0200342\n",
      "[50]\tvalid's binary_logloss: 0.0160884\n",
      "[60]\tvalid's binary_logloss: 0.0139571\n",
      "[70]\tvalid's binary_logloss: 0.0123012\n",
      "[80]\tvalid's binary_logloss: 0.0119813\n",
      "[90]\tvalid's binary_logloss: 0.0116471\n",
      "[100]\tvalid's binary_logloss: 0.0114315\n",
      "[110]\tvalid's binary_logloss: 0.0112918\n",
      "[120]\tvalid's binary_logloss: 0.0111936\n",
      "[130]\tvalid's binary_logloss: 0.0111672\n",
      "[140]\tvalid's binary_logloss: 0.0111635\n",
      "[150]\tvalid's binary_logloss: 0.0111637\n",
      "Early stopping, best iteration is:\n",
      "[126]\tvalid's binary_logloss: 0.0111584\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total= 1.1min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0917201\n",
      "[20]\tvalid's binary_logloss: 0.049831\n",
      "[30]\tvalid's binary_logloss: 0.0309396\n",
      "[40]\tvalid's binary_logloss: 0.0227365\n",
      "[50]\tvalid's binary_logloss: 0.0184573\n",
      "[60]\tvalid's binary_logloss: 0.0163485\n",
      "[70]\tvalid's binary_logloss: 0.0154468\n",
      "[80]\tvalid's binary_logloss: 0.0150323\n",
      "[90]\tvalid's binary_logloss: 0.0148628\n",
      "[100]\tvalid's binary_logloss: 0.0147125\n",
      "[110]\tvalid's binary_logloss: 0.0149581\n",
      "[120]\tvalid's binary_logloss: 0.0150126\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid's binary_logloss: 0.0146756\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.8849557522123894, total= 1.2min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0885198\n",
      "[20]\tvalid's binary_logloss: 0.0468039\n",
      "[30]\tvalid's binary_logloss: 0.0289959\n",
      "[40]\tvalid's binary_logloss: 0.0199027\n",
      "[50]\tvalid's binary_logloss: 0.0150565\n",
      "[60]\tvalid's binary_logloss: 0.0125777\n",
      "[70]\tvalid's binary_logloss: 0.0114792\n",
      "[80]\tvalid's binary_logloss: 0.0106992\n",
      "[90]\tvalid's binary_logloss: 0.0105407\n",
      "[100]\tvalid's binary_logloss: 0.0102309\n",
      "[110]\tvalid's binary_logloss: 0.0100766\n",
      "[120]\tvalid's binary_logloss: 0.00994738\n",
      "[130]\tvalid's binary_logloss: 0.00992805\n",
      "[140]\tvalid's binary_logloss: 0.00992805\n",
      "[150]\tvalid's binary_logloss: 0.00992805\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid's binary_logloss: 0.00987015\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9137931034482758, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0972043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tvalid's binary_logloss: 0.0532205\n",
      "[30]\tvalid's binary_logloss: 0.0342323\n",
      "[40]\tvalid's binary_logloss: 0.0253504\n",
      "[50]\tvalid's binary_logloss: 0.0203567\n",
      "[60]\tvalid's binary_logloss: 0.0174739\n",
      "[70]\tvalid's binary_logloss: 0.0153818\n",
      "[80]\tvalid's binary_logloss: 0.0142184\n",
      "[90]\tvalid's binary_logloss: 0.0134061\n",
      "[100]\tvalid's binary_logloss: 0.0129318\n",
      "[110]\tvalid's binary_logloss: 0.0128083\n",
      "[120]\tvalid's binary_logloss: 0.0128083\n",
      "[130]\tvalid's binary_logloss: 0.0128083\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid's binary_logloss: 0.0128083\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.923076923076923, total= 1.6min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0963368\n",
      "[20]\tvalid's binary_logloss: 0.0535315\n",
      "[30]\tvalid's binary_logloss: 0.0358251\n",
      "[40]\tvalid's binary_logloss: 0.0267178\n",
      "[50]\tvalid's binary_logloss: 0.0217226\n",
      "[60]\tvalid's binary_logloss: 0.0186848\n",
      "[70]\tvalid's binary_logloss: 0.0166997\n",
      "[80]\tvalid's binary_logloss: 0.0154774\n",
      "[90]\tvalid's binary_logloss: 0.0145658\n",
      "[100]\tvalid's binary_logloss: 0.0141197\n",
      "[110]\tvalid's binary_logloss: 0.0139665\n",
      "[120]\tvalid's binary_logloss: 0.0139665\n",
      "[130]\tvalid's binary_logloss: 0.0139665\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid's binary_logloss: 0.0139649\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.8869565217391304, total= 1.6min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0994751\n",
      "[20]\tvalid's binary_logloss: 0.0560104\n",
      "[30]\tvalid's binary_logloss: 0.0373047\n",
      "[40]\tvalid's binary_logloss: 0.0276327\n",
      "[50]\tvalid's binary_logloss: 0.0225099\n",
      "[60]\tvalid's binary_logloss: 0.0198617\n",
      "[70]\tvalid's binary_logloss: 0.0178898\n",
      "[80]\tvalid's binary_logloss: 0.016894\n",
      "[90]\tvalid's binary_logloss: 0.0161831\n",
      "[100]\tvalid's binary_logloss: 0.0159072\n",
      "[110]\tvalid's binary_logloss: 0.0157341\n",
      "[120]\tvalid's binary_logloss: 0.0157448\n",
      "[130]\tvalid's binary_logloss: 0.0157448\n",
      "[140]\tvalid's binary_logloss: 0.0157448\n",
      "Early stopping, best iteration is:\n",
      "[114]\tvalid's binary_logloss: 0.0157327\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.8849557522123894, total= 1.7min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0940511\n",
      "[20]\tvalid's binary_logloss: 0.0533288\n",
      "[30]\tvalid's binary_logloss: 0.0353715\n",
      "[40]\tvalid's binary_logloss: 0.025506\n",
      "[50]\tvalid's binary_logloss: 0.0202613\n",
      "[60]\tvalid's binary_logloss: 0.0172773\n",
      "[70]\tvalid's binary_logloss: 0.0155311\n",
      "[80]\tvalid's binary_logloss: 0.0143423\n",
      "[90]\tvalid's binary_logloss: 0.0135816\n",
      "[100]\tvalid's binary_logloss: 0.0129965\n",
      "[110]\tvalid's binary_logloss: 0.0127825\n",
      "[120]\tvalid's binary_logloss: 0.0127649\n",
      "[130]\tvalid's binary_logloss: 0.0127653\n",
      "[140]\tvalid's binary_logloss: 0.0127655\n",
      "[150]\tvalid's binary_logloss: 0.0127655\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid's binary_logloss: 0.0127649\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.8965517241379309, total= 1.7min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0929964\n",
      "[20]\tvalid's binary_logloss: 0.050258\n",
      "[30]\tvalid's binary_logloss: 0.0320953\n",
      "[40]\tvalid's binary_logloss: 0.0234356\n",
      "[50]\tvalid's binary_logloss: 0.0187439\n",
      "[60]\tvalid's binary_logloss: 0.0158936\n",
      "[70]\tvalid's binary_logloss: 0.0145914\n",
      "[80]\tvalid's binary_logloss: 0.0134666\n",
      "[90]\tvalid's binary_logloss: 0.0131404\n",
      "[100]\tvalid's binary_logloss: 0.0131404\n",
      "[110]\tvalid's binary_logloss: 0.0131404\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's binary_logloss: 0.0131404\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total= 1.7min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0927376\n",
      "[20]\tvalid's binary_logloss: 0.0505692\n",
      "[30]\tvalid's binary_logloss: 0.0328199\n",
      "[40]\tvalid's binary_logloss: 0.0243927\n",
      "[50]\tvalid's binary_logloss: 0.0199319\n",
      "[60]\tvalid's binary_logloss: 0.0172986\n",
      "[70]\tvalid's binary_logloss: 0.0156947\n",
      "[80]\tvalid's binary_logloss: 0.0147481\n",
      "[90]\tvalid's binary_logloss: 0.014129\n",
      "[100]\tvalid's binary_logloss: 0.0140889\n",
      "[110]\tvalid's binary_logloss: 0.014089\n",
      "[120]\tvalid's binary_logloss: 0.0140891\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid's binary_logloss: 0.0140888\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.923076923076923, total= 1.6min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0945958\n",
      "[20]\tvalid's binary_logloss: 0.0524931\n",
      "[30]\tvalid's binary_logloss: 0.0347845\n",
      "[40]\tvalid's binary_logloss: 0.0259523\n",
      "[50]\tvalid's binary_logloss: 0.021439\n",
      "[60]\tvalid's binary_logloss: 0.0188937\n",
      "[70]\tvalid's binary_logloss: 0.0174084\n",
      "[80]\tvalid's binary_logloss: 0.0166219\n",
      "[90]\tvalid's binary_logloss: 0.01631\n",
      "[100]\tvalid's binary_logloss: 0.01631\n",
      "[110]\tvalid's binary_logloss: 0.01631\n",
      "[120]\tvalid's binary_logloss: 0.01631\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's binary_logloss: 0.01631\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.8947368421052632, total= 1.7min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0927203\n",
      "[20]\tvalid's binary_logloss: 0.0515046\n",
      "[30]\tvalid's binary_logloss: 0.0332084\n",
      "[40]\tvalid's binary_logloss: 0.0235106\n",
      "[50]\tvalid's binary_logloss: 0.0185106\n",
      "[60]\tvalid's binary_logloss: 0.0156931\n",
      "[70]\tvalid's binary_logloss: 0.0140772\n",
      "[80]\tvalid's binary_logloss: 0.0130383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90]\tvalid's binary_logloss: 0.0125328\n",
      "[100]\tvalid's binary_logloss: 0.0125415\n",
      "[110]\tvalid's binary_logloss: 0.0125415\n",
      "[120]\tvalid's binary_logloss: 0.0125415\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's binary_logloss: 0.0125328\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.8869565217391304, total= 1.7min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0907469\n",
      "[20]\tvalid's binary_logloss: 0.0497535\n",
      "[30]\tvalid's binary_logloss: 0.0309537\n",
      "[40]\tvalid's binary_logloss: 0.0228444\n",
      "[50]\tvalid's binary_logloss: 0.0184286\n",
      "[60]\tvalid's binary_logloss: 0.0156789\n",
      "[70]\tvalid's binary_logloss: 0.0140555\n",
      "[80]\tvalid's binary_logloss: 0.0133474\n",
      "[90]\tvalid's binary_logloss: 0.0130514\n",
      "[100]\tvalid's binary_logloss: 0.0130515\n",
      "[110]\tvalid's binary_logloss: 0.0130515\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid's binary_logloss: 0.0130514\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9152542372881356, total= 1.7min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0921967\n",
      "[20]\tvalid's binary_logloss: 0.0500064\n",
      "[30]\tvalid's binary_logloss: 0.0322213\n",
      "[40]\tvalid's binary_logloss: 0.0239782\n",
      "[50]\tvalid's binary_logloss: 0.0196709\n",
      "[60]\tvalid's binary_logloss: 0.0170768\n",
      "[70]\tvalid's binary_logloss: 0.0155735\n",
      "[80]\tvalid's binary_logloss: 0.0147334\n",
      "[90]\tvalid's binary_logloss: 0.0144171\n",
      "[100]\tvalid's binary_logloss: 0.0143845\n",
      "[110]\tvalid's binary_logloss: 0.0143845\n",
      "[120]\tvalid's binary_logloss: 0.0143845\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid's binary_logloss: 0.0143784\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.8869565217391304, total= 1.7min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0940942\n",
      "[20]\tvalid's binary_logloss: 0.0516834\n",
      "[30]\tvalid's binary_logloss: 0.0342663\n",
      "[40]\tvalid's binary_logloss: 0.0258426\n",
      "[50]\tvalid's binary_logloss: 0.0214021\n",
      "[60]\tvalid's binary_logloss: 0.0188203\n",
      "[70]\tvalid's binary_logloss: 0.0172914\n",
      "[80]\tvalid's binary_logloss: 0.0167219\n",
      "[90]\tvalid's binary_logloss: 0.0164802\n",
      "[100]\tvalid's binary_logloss: 0.0164805\n",
      "[110]\tvalid's binary_logloss: 0.0164806\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid's binary_logloss: 0.01648\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.8947368421052632, total=  40.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0923976\n",
      "[20]\tvalid's binary_logloss: 0.0505755\n",
      "[30]\tvalid's binary_logloss: 0.0324094\n",
      "[40]\tvalid's binary_logloss: 0.022707\n",
      "[50]\tvalid's binary_logloss: 0.0176931\n",
      "[60]\tvalid's binary_logloss: 0.015294\n",
      "[70]\tvalid's binary_logloss: 0.0138752\n",
      "[80]\tvalid's binary_logloss: 0.0128335\n",
      "[90]\tvalid's binary_logloss: 0.0124402\n",
      "[100]\tvalid's binary_logloss: 0.0124293\n",
      "[110]\tvalid's binary_logloss: 0.0124293\n",
      "[120]\tvalid's binary_logloss: 0.0124293\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid's binary_logloss: 0.0124293\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  41.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0932861\n",
      "[20]\tvalid's binary_logloss: 0.0494427\n",
      "[30]\tvalid's binary_logloss: 0.030484\n",
      "[40]\tvalid's binary_logloss: 0.0215331\n",
      "[50]\tvalid's binary_logloss: 0.0168609\n",
      "[60]\tvalid's binary_logloss: 0.0144694\n",
      "[70]\tvalid's binary_logloss: 0.0130997\n",
      "[80]\tvalid's binary_logloss: 0.0121557\n",
      "[90]\tvalid's binary_logloss: 0.0117383\n",
      "[100]\tvalid's binary_logloss: 0.0114453\n",
      "[110]\tvalid's binary_logloss: 0.0111692\n",
      "[120]\tvalid's binary_logloss: 0.0111912\n",
      "[130]\tvalid's binary_logloss: 0.0110719\n",
      "[140]\tvalid's binary_logloss: 0.0111251\n",
      "[150]\tvalid's binary_logloss: 0.0110881\n",
      "[160]\tvalid's binary_logloss: 0.0111294\n",
      "[170]\tvalid's binary_logloss: 0.0111105\n",
      "[180]\tvalid's binary_logloss: 0.0110834\n",
      "Early stopping, best iteration is:\n",
      "[153]\tvalid's binary_logloss: 0.0110287\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  51.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0946626\n",
      "[20]\tvalid's binary_logloss: 0.0505159\n",
      "[30]\tvalid's binary_logloss: 0.0320799\n",
      "[40]\tvalid's binary_logloss: 0.0229103\n",
      "[50]\tvalid's binary_logloss: 0.0180791\n",
      "[60]\tvalid's binary_logloss: 0.015314\n",
      "[70]\tvalid's binary_logloss: 0.0136881\n",
      "[80]\tvalid's binary_logloss: 0.0125998\n",
      "[90]\tvalid's binary_logloss: 0.0119304\n",
      "[100]\tvalid's binary_logloss: 0.011545\n",
      "[110]\tvalid's binary_logloss: 0.011284\n",
      "[120]\tvalid's binary_logloss: 0.0110281\n",
      "[130]\tvalid's binary_logloss: 0.0108555\n",
      "[140]\tvalid's binary_logloss: 0.010759\n",
      "[150]\tvalid's binary_logloss: 0.01058\n",
      "[160]\tvalid's binary_logloss: 0.0105441\n",
      "[170]\tvalid's binary_logloss: 0.0104514\n",
      "[180]\tvalid's binary_logloss: 0.0104233\n",
      "[190]\tvalid's binary_logloss: 0.0103581\n",
      "[200]\tvalid's binary_logloss: 0.0103231\n",
      "[210]\tvalid's binary_logloss: 0.0103633\n",
      "[220]\tvalid's binary_logloss: 0.0103388\n",
      "Early stopping, best iteration is:\n",
      "[194]\tvalid's binary_logloss: 0.010314\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  53.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0963031\n",
      "[20]\tvalid's binary_logloss: 0.0524098\n",
      "[30]\tvalid's binary_logloss: 0.0336716\n",
      "[40]\tvalid's binary_logloss: 0.0248655\n",
      "[50]\tvalid's binary_logloss: 0.0201402\n",
      "[60]\tvalid's binary_logloss: 0.0175498\n",
      "[70]\tvalid's binary_logloss: 0.0160841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80]\tvalid's binary_logloss: 0.0152615\n",
      "[90]\tvalid's binary_logloss: 0.0149374\n",
      "[100]\tvalid's binary_logloss: 0.0148011\n",
      "[110]\tvalid's binary_logloss: 0.01468\n",
      "[120]\tvalid's binary_logloss: 0.014623\n",
      "[130]\tvalid's binary_logloss: 0.0146829\n",
      "[140]\tvalid's binary_logloss: 0.014762\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid's binary_logloss: 0.0146224\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  45.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0928242\n",
      "[20]\tvalid's binary_logloss: 0.0505185\n",
      "[30]\tvalid's binary_logloss: 0.0321786\n",
      "[40]\tvalid's binary_logloss: 0.0231276\n",
      "[50]\tvalid's binary_logloss: 0.0179645\n",
      "[60]\tvalid's binary_logloss: 0.0150084\n",
      "[70]\tvalid's binary_logloss: 0.0133792\n",
      "[80]\tvalid's binary_logloss: 0.0122213\n",
      "[90]\tvalid's binary_logloss: 0.0116883\n",
      "[100]\tvalid's binary_logloss: 0.0112682\n",
      "[110]\tvalid's binary_logloss: 0.0108952\n",
      "[120]\tvalid's binary_logloss: 0.0107977\n",
      "[130]\tvalid's binary_logloss: 0.010545\n",
      "[140]\tvalid's binary_logloss: 0.0104192\n",
      "[150]\tvalid's binary_logloss: 0.0103666\n",
      "[160]\tvalid's binary_logloss: 0.0103276\n",
      "[170]\tvalid's binary_logloss: 0.0102812\n",
      "[180]\tvalid's binary_logloss: 0.010208\n",
      "[190]\tvalid's binary_logloss: 0.0101912\n",
      "[200]\tvalid's binary_logloss: 0.0101918\n",
      "[210]\tvalid's binary_logloss: 0.0101885\n",
      "[220]\tvalid's binary_logloss: 0.010175\n",
      "[230]\tvalid's binary_logloss: 0.0101671\n",
      "[240]\tvalid's binary_logloss: 0.0101581\n",
      "[250]\tvalid's binary_logloss: 0.0101782\n",
      "[260]\tvalid's binary_logloss: 0.0101941\n",
      "Early stopping, best iteration is:\n",
      "[238]\tvalid's binary_logloss: 0.0101456\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  53.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0892818\n",
      "[20]\tvalid's binary_logloss: 0.046298\n",
      "[30]\tvalid's binary_logloss: 0.0280209\n",
      "[40]\tvalid's binary_logloss: 0.0193231\n",
      "[50]\tvalid's binary_logloss: 0.0150211\n",
      "[60]\tvalid's binary_logloss: 0.0129917\n",
      "[70]\tvalid's binary_logloss: 0.0119025\n",
      "[80]\tvalid's binary_logloss: 0.0114321\n",
      "[90]\tvalid's binary_logloss: 0.0113186\n",
      "[100]\tvalid's binary_logloss: 0.0114139\n",
      "[110]\tvalid's binary_logloss: 0.0115949\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's binary_logloss: 0.0112269\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  44.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0882476\n",
      "[20]\tvalid's binary_logloss: 0.0457284\n",
      "[30]\tvalid's binary_logloss: 0.0282226\n",
      "[40]\tvalid's binary_logloss: 0.0201653\n",
      "[50]\tvalid's binary_logloss: 0.0158068\n",
      "[60]\tvalid's binary_logloss: 0.0134284\n",
      "[70]\tvalid's binary_logloss: 0.0123694\n",
      "[80]\tvalid's binary_logloss: 0.0117893\n",
      "[90]\tvalid's binary_logloss: 0.0114529\n",
      "[100]\tvalid's binary_logloss: 0.0112781\n",
      "[110]\tvalid's binary_logloss: 0.0113205\n",
      "[120]\tvalid's binary_logloss: 0.0112988\n",
      "[130]\tvalid's binary_logloss: 0.0112718\n",
      "Early stopping, best iteration is:\n",
      "[102]\tvalid's binary_logloss: 0.0112063\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  47.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0917121\n",
      "[20]\tvalid's binary_logloss: 0.0489891\n",
      "[30]\tvalid's binary_logloss: 0.0305924\n",
      "[40]\tvalid's binary_logloss: 0.0225551\n",
      "[50]\tvalid's binary_logloss: 0.0179916\n",
      "[60]\tvalid's binary_logloss: 0.015685\n",
      "[70]\tvalid's binary_logloss: 0.0147658\n",
      "[80]\tvalid's binary_logloss: 0.014674\n",
      "[90]\tvalid's binary_logloss: 0.0147104\n",
      "[100]\tvalid's binary_logloss: 0.0147717\n",
      "[110]\tvalid's binary_logloss: 0.0151304\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid's binary_logloss: 0.0146255\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  43.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0889095\n",
      "[20]\tvalid's binary_logloss: 0.0470462\n",
      "[30]\tvalid's binary_logloss: 0.0287452\n",
      "[40]\tvalid's binary_logloss: 0.0203001\n",
      "[50]\tvalid's binary_logloss: 0.0158666\n",
      "[60]\tvalid's binary_logloss: 0.0130501\n",
      "[70]\tvalid's binary_logloss: 0.0118776\n",
      "[80]\tvalid's binary_logloss: 0.0111444\n",
      "[90]\tvalid's binary_logloss: 0.010991\n",
      "[100]\tvalid's binary_logloss: 0.0108112\n",
      "[110]\tvalid's binary_logloss: 0.0108054\n",
      "[120]\tvalid's binary_logloss: 0.0107751\n",
      "[130]\tvalid's binary_logloss: 0.0107998\n",
      "[140]\tvalid's binary_logloss: 0.010829\n",
      "[150]\tvalid's binary_logloss: 0.0108735\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid's binary_logloss: 0.0107254\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.923076923076923, total=  46.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0882275\n",
      "[20]\tvalid's binary_logloss: 0.0454328\n",
      "[30]\tvalid's binary_logloss: 0.0268276\n",
      "[40]\tvalid's binary_logloss: 0.0186009\n",
      "[50]\tvalid's binary_logloss: 0.014414\n",
      "[60]\tvalid's binary_logloss: 0.0124107\n",
      "[70]\tvalid's binary_logloss: 0.0116009\n",
      "[80]\tvalid's binary_logloss: 0.0114113\n",
      "[90]\tvalid's binary_logloss: 0.0117779\n",
      "[100]\tvalid's binary_logloss: 0.0120357\n",
      "[110]\tvalid's binary_logloss: 0.0126673\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's binary_logloss: 0.0114113\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  46.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0880298\n",
      "[20]\tvalid's binary_logloss: 0.0463705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalid's binary_logloss: 0.0286364\n",
      "[40]\tvalid's binary_logloss: 0.0197203\n",
      "[50]\tvalid's binary_logloss: 0.0155504\n",
      "[60]\tvalid's binary_logloss: 0.0132114\n",
      "[70]\tvalid's binary_logloss: 0.0121348\n",
      "[80]\tvalid's binary_logloss: 0.0113077\n",
      "[90]\tvalid's binary_logloss: 0.0108484\n",
      "[100]\tvalid's binary_logloss: 0.0109067\n",
      "[110]\tvalid's binary_logloss: 0.011098\n",
      "[120]\tvalid's binary_logloss: 0.0115504\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid's binary_logloss: 0.0107769\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  48.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.092875\n",
      "[20]\tvalid's binary_logloss: 0.04969\n",
      "[30]\tvalid's binary_logloss: 0.0308121\n",
      "[40]\tvalid's binary_logloss: 0.0218861\n",
      "[50]\tvalid's binary_logloss: 0.0176439\n",
      "[60]\tvalid's binary_logloss: 0.0160177\n",
      "[70]\tvalid's binary_logloss: 0.0149733\n",
      "[80]\tvalid's binary_logloss: 0.014672\n",
      "[90]\tvalid's binary_logloss: 0.0146037\n",
      "[100]\tvalid's binary_logloss: 0.0149315\n",
      "[110]\tvalid's binary_logloss: 0.0155006\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid's binary_logloss: 0.014467\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.8750000000000001, total=  46.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0880636\n",
      "[20]\tvalid's binary_logloss: 0.0471637\n",
      "[30]\tvalid's binary_logloss: 0.0298009\n",
      "[40]\tvalid's binary_logloss: 0.0214684\n",
      "[50]\tvalid's binary_logloss: 0.0164837\n",
      "[60]\tvalid's binary_logloss: 0.0138193\n",
      "[70]\tvalid's binary_logloss: 0.0122176\n",
      "[80]\tvalid's binary_logloss: 0.0113611\n",
      "[90]\tvalid's binary_logloss: 0.0110642\n",
      "[100]\tvalid's binary_logloss: 0.0110197\n",
      "[110]\tvalid's binary_logloss: 0.0110424\n",
      "[120]\tvalid's binary_logloss: 0.0112557\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's binary_logloss: 0.01095\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  46.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0937044\n",
      "[20]\tvalid's binary_logloss: 0.0497217\n",
      "[30]\tvalid's binary_logloss: 0.030912\n",
      "[40]\tvalid's binary_logloss: 0.0217292\n",
      "[50]\tvalid's binary_logloss: 0.0169949\n",
      "[60]\tvalid's binary_logloss: 0.0145588\n",
      "[70]\tvalid's binary_logloss: 0.0128792\n",
      "[80]\tvalid's binary_logloss: 0.0121728\n",
      "[90]\tvalid's binary_logloss: 0.0116287\n",
      "[100]\tvalid's binary_logloss: 0.0111217\n",
      "[110]\tvalid's binary_logloss: 0.0108263\n",
      "[120]\tvalid's binary_logloss: 0.0105962\n",
      "[130]\tvalid's binary_logloss: 0.0104914\n",
      "[140]\tvalid's binary_logloss: 0.0103481\n",
      "[150]\tvalid's binary_logloss: 0.0103342\n",
      "[160]\tvalid's binary_logloss: 0.0102945\n",
      "[170]\tvalid's binary_logloss: 0.0103768\n",
      "[180]\tvalid's binary_logloss: 0.0103042\n",
      "[190]\tvalid's binary_logloss: 0.0103281\n",
      "Early stopping, best iteration is:\n",
      "[161]\tvalid's binary_logloss: 0.010259\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  46.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0949254\n",
      "[20]\tvalid's binary_logloss: 0.0507696\n",
      "[30]\tvalid's binary_logloss: 0.0321696\n",
      "[40]\tvalid's binary_logloss: 0.0229974\n",
      "[50]\tvalid's binary_logloss: 0.0179217\n",
      "[60]\tvalid's binary_logloss: 0.0151909\n",
      "[70]\tvalid's binary_logloss: 0.0136189\n",
      "[80]\tvalid's binary_logloss: 0.0126355\n",
      "[90]\tvalid's binary_logloss: 0.0119766\n",
      "[100]\tvalid's binary_logloss: 0.011525\n",
      "[110]\tvalid's binary_logloss: 0.0113012\n",
      "[120]\tvalid's binary_logloss: 0.0109943\n",
      "[130]\tvalid's binary_logloss: 0.010832\n",
      "[140]\tvalid's binary_logloss: 0.0106638\n",
      "[150]\tvalid's binary_logloss: 0.0106097\n",
      "[160]\tvalid's binary_logloss: 0.010593\n",
      "[170]\tvalid's binary_logloss: 0.0105199\n",
      "[180]\tvalid's binary_logloss: 0.0105059\n",
      "[190]\tvalid's binary_logloss: 0.0104609\n",
      "[200]\tvalid's binary_logloss: 0.0103981\n",
      "[210]\tvalid's binary_logloss: 0.0103406\n",
      "[220]\tvalid's binary_logloss: 0.0102811\n",
      "[230]\tvalid's binary_logloss: 0.0102539\n",
      "[240]\tvalid's binary_logloss: 0.0102355\n",
      "[250]\tvalid's binary_logloss: 0.0102003\n",
      "[260]\tvalid's binary_logloss: 0.0101717\n",
      "[270]\tvalid's binary_logloss: 0.0101537\n",
      "[280]\tvalid's binary_logloss: 0.0101574\n",
      "[290]\tvalid's binary_logloss: 0.0101574\n",
      "[300]\tvalid's binary_logloss: 0.0101574\n",
      "Early stopping, best iteration is:\n",
      "[270]\tvalid's binary_logloss: 0.0101537\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9411764705882353, total=  49.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0966835\n",
      "[20]\tvalid's binary_logloss: 0.0528896\n",
      "[30]\tvalid's binary_logloss: 0.0337677\n",
      "[40]\tvalid's binary_logloss: 0.0253112\n",
      "[50]\tvalid's binary_logloss: 0.0204368\n",
      "[60]\tvalid's binary_logloss: 0.0178434\n",
      "[70]\tvalid's binary_logloss: 0.0162092\n",
      "[80]\tvalid's binary_logloss: 0.0153297\n",
      "[90]\tvalid's binary_logloss: 0.0148954\n",
      "[100]\tvalid's binary_logloss: 0.0147025\n",
      "[110]\tvalid's binary_logloss: 0.0145803\n",
      "[120]\tvalid's binary_logloss: 0.0145211\n",
      "[130]\tvalid's binary_logloss: 0.0143316\n",
      "[140]\tvalid's binary_logloss: 0.0143501\n",
      "[150]\tvalid's binary_logloss: 0.0144278\n",
      "[160]\tvalid's binary_logloss: 0.014363\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid's binary_logloss: 0.0142884\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  45.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0931404\n",
      "[20]\tvalid's binary_logloss: 0.0505119\n",
      "[30]\tvalid's binary_logloss: 0.0318603\n",
      "[40]\tvalid's binary_logloss: 0.0225813\n",
      "[50]\tvalid's binary_logloss: 0.0179433\n",
      "[60]\tvalid's binary_logloss: 0.0150724\n",
      "[70]\tvalid's binary_logloss: 0.0133308\n",
      "[80]\tvalid's binary_logloss: 0.0123336\n",
      "[90]\tvalid's binary_logloss: 0.0117268\n",
      "[100]\tvalid's binary_logloss: 0.0112184\n",
      "[110]\tvalid's binary_logloss: 0.0109891\n",
      "[120]\tvalid's binary_logloss: 0.0108244\n",
      "[130]\tvalid's binary_logloss: 0.0106484\n",
      "[140]\tvalid's binary_logloss: 0.010516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150]\tvalid's binary_logloss: 0.0104619\n",
      "[160]\tvalid's binary_logloss: 0.010387\n",
      "[170]\tvalid's binary_logloss: 0.0104418\n",
      "[180]\tvalid's binary_logloss: 0.0103515\n",
      "[190]\tvalid's binary_logloss: 0.0102759\n",
      "[200]\tvalid's binary_logloss: 0.0102188\n",
      "[210]\tvalid's binary_logloss: 0.0101652\n",
      "[220]\tvalid's binary_logloss: 0.0101382\n",
      "[230]\tvalid's binary_logloss: 0.0101466\n",
      "[240]\tvalid's binary_logloss: 0.0100996\n",
      "[250]\tvalid's binary_logloss: 0.010081\n",
      "[260]\tvalid's binary_logloss: 0.010083\n",
      "[270]\tvalid's binary_logloss: 0.0100519\n",
      "[280]\tvalid's binary_logloss: 0.010055\n",
      "[290]\tvalid's binary_logloss: 0.0100552\n",
      "Early stopping, best iteration is:\n",
      "[267]\tvalid's binary_logloss: 0.0100369\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  50.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0894271\n",
      "[20]\tvalid's binary_logloss: 0.046392\n",
      "[30]\tvalid's binary_logloss: 0.0279622\n",
      "[40]\tvalid's binary_logloss: 0.0188404\n",
      "[50]\tvalid's binary_logloss: 0.0149669\n",
      "[60]\tvalid's binary_logloss: 0.0126106\n",
      "[70]\tvalid's binary_logloss: 0.0116314\n",
      "[80]\tvalid's binary_logloss: 0.0110977\n",
      "[90]\tvalid's binary_logloss: 0.0108233\n",
      "[100]\tvalid's binary_logloss: 0.0107336\n",
      "[110]\tvalid's binary_logloss: 0.0107446\n",
      "[120]\tvalid's binary_logloss: 0.0106143\n",
      "[130]\tvalid's binary_logloss: 0.010645\n",
      "[140]\tvalid's binary_logloss: 0.0106804\n",
      "[150]\tvalid's binary_logloss: 0.01067\n",
      "Early stopping, best iteration is:\n",
      "[128]\tvalid's binary_logloss: 0.0105462\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  45.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0892669\n",
      "[20]\tvalid's binary_logloss: 0.0467747\n",
      "[30]\tvalid's binary_logloss: 0.0289609\n",
      "[40]\tvalid's binary_logloss: 0.020937\n",
      "[50]\tvalid's binary_logloss: 0.0164452\n",
      "[60]\tvalid's binary_logloss: 0.0141998\n",
      "[70]\tvalid's binary_logloss: 0.0126585\n",
      "[80]\tvalid's binary_logloss: 0.0120905\n",
      "[90]\tvalid's binary_logloss: 0.0115252\n",
      "[100]\tvalid's binary_logloss: 0.0112103\n",
      "[110]\tvalid's binary_logloss: 0.0109107\n",
      "[120]\tvalid's binary_logloss: 0.0108825\n",
      "[130]\tvalid's binary_logloss: 0.0108354\n",
      "[140]\tvalid's binary_logloss: 0.010798\n",
      "[150]\tvalid's binary_logloss: 0.0107995\n",
      "[160]\tvalid's binary_logloss: 0.0107995\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid's binary_logloss: 0.0107729\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9411764705882353, total=  44.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0920661\n",
      "[20]\tvalid's binary_logloss: 0.0492056\n",
      "[30]\tvalid's binary_logloss: 0.0303852\n",
      "[40]\tvalid's binary_logloss: 0.0221752\n",
      "[50]\tvalid's binary_logloss: 0.0177464\n",
      "[60]\tvalid's binary_logloss: 0.015693\n",
      "[70]\tvalid's binary_logloss: 0.0148287\n",
      "[80]\tvalid's binary_logloss: 0.0141402\n",
      "[90]\tvalid's binary_logloss: 0.0141063\n",
      "[100]\tvalid's binary_logloss: 0.0141627\n",
      "[110]\tvalid's binary_logloss: 0.0142716\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid's binary_logloss: 0.0140366\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9043478260869565, total=  42.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0895059\n",
      "[20]\tvalid's binary_logloss: 0.0474749\n",
      "[30]\tvalid's binary_logloss: 0.0296643\n",
      "[40]\tvalid's binary_logloss: 0.0204995\n",
      "[50]\tvalid's binary_logloss: 0.0159638\n",
      "[60]\tvalid's binary_logloss: 0.0134468\n",
      "[70]\tvalid's binary_logloss: 0.0121038\n",
      "[80]\tvalid's binary_logloss: 0.0112485\n",
      "[90]\tvalid's binary_logloss: 0.0107695\n",
      "[100]\tvalid's binary_logloss: 0.0106069\n",
      "[110]\tvalid's binary_logloss: 0.0104948\n",
      "[120]\tvalid's binary_logloss: 0.0106048\n",
      "[130]\tvalid's binary_logloss: 0.010454\n",
      "[140]\tvalid's binary_logloss: 0.0104387\n",
      "[150]\tvalid's binary_logloss: 0.0104325\n",
      "[160]\tvalid's binary_logloss: 0.0104325\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid's binary_logloss: 0.0104233\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  44.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0885512\n",
      "[20]\tvalid's binary_logloss: 0.0454268\n",
      "[30]\tvalid's binary_logloss: 0.0268005\n",
      "[40]\tvalid's binary_logloss: 0.0185255\n",
      "[50]\tvalid's binary_logloss: 0.0144257\n",
      "[60]\tvalid's binary_logloss: 0.012088\n",
      "[70]\tvalid's binary_logloss: 0.0113445\n",
      "[80]\tvalid's binary_logloss: 0.0107797\n",
      "[90]\tvalid's binary_logloss: 0.0106465\n",
      "[100]\tvalid's binary_logloss: 0.0106836\n",
      "[110]\tvalid's binary_logloss: 0.0105844\n",
      "[120]\tvalid's binary_logloss: 0.0106635\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid's binary_logloss: 0.0105745\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  43.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0887213\n",
      "[20]\tvalid's binary_logloss: 0.046018\n",
      "[30]\tvalid's binary_logloss: 0.0281731\n",
      "[40]\tvalid's binary_logloss: 0.0200342\n",
      "[50]\tvalid's binary_logloss: 0.0160884\n",
      "[60]\tvalid's binary_logloss: 0.0139571\n",
      "[70]\tvalid's binary_logloss: 0.0123012\n",
      "[80]\tvalid's binary_logloss: 0.0119813\n",
      "[90]\tvalid's binary_logloss: 0.0116471\n",
      "[100]\tvalid's binary_logloss: 0.0114315\n",
      "[110]\tvalid's binary_logloss: 0.0112918\n",
      "[120]\tvalid's binary_logloss: 0.0111936\n",
      "[130]\tvalid's binary_logloss: 0.0111672\n",
      "[140]\tvalid's binary_logloss: 0.0111635\n",
      "[150]\tvalid's binary_logloss: 0.0111637\n",
      "Early stopping, best iteration is:\n",
      "[126]\tvalid's binary_logloss: 0.0111584\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  44.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0917201\n",
      "[20]\tvalid's binary_logloss: 0.049831\n",
      "[30]\tvalid's binary_logloss: 0.0309396\n",
      "[40]\tvalid's binary_logloss: 0.0227365\n",
      "[50]\tvalid's binary_logloss: 0.0184573\n",
      "[60]\tvalid's binary_logloss: 0.0163485\n",
      "[70]\tvalid's binary_logloss: 0.0154468\n",
      "[80]\tvalid's binary_logloss: 0.0150323\n",
      "[90]\tvalid's binary_logloss: 0.0148628\n",
      "[100]\tvalid's binary_logloss: 0.0147125\n",
      "[110]\tvalid's binary_logloss: 0.0149581\n",
      "[120]\tvalid's binary_logloss: 0.0150126\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid's binary_logloss: 0.0146756\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.8849557522123894, total=  42.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0885198\n",
      "[20]\tvalid's binary_logloss: 0.0468039\n",
      "[30]\tvalid's binary_logloss: 0.0289959\n",
      "[40]\tvalid's binary_logloss: 0.0199027\n",
      "[50]\tvalid's binary_logloss: 0.0150565\n",
      "[60]\tvalid's binary_logloss: 0.0125777\n",
      "[70]\tvalid's binary_logloss: 0.0114792\n",
      "[80]\tvalid's binary_logloss: 0.0106992\n",
      "[90]\tvalid's binary_logloss: 0.0105407\n",
      "[100]\tvalid's binary_logloss: 0.0102309\n",
      "[110]\tvalid's binary_logloss: 0.0100766\n",
      "[120]\tvalid's binary_logloss: 0.00994738\n",
      "[130]\tvalid's binary_logloss: 0.00992805\n",
      "[140]\tvalid's binary_logloss: 0.00992805\n",
      "[150]\tvalid's binary_logloss: 0.00992805\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid's binary_logloss: 0.00987015\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9137931034482758, total=  44.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0972043\n",
      "[20]\tvalid's binary_logloss: 0.0532205\n",
      "[30]\tvalid's binary_logloss: 0.0342323\n",
      "[40]\tvalid's binary_logloss: 0.0253504\n",
      "[50]\tvalid's binary_logloss: 0.0203567\n",
      "[60]\tvalid's binary_logloss: 0.0174739\n",
      "[70]\tvalid's binary_logloss: 0.0153818\n",
      "[80]\tvalid's binary_logloss: 0.0142184\n",
      "[90]\tvalid's binary_logloss: 0.0134061\n",
      "[100]\tvalid's binary_logloss: 0.0129318\n",
      "[110]\tvalid's binary_logloss: 0.0128083\n",
      "[120]\tvalid's binary_logloss: 0.0128083\n",
      "[130]\tvalid's binary_logloss: 0.0128083\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid's binary_logloss: 0.0128083\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.923076923076923, total=  43.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0963368\n",
      "[20]\tvalid's binary_logloss: 0.0535315\n",
      "[30]\tvalid's binary_logloss: 0.0358251\n",
      "[40]\tvalid's binary_logloss: 0.0267178\n",
      "[50]\tvalid's binary_logloss: 0.0217226\n",
      "[60]\tvalid's binary_logloss: 0.0186848\n",
      "[70]\tvalid's binary_logloss: 0.0166997\n",
      "[80]\tvalid's binary_logloss: 0.0154774\n",
      "[90]\tvalid's binary_logloss: 0.0145658\n",
      "[100]\tvalid's binary_logloss: 0.0141197\n",
      "[110]\tvalid's binary_logloss: 0.0139665\n",
      "[120]\tvalid's binary_logloss: 0.0139665\n",
      "[130]\tvalid's binary_logloss: 0.0139665\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid's binary_logloss: 0.0139649\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.8869565217391304, total=  41.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0994751\n",
      "[20]\tvalid's binary_logloss: 0.0560104\n",
      "[30]\tvalid's binary_logloss: 0.0373047\n",
      "[40]\tvalid's binary_logloss: 0.0276327\n",
      "[50]\tvalid's binary_logloss: 0.0225099\n",
      "[60]\tvalid's binary_logloss: 0.0198617\n",
      "[70]\tvalid's binary_logloss: 0.0178898\n",
      "[80]\tvalid's binary_logloss: 0.016894\n",
      "[90]\tvalid's binary_logloss: 0.0161831\n",
      "[100]\tvalid's binary_logloss: 0.0159072\n",
      "[110]\tvalid's binary_logloss: 0.0157341\n",
      "[120]\tvalid's binary_logloss: 0.0157448\n",
      "[130]\tvalid's binary_logloss: 0.0157448\n",
      "[140]\tvalid's binary_logloss: 0.0157448\n",
      "Early stopping, best iteration is:\n",
      "[114]\tvalid's binary_logloss: 0.0157327\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.8849557522123894, total=  41.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0940511\n",
      "[20]\tvalid's binary_logloss: 0.0533288\n",
      "[30]\tvalid's binary_logloss: 0.0353715\n",
      "[40]\tvalid's binary_logloss: 0.025506\n",
      "[50]\tvalid's binary_logloss: 0.0202613\n",
      "[60]\tvalid's binary_logloss: 0.0172773\n",
      "[70]\tvalid's binary_logloss: 0.0155311\n",
      "[80]\tvalid's binary_logloss: 0.0143423\n",
      "[90]\tvalid's binary_logloss: 0.0135816\n",
      "[100]\tvalid's binary_logloss: 0.0129965\n",
      "[110]\tvalid's binary_logloss: 0.0127825\n",
      "[120]\tvalid's binary_logloss: 0.0127649\n",
      "[130]\tvalid's binary_logloss: 0.0127653\n",
      "[140]\tvalid's binary_logloss: 0.0127655\n",
      "[150]\tvalid's binary_logloss: 0.0127655\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid's binary_logloss: 0.0127649\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.8965517241379309, total=  42.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0929964\n",
      "[20]\tvalid's binary_logloss: 0.050258\n",
      "[30]\tvalid's binary_logloss: 0.0320953\n",
      "[40]\tvalid's binary_logloss: 0.0234356\n",
      "[50]\tvalid's binary_logloss: 0.0187439\n",
      "[60]\tvalid's binary_logloss: 0.0158936\n",
      "[70]\tvalid's binary_logloss: 0.0145914\n",
      "[80]\tvalid's binary_logloss: 0.0134666\n",
      "[90]\tvalid's binary_logloss: 0.0131404\n",
      "[100]\tvalid's binary_logloss: 0.0131404\n",
      "[110]\tvalid's binary_logloss: 0.0131404\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's binary_logloss: 0.0131404\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9322033898305084, total=  41.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0927376\n",
      "[20]\tvalid's binary_logloss: 0.0505692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalid's binary_logloss: 0.0328199\n",
      "[40]\tvalid's binary_logloss: 0.0243927\n",
      "[50]\tvalid's binary_logloss: 0.0199319\n",
      "[60]\tvalid's binary_logloss: 0.0172986\n",
      "[70]\tvalid's binary_logloss: 0.0156947\n",
      "[80]\tvalid's binary_logloss: 0.0147481\n",
      "[90]\tvalid's binary_logloss: 0.014129\n",
      "[100]\tvalid's binary_logloss: 0.0140889\n",
      "[110]\tvalid's binary_logloss: 0.014089\n",
      "[120]\tvalid's binary_logloss: 0.0140891\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid's binary_logloss: 0.0140888\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.923076923076923, total=  40.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0945958\n",
      "[20]\tvalid's binary_logloss: 0.0524931\n",
      "[30]\tvalid's binary_logloss: 0.0347845\n",
      "[40]\tvalid's binary_logloss: 0.0259523\n",
      "[50]\tvalid's binary_logloss: 0.021439\n",
      "[60]\tvalid's binary_logloss: 0.0188937\n",
      "[70]\tvalid's binary_logloss: 0.0174084\n",
      "[80]\tvalid's binary_logloss: 0.0166219\n",
      "[90]\tvalid's binary_logloss: 0.01631\n",
      "[100]\tvalid's binary_logloss: 0.01631\n",
      "[110]\tvalid's binary_logloss: 0.01631\n",
      "[120]\tvalid's binary_logloss: 0.01631\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's binary_logloss: 0.01631\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.8947368421052632, total=  40.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0927203\n",
      "[20]\tvalid's binary_logloss: 0.0515046\n",
      "[30]\tvalid's binary_logloss: 0.0332084\n",
      "[40]\tvalid's binary_logloss: 0.0235106\n",
      "[50]\tvalid's binary_logloss: 0.0185106\n",
      "[60]\tvalid's binary_logloss: 0.0156931\n",
      "[70]\tvalid's binary_logloss: 0.0140772\n",
      "[80]\tvalid's binary_logloss: 0.0130383\n",
      "[90]\tvalid's binary_logloss: 0.0125328\n",
      "[100]\tvalid's binary_logloss: 0.0125415\n",
      "[110]\tvalid's binary_logloss: 0.0125415\n",
      "[120]\tvalid's binary_logloss: 0.0125415\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's binary_logloss: 0.0125328\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.8869565217391304, total=  40.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0907469\n",
      "[20]\tvalid's binary_logloss: 0.0497535\n",
      "[30]\tvalid's binary_logloss: 0.0309537\n",
      "[40]\tvalid's binary_logloss: 0.0228444\n",
      "[50]\tvalid's binary_logloss: 0.0184286\n",
      "[60]\tvalid's binary_logloss: 0.0156789\n",
      "[70]\tvalid's binary_logloss: 0.0140555\n",
      "[80]\tvalid's binary_logloss: 0.0133474\n",
      "[90]\tvalid's binary_logloss: 0.0130514\n",
      "[100]\tvalid's binary_logloss: 0.0130515\n",
      "[110]\tvalid's binary_logloss: 0.0130515\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid's binary_logloss: 0.0130514\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9152542372881356, total=  41.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0921967\n",
      "[20]\tvalid's binary_logloss: 0.0500064\n",
      "[30]\tvalid's binary_logloss: 0.0322213\n",
      "[40]\tvalid's binary_logloss: 0.0239782\n",
      "[50]\tvalid's binary_logloss: 0.0196709\n",
      "[60]\tvalid's binary_logloss: 0.0170768\n",
      "[70]\tvalid's binary_logloss: 0.0155735\n",
      "[80]\tvalid's binary_logloss: 0.0147334\n",
      "[90]\tvalid's binary_logloss: 0.0144171\n",
      "[100]\tvalid's binary_logloss: 0.0143845\n",
      "[110]\tvalid's binary_logloss: 0.0143845\n",
      "[120]\tvalid's binary_logloss: 0.0143845\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid's binary_logloss: 0.0143784\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.8869565217391304, total=  41.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0940942\n",
      "[20]\tvalid's binary_logloss: 0.0516834\n",
      "[30]\tvalid's binary_logloss: 0.0342663\n",
      "[40]\tvalid's binary_logloss: 0.0258426\n",
      "[50]\tvalid's binary_logloss: 0.0214021\n",
      "[60]\tvalid's binary_logloss: 0.0188203\n",
      "[70]\tvalid's binary_logloss: 0.0172914\n",
      "[80]\tvalid's binary_logloss: 0.0167219\n",
      "[90]\tvalid's binary_logloss: 0.0164802\n",
      "[100]\tvalid's binary_logloss: 0.0164805\n",
      "[110]\tvalid's binary_logloss: 0.0164806\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid's binary_logloss: 0.01648\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.8947368421052632, total=  40.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0923976\n",
      "[20]\tvalid's binary_logloss: 0.0505755\n",
      "[30]\tvalid's binary_logloss: 0.0324094\n",
      "[40]\tvalid's binary_logloss: 0.022707\n",
      "[50]\tvalid's binary_logloss: 0.0176931\n",
      "[60]\tvalid's binary_logloss: 0.015294\n",
      "[70]\tvalid's binary_logloss: 0.0138752\n",
      "[80]\tvalid's binary_logloss: 0.0128335\n",
      "[90]\tvalid's binary_logloss: 0.0124402\n",
      "[100]\tvalid's binary_logloss: 0.0124293\n",
      "[110]\tvalid's binary_logloss: 0.0124293\n",
      "[120]\tvalid's binary_logloss: 0.0124293\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid's binary_logloss: 0.0124293\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=5, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  41.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.089118\n",
      "[20]\tvalid's binary_logloss: 0.0465301\n",
      "[30]\tvalid's binary_logloss: 0.0287903\n",
      "[40]\tvalid's binary_logloss: 0.0206926\n",
      "[50]\tvalid's binary_logloss: 0.0164851\n",
      "[60]\tvalid's binary_logloss: 0.0145001\n",
      "[70]\tvalid's binary_logloss: 0.0130773\n",
      "[80]\tvalid's binary_logloss: 0.0123617\n",
      "[90]\tvalid's binary_logloss: 0.0121527\n",
      "[100]\tvalid's binary_logloss: 0.0118123\n",
      "[110]\tvalid's binary_logloss: 0.011684\n",
      "[120]\tvalid's binary_logloss: 0.0116644\n",
      "[130]\tvalid's binary_logloss: 0.0116468\n",
      "[140]\tvalid's binary_logloss: 0.0116361\n",
      "[150]\tvalid's binary_logloss: 0.0116759\n",
      "Early stopping, best iteration is:\n",
      "[123]\tvalid's binary_logloss: 0.0115996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.923076923076923, total=  47.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0905412\n",
      "[20]\tvalid's binary_logloss: 0.0476732\n",
      "[30]\tvalid's binary_logloss: 0.0292013\n",
      "[40]\tvalid's binary_logloss: 0.0208274\n",
      "[50]\tvalid's binary_logloss: 0.0167288\n",
      "[60]\tvalid's binary_logloss: 0.0145599\n",
      "[70]\tvalid's binary_logloss: 0.0131728\n",
      "[80]\tvalid's binary_logloss: 0.0124028\n",
      "[90]\tvalid's binary_logloss: 0.0117249\n",
      "[100]\tvalid's binary_logloss: 0.0114134\n",
      "[110]\tvalid's binary_logloss: 0.0112565\n",
      "[120]\tvalid's binary_logloss: 0.0110646\n",
      "[130]\tvalid's binary_logloss: 0.0109653\n",
      "[140]\tvalid's binary_logloss: 0.0108992\n",
      "[150]\tvalid's binary_logloss: 0.010775\n",
      "[160]\tvalid's binary_logloss: 0.0107469\n",
      "[170]\tvalid's binary_logloss: 0.0107565\n",
      "[180]\tvalid's binary_logloss: 0.0107371\n",
      "[190]\tvalid's binary_logloss: 0.0107133\n",
      "[200]\tvalid's binary_logloss: 0.0107016\n",
      "[210]\tvalid's binary_logloss: 0.0106843\n",
      "[220]\tvalid's binary_logloss: 0.0106421\n",
      "[230]\tvalid's binary_logloss: 0.0106245\n",
      "[240]\tvalid's binary_logloss: 0.010621\n",
      "[250]\tvalid's binary_logloss: 0.0106156\n",
      "[260]\tvalid's binary_logloss: 0.0106171\n",
      "[270]\tvalid's binary_logloss: 0.0106097\n",
      "Early stopping, best iteration is:\n",
      "[244]\tvalid's binary_logloss: 0.010581\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9333333333333333, total=  55.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0923414\n",
      "[20]\tvalid's binary_logloss: 0.0486611\n",
      "[30]\tvalid's binary_logloss: 0.0312534\n",
      "[40]\tvalid's binary_logloss: 0.0228972\n",
      "[50]\tvalid's binary_logloss: 0.0188787\n",
      "[60]\tvalid's binary_logloss: 0.0165823\n",
      "[70]\tvalid's binary_logloss: 0.0153075\n",
      "[80]\tvalid's binary_logloss: 0.014749\n",
      "[90]\tvalid's binary_logloss: 0.014442\n",
      "[100]\tvalid's binary_logloss: 0.0142556\n",
      "[110]\tvalid's binary_logloss: 0.0142567\n",
      "[120]\tvalid's binary_logloss: 0.0142405\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid's binary_logloss: 0.0142128\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.8849557522123894, total=  52.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0908009\n",
      "[20]\tvalid's binary_logloss: 0.0476859\n",
      "[30]\tvalid's binary_logloss: 0.0299542\n",
      "[40]\tvalid's binary_logloss: 0.0215883\n",
      "[50]\tvalid's binary_logloss: 0.0172178\n",
      "[60]\tvalid's binary_logloss: 0.0148326\n",
      "[70]\tvalid's binary_logloss: 0.0133646\n",
      "[80]\tvalid's binary_logloss: 0.0124674\n",
      "[90]\tvalid's binary_logloss: 0.0119612\n",
      "[100]\tvalid's binary_logloss: 0.0116996\n",
      "[110]\tvalid's binary_logloss: 0.0114243\n",
      "[120]\tvalid's binary_logloss: 0.0113361\n",
      "[130]\tvalid's binary_logloss: 0.0112555\n",
      "[140]\tvalid's binary_logloss: 0.0111732\n",
      "[150]\tvalid's binary_logloss: 0.0111736\n",
      "[160]\tvalid's binary_logloss: 0.0111805\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid's binary_logloss: 0.0111495\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.8869565217391304, total=  49.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0837134\n",
      "[20]\tvalid's binary_logloss: 0.0420519\n",
      "[30]\tvalid's binary_logloss: 0.0246768\n",
      "[40]\tvalid's binary_logloss: 0.0174155\n",
      "[50]\tvalid's binary_logloss: 0.0137975\n",
      "[60]\tvalid's binary_logloss: 0.0125085\n",
      "[70]\tvalid's binary_logloss: 0.012259\n",
      "[80]\tvalid's binary_logloss: 0.0122807\n",
      "[90]\tvalid's binary_logloss: 0.0122927\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid's binary_logloss: 0.0121991\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  42.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0849737\n",
      "[20]\tvalid's binary_logloss: 0.0430953\n",
      "[30]\tvalid's binary_logloss: 0.0257685\n",
      "[40]\tvalid's binary_logloss: 0.0177347\n",
      "[50]\tvalid's binary_logloss: 0.0141035\n",
      "[60]\tvalid's binary_logloss: 0.012563\n",
      "[70]\tvalid's binary_logloss: 0.0118777\n",
      "[80]\tvalid's binary_logloss: 0.0116199\n",
      "[90]\tvalid's binary_logloss: 0.0112391\n",
      "[100]\tvalid's binary_logloss: 0.0113364\n",
      "[110]\tvalid's binary_logloss: 0.0112599\n",
      "[120]\tvalid's binary_logloss: 0.0113354\n",
      "[130]\tvalid's binary_logloss: 0.0113805\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid's binary_logloss: 0.0111815\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9411764705882353, total=  45.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0879699\n",
      "[20]\tvalid's binary_logloss: 0.0453921\n",
      "[30]\tvalid's binary_logloss: 0.0278318\n",
      "[40]\tvalid's binary_logloss: 0.0201702\n",
      "[50]\tvalid's binary_logloss: 0.0165578\n",
      "[60]\tvalid's binary_logloss: 0.0152823\n",
      "[70]\tvalid's binary_logloss: 0.0151035\n",
      "[80]\tvalid's binary_logloss: 0.0150943\n",
      "[90]\tvalid's binary_logloss: 0.0152781\n",
      "[100]\tvalid's binary_logloss: 0.0153488\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's binary_logloss: 0.0149445\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  42.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0845326\n",
      "[20]\tvalid's binary_logloss: 0.0430909\n",
      "[30]\tvalid's binary_logloss: 0.0263749\n",
      "[40]\tvalid's binary_logloss: 0.0186653\n",
      "[50]\tvalid's binary_logloss: 0.0149744\n",
      "[60]\tvalid's binary_logloss: 0.0131963\n",
      "[70]\tvalid's binary_logloss: 0.012213\n",
      "[80]\tvalid's binary_logloss: 0.0118145\n",
      "[90]\tvalid's binary_logloss: 0.0114918\n",
      "[100]\tvalid's binary_logloss: 0.0115263\n",
      "[110]\tvalid's binary_logloss: 0.0115157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120]\tvalid's binary_logloss: 0.0117257\n",
      "[130]\tvalid's binary_logloss: 0.0117293\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid's binary_logloss: 0.0113975\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  44.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0834715\n",
      "[20]\tvalid's binary_logloss: 0.0417063\n",
      "[30]\tvalid's binary_logloss: 0.0244881\n",
      "[40]\tvalid's binary_logloss: 0.0177994\n",
      "[50]\tvalid's binary_logloss: 0.0141877\n",
      "[60]\tvalid's binary_logloss: 0.0130341\n",
      "[70]\tvalid's binary_logloss: 0.0128096\n",
      "[80]\tvalid's binary_logloss: 0.0129257\n",
      "[90]\tvalid's binary_logloss: 0.0132139\n",
      "[100]\tvalid's binary_logloss: 0.0136487\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid's binary_logloss: 0.0128096\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  46.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0844293\n",
      "[20]\tvalid's binary_logloss: 0.0424448\n",
      "[30]\tvalid's binary_logloss: 0.0254481\n",
      "[40]\tvalid's binary_logloss: 0.0177285\n",
      "[50]\tvalid's binary_logloss: 0.0143115\n",
      "[60]\tvalid's binary_logloss: 0.0127744\n",
      "[70]\tvalid's binary_logloss: 0.0118974\n",
      "[80]\tvalid's binary_logloss: 0.0120883\n",
      "[90]\tvalid's binary_logloss: 0.0120666\n",
      "[100]\tvalid's binary_logloss: 0.0122993\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's binary_logloss: 0.0118817\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.905982905982906, total=  46.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0868168\n",
      "[20]\tvalid's binary_logloss: 0.0452452\n",
      "[30]\tvalid's binary_logloss: 0.0279654\n",
      "[40]\tvalid's binary_logloss: 0.0203459\n",
      "[50]\tvalid's binary_logloss: 0.0169977\n",
      "[60]\tvalid's binary_logloss: 0.0157603\n",
      "[70]\tvalid's binary_logloss: 0.0157491\n",
      "[80]\tvalid's binary_logloss: 0.0162491\n",
      "[90]\tvalid's binary_logloss: 0.0167511\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's binary_logloss: 0.0154272\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  46.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0832241\n",
      "[20]\tvalid's binary_logloss: 0.0428957\n",
      "[30]\tvalid's binary_logloss: 0.0263857\n",
      "[40]\tvalid's binary_logloss: 0.0186772\n",
      "[50]\tvalid's binary_logloss: 0.0147854\n",
      "[60]\tvalid's binary_logloss: 0.013313\n",
      "[70]\tvalid's binary_logloss: 0.0125145\n",
      "[80]\tvalid's binary_logloss: 0.0119281\n",
      "[90]\tvalid's binary_logloss: 0.0120871\n",
      "[100]\tvalid's binary_logloss: 0.0121043\n",
      "[110]\tvalid's binary_logloss: 0.0127316\n",
      "[120]\tvalid's binary_logloss: 0.0130072\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's binary_logloss: 0.0118194\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.8965517241379309, total=  49.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0895017\n",
      "[20]\tvalid's binary_logloss: 0.0468828\n",
      "[30]\tvalid's binary_logloss: 0.0289624\n",
      "[40]\tvalid's binary_logloss: 0.0208599\n",
      "[50]\tvalid's binary_logloss: 0.0167901\n",
      "[60]\tvalid's binary_logloss: 0.0144905\n",
      "[70]\tvalid's binary_logloss: 0.0133122\n",
      "[80]\tvalid's binary_logloss: 0.0125234\n",
      "[90]\tvalid's binary_logloss: 0.0121318\n",
      "[100]\tvalid's binary_logloss: 0.0116658\n",
      "[110]\tvalid's binary_logloss: 0.0115074\n",
      "[120]\tvalid's binary_logloss: 0.0113263\n",
      "[130]\tvalid's binary_logloss: 0.0111896\n",
      "[140]\tvalid's binary_logloss: 0.0111483\n",
      "[150]\tvalid's binary_logloss: 0.0111203\n",
      "[160]\tvalid's binary_logloss: 0.0110874\n",
      "[170]\tvalid's binary_logloss: 0.011101\n",
      "[180]\tvalid's binary_logloss: 0.0110481\n",
      "[190]\tvalid's binary_logloss: 0.0110583\n",
      "[200]\tvalid's binary_logloss: 0.0110485\n",
      "[210]\tvalid's binary_logloss: 0.011064\n",
      "Early stopping, best iteration is:\n",
      "[188]\tvalid's binary_logloss: 0.0109916\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.923076923076923, total=  49.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0909198\n",
      "[20]\tvalid's binary_logloss: 0.0477165\n",
      "[30]\tvalid's binary_logloss: 0.0292872\n",
      "[40]\tvalid's binary_logloss: 0.0210568\n",
      "[50]\tvalid's binary_logloss: 0.0167835\n",
      "[60]\tvalid's binary_logloss: 0.0145695\n",
      "[70]\tvalid's binary_logloss: 0.0132926\n",
      "[80]\tvalid's binary_logloss: 0.0124092\n",
      "[90]\tvalid's binary_logloss: 0.0118215\n",
      "[100]\tvalid's binary_logloss: 0.0115227\n",
      "[110]\tvalid's binary_logloss: 0.0112605\n",
      "[120]\tvalid's binary_logloss: 0.0111153\n",
      "[130]\tvalid's binary_logloss: 0.0109413\n",
      "[140]\tvalid's binary_logloss: 0.01093\n",
      "[150]\tvalid's binary_logloss: 0.0108383\n",
      "[160]\tvalid's binary_logloss: 0.0108004\n",
      "[170]\tvalid's binary_logloss: 0.0107396\n",
      "[180]\tvalid's binary_logloss: 0.0106689\n",
      "[190]\tvalid's binary_logloss: 0.0106213\n",
      "[200]\tvalid's binary_logloss: 0.0106156\n",
      "[210]\tvalid's binary_logloss: 0.0105958\n",
      "[220]\tvalid's binary_logloss: 0.0105621\n",
      "[230]\tvalid's binary_logloss: 0.0105604\n",
      "[240]\tvalid's binary_logloss: 0.0105447\n",
      "[250]\tvalid's binary_logloss: 0.0105127\n",
      "[260]\tvalid's binary_logloss: 0.0105127\n",
      "[270]\tvalid's binary_logloss: 0.0105127\n",
      "Early stopping, best iteration is:\n",
      "[248]\tvalid's binary_logloss: 0.0105099\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  47.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0926221\n",
      "[20]\tvalid's binary_logloss: 0.0490795\n",
      "[30]\tvalid's binary_logloss: 0.0317025\n",
      "[40]\tvalid's binary_logloss: 0.0233903\n",
      "[50]\tvalid's binary_logloss: 0.0193081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60]\tvalid's binary_logloss: 0.0171661\n",
      "[70]\tvalid's binary_logloss: 0.0158644\n",
      "[80]\tvalid's binary_logloss: 0.0152075\n",
      "[90]\tvalid's binary_logloss: 0.014913\n",
      "[100]\tvalid's binary_logloss: 0.0145757\n",
      "[110]\tvalid's binary_logloss: 0.0145939\n",
      "[120]\tvalid's binary_logloss: 0.0144382\n",
      "[130]\tvalid's binary_logloss: 0.0143724\n",
      "[140]\tvalid's binary_logloss: 0.0144224\n",
      "[150]\tvalid's binary_logloss: 0.0144928\n",
      "[160]\tvalid's binary_logloss: 0.0144941\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid's binary_logloss: 0.0143572\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.8849557522123894, total=  44.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0893209\n",
      "[20]\tvalid's binary_logloss: 0.0476667\n",
      "[30]\tvalid's binary_logloss: 0.0299314\n",
      "[40]\tvalid's binary_logloss: 0.0209397\n",
      "[50]\tvalid's binary_logloss: 0.0167675\n",
      "[60]\tvalid's binary_logloss: 0.0147518\n",
      "[70]\tvalid's binary_logloss: 0.0133965\n",
      "[80]\tvalid's binary_logloss: 0.0124777\n",
      "[90]\tvalid's binary_logloss: 0.0118418\n",
      "[100]\tvalid's binary_logloss: 0.0114831\n",
      "[110]\tvalid's binary_logloss: 0.0112183\n",
      "[120]\tvalid's binary_logloss: 0.0110438\n",
      "[130]\tvalid's binary_logloss: 0.0108771\n",
      "[140]\tvalid's binary_logloss: 0.0107864\n",
      "[150]\tvalid's binary_logloss: 0.0106383\n",
      "[160]\tvalid's binary_logloss: 0.0105724\n",
      "[170]\tvalid's binary_logloss: 0.0104811\n",
      "[180]\tvalid's binary_logloss: 0.0104213\n",
      "[190]\tvalid's binary_logloss: 0.0103569\n",
      "[200]\tvalid's binary_logloss: 0.0102864\n",
      "[210]\tvalid's binary_logloss: 0.0102652\n",
      "[220]\tvalid's binary_logloss: 0.0102334\n",
      "[230]\tvalid's binary_logloss: 0.0101968\n",
      "[240]\tvalid's binary_logloss: 0.0102113\n",
      "[250]\tvalid's binary_logloss: 0.0102072\n",
      "[260]\tvalid's binary_logloss: 0.0102146\n",
      "Early stopping, best iteration is:\n",
      "[233]\tvalid's binary_logloss: 0.0101871\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9137931034482758, total=  47.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0842072\n",
      "[20]\tvalid's binary_logloss: 0.042009\n",
      "[30]\tvalid's binary_logloss: 0.0251595\n",
      "[40]\tvalid's binary_logloss: 0.0177125\n",
      "[50]\tvalid's binary_logloss: 0.0143193\n",
      "[60]\tvalid's binary_logloss: 0.0127963\n",
      "[70]\tvalid's binary_logloss: 0.0120826\n",
      "[80]\tvalid's binary_logloss: 0.0118012\n",
      "[90]\tvalid's binary_logloss: 0.0114421\n",
      "[100]\tvalid's binary_logloss: 0.0112528\n",
      "[110]\tvalid's binary_logloss: 0.0112986\n",
      "[120]\tvalid's binary_logloss: 0.011269\n",
      "[130]\tvalid's binary_logloss: 0.0112826\n",
      "Early stopping, best iteration is:\n",
      "[103]\tvalid's binary_logloss: 0.0111499\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  42.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0856073\n",
      "[20]\tvalid's binary_logloss: 0.0439746\n",
      "[30]\tvalid's binary_logloss: 0.0264989\n",
      "[40]\tvalid's binary_logloss: 0.0189474\n",
      "[50]\tvalid's binary_logloss: 0.0152369\n",
      "[60]\tvalid's binary_logloss: 0.0133689\n",
      "[70]\tvalid's binary_logloss: 0.0123993\n",
      "[80]\tvalid's binary_logloss: 0.0119655\n",
      "[90]\tvalid's binary_logloss: 0.0116468\n",
      "[100]\tvalid's binary_logloss: 0.0115266\n",
      "[110]\tvalid's binary_logloss: 0.0114033\n",
      "[120]\tvalid's binary_logloss: 0.0113186\n",
      "[130]\tvalid's binary_logloss: 0.011246\n",
      "[140]\tvalid's binary_logloss: 0.0112514\n",
      "[150]\tvalid's binary_logloss: 0.0112514\n",
      "[160]\tvalid's binary_logloss: 0.0112514\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid's binary_logloss: 0.011246\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9152542372881356, total=  42.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0879067\n",
      "[20]\tvalid's binary_logloss: 0.0448043\n",
      "[30]\tvalid's binary_logloss: 0.0278625\n",
      "[40]\tvalid's binary_logloss: 0.020418\n",
      "[50]\tvalid's binary_logloss: 0.0171633\n",
      "[60]\tvalid's binary_logloss: 0.0156697\n",
      "[70]\tvalid's binary_logloss: 0.0149287\n",
      "[80]\tvalid's binary_logloss: 0.0147985\n",
      "[90]\tvalid's binary_logloss: 0.0147491\n",
      "[100]\tvalid's binary_logloss: 0.014687\n",
      "[110]\tvalid's binary_logloss: 0.0147198\n",
      "[120]\tvalid's binary_logloss: 0.0147442\n",
      "[130]\tvalid's binary_logloss: 0.0148063\n",
      "[140]\tvalid's binary_logloss: 0.0148199\n",
      "Early stopping, best iteration is:\n",
      "[117]\tvalid's binary_logloss: 0.0146661\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9043478260869565, total=  42.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.084638\n",
      "[20]\tvalid's binary_logloss: 0.0431312\n",
      "[30]\tvalid's binary_logloss: 0.0265418\n",
      "[40]\tvalid's binary_logloss: 0.0188266\n",
      "[50]\tvalid's binary_logloss: 0.0152281\n",
      "[60]\tvalid's binary_logloss: 0.0130431\n",
      "[70]\tvalid's binary_logloss: 0.0122244\n",
      "[80]\tvalid's binary_logloss: 0.0114733\n",
      "[90]\tvalid's binary_logloss: 0.0111535\n",
      "[100]\tvalid's binary_logloss: 0.0109658\n",
      "[110]\tvalid's binary_logloss: 0.010855\n",
      "[120]\tvalid's binary_logloss: 0.0107756\n",
      "[130]\tvalid's binary_logloss: 0.0107253\n",
      "[140]\tvalid's binary_logloss: 0.0107253\n",
      "[150]\tvalid's binary_logloss: 0.0107253\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid's binary_logloss: 0.0106984\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  42.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0838315\n",
      "[20]\tvalid's binary_logloss: 0.042557\n",
      "[30]\tvalid's binary_logloss: 0.0249705\n",
      "[40]\tvalid's binary_logloss: 0.0174178\n",
      "[50]\tvalid's binary_logloss: 0.0139446\n",
      "[60]\tvalid's binary_logloss: 0.0125953\n",
      "[70]\tvalid's binary_logloss: 0.0117043\n",
      "[80]\tvalid's binary_logloss: 0.0113518\n",
      "[90]\tvalid's binary_logloss: 0.0111686\n",
      "[100]\tvalid's binary_logloss: 0.0108909\n",
      "[110]\tvalid's binary_logloss: 0.01111\n",
      "[120]\tvalid's binary_logloss: 0.0111299\n",
      "[130]\tvalid's binary_logloss: 0.0111299\n",
      "Early stopping, best iteration is:\n",
      "[101]\tvalid's binary_logloss: 0.0108752\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  42.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0849533\n",
      "[20]\tvalid's binary_logloss: 0.0435513\n",
      "[30]\tvalid's binary_logloss: 0.0260779\n",
      "[40]\tvalid's binary_logloss: 0.0186028\n",
      "[50]\tvalid's binary_logloss: 0.0146363\n",
      "[60]\tvalid's binary_logloss: 0.0129619\n",
      "[70]\tvalid's binary_logloss: 0.0119355\n",
      "[80]\tvalid's binary_logloss: 0.0115934\n",
      "[90]\tvalid's binary_logloss: 0.011314\n",
      "[100]\tvalid's binary_logloss: 0.0111198\n",
      "[110]\tvalid's binary_logloss: 0.0109898\n",
      "[120]\tvalid's binary_logloss: 0.0109662\n",
      "[130]\tvalid's binary_logloss: 0.0109662\n",
      "[140]\tvalid's binary_logloss: 0.0109662\n",
      "Early stopping, best iteration is:\n",
      "[113]\tvalid's binary_logloss: 0.0109625\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  42.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0879459\n",
      "[20]\tvalid's binary_logloss: 0.0456743\n",
      "[30]\tvalid's binary_logloss: 0.0284976\n",
      "[40]\tvalid's binary_logloss: 0.0211891\n",
      "[50]\tvalid's binary_logloss: 0.0177802\n",
      "[60]\tvalid's binary_logloss: 0.0166988\n",
      "[70]\tvalid's binary_logloss: 0.0162643\n",
      "[80]\tvalid's binary_logloss: 0.0160829\n",
      "[90]\tvalid's binary_logloss: 0.0160154\n",
      "[100]\tvalid's binary_logloss: 0.0161271\n",
      "[110]\tvalid's binary_logloss: 0.0162963\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid's binary_logloss: 0.0160104\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.8947368421052632, total=  41.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0840822\n",
      "[20]\tvalid's binary_logloss: 0.0431884\n",
      "[30]\tvalid's binary_logloss: 0.026913\n",
      "[40]\tvalid's binary_logloss: 0.0189989\n",
      "[50]\tvalid's binary_logloss: 0.0149637\n",
      "[60]\tvalid's binary_logloss: 0.0130823\n",
      "[70]\tvalid's binary_logloss: 0.0120535\n",
      "[80]\tvalid's binary_logloss: 0.0114917\n",
      "[90]\tvalid's binary_logloss: 0.0112362\n",
      "[100]\tvalid's binary_logloss: 0.0112079\n",
      "[110]\tvalid's binary_logloss: 0.0110748\n",
      "[120]\tvalid's binary_logloss: 0.0110712\n",
      "[130]\tvalid's binary_logloss: 0.0110712\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid's binary_logloss: 0.011056\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.8965517241379309, total=  41.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0920988\n",
      "[20]\tvalid's binary_logloss: 0.0494416\n",
      "[30]\tvalid's binary_logloss: 0.0320775\n",
      "[40]\tvalid's binary_logloss: 0.0235975\n",
      "[50]\tvalid's binary_logloss: 0.0187356\n",
      "[60]\tvalid's binary_logloss: 0.016012\n",
      "[70]\tvalid's binary_logloss: 0.0143483\n",
      "[80]\tvalid's binary_logloss: 0.0132597\n",
      "[90]\tvalid's binary_logloss: 0.0127943\n",
      "[100]\tvalid's binary_logloss: 0.0125575\n",
      "[110]\tvalid's binary_logloss: 0.0125568\n",
      "[120]\tvalid's binary_logloss: 0.0125568\n",
      "[130]\tvalid's binary_logloss: 0.0125568\n",
      "Early stopping, best iteration is:\n",
      "[102]\tvalid's binary_logloss: 0.0125568\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9137931034482758, total=  40.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0936929\n",
      "[20]\tvalid's binary_logloss: 0.0512704\n",
      "[30]\tvalid's binary_logloss: 0.0329636\n",
      "[40]\tvalid's binary_logloss: 0.0243384\n",
      "[50]\tvalid's binary_logloss: 0.0198861\n",
      "[60]\tvalid's binary_logloss: 0.0170384\n",
      "[70]\tvalid's binary_logloss: 0.0154478\n",
      "[80]\tvalid's binary_logloss: 0.0143562\n",
      "[90]\tvalid's binary_logloss: 0.0137016\n",
      "[100]\tvalid's binary_logloss: 0.0133228\n",
      "[110]\tvalid's binary_logloss: 0.0133198\n",
      "[120]\tvalid's binary_logloss: 0.0133198\n",
      "Early stopping, best iteration is:\n",
      "[99]\tvalid's binary_logloss: 0.0133119\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  40.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0951908\n",
      "[20]\tvalid's binary_logloss: 0.0524025\n",
      "[30]\tvalid's binary_logloss: 0.0353567\n",
      "[40]\tvalid's binary_logloss: 0.0270435\n",
      "[50]\tvalid's binary_logloss: 0.0224333\n",
      "[60]\tvalid's binary_logloss: 0.0196862\n",
      "[70]\tvalid's binary_logloss: 0.017855\n",
      "[80]\tvalid's binary_logloss: 0.0169771\n",
      "[90]\tvalid's binary_logloss: 0.0165469\n",
      "[100]\tvalid's binary_logloss: 0.0164143\n",
      "[110]\tvalid's binary_logloss: 0.0164127\n",
      "[120]\tvalid's binary_logloss: 0.0164127\n",
      "Early stopping, best iteration is:\n",
      "[99]\tvalid's binary_logloss: 0.01641\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.8771929824561403, total=  40.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0919924\n",
      "[20]\tvalid's binary_logloss: 0.0517817\n",
      "[30]\tvalid's binary_logloss: 0.0336756\n",
      "[40]\tvalid's binary_logloss: 0.0241727\n",
      "[50]\tvalid's binary_logloss: 0.0193574\n",
      "[60]\tvalid's binary_logloss: 0.0164406\n",
      "[70]\tvalid's binary_logloss: 0.014844\n",
      "[80]\tvalid's binary_logloss: 0.013768\n",
      "[90]\tvalid's binary_logloss: 0.013006\n",
      "[100]\tvalid's binary_logloss: 0.0127354\n",
      "[110]\tvalid's binary_logloss: 0.0127354\n",
      "[120]\tvalid's binary_logloss: 0.0127354\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid's binary_logloss: 0.0127354\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  40.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0886835\n",
      "[20]\tvalid's binary_logloss: 0.0471729\n",
      "[30]\tvalid's binary_logloss: 0.0301582\n",
      "[40]\tvalid's binary_logloss: 0.0219045\n",
      "[50]\tvalid's binary_logloss: 0.0177717\n",
      "[60]\tvalid's binary_logloss: 0.0154669\n",
      "[70]\tvalid's binary_logloss: 0.0141533\n",
      "[80]\tvalid's binary_logloss: 0.0135907\n",
      "[90]\tvalid's binary_logloss: 0.0135003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid's binary_logloss: 0.0135003\n",
      "[110]\tvalid's binary_logloss: 0.0135003\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid's binary_logloss: 0.0135003\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  41.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0887689\n",
      "[20]\tvalid's binary_logloss: 0.0477637\n",
      "[30]\tvalid's binary_logloss: 0.0306707\n",
      "[40]\tvalid's binary_logloss: 0.022432\n",
      "[50]\tvalid's binary_logloss: 0.0181257\n",
      "[60]\tvalid's binary_logloss: 0.0158996\n",
      "[70]\tvalid's binary_logloss: 0.0146343\n",
      "[80]\tvalid's binary_logloss: 0.0139438\n",
      "[90]\tvalid's binary_logloss: 0.013798\n",
      "[100]\tvalid's binary_logloss: 0.013798\n",
      "[110]\tvalid's binary_logloss: 0.013798\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's binary_logloss: 0.013798\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9152542372881356, total=  39.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0921132\n",
      "[20]\tvalid's binary_logloss: 0.0496675\n",
      "[30]\tvalid's binary_logloss: 0.0328503\n",
      "[40]\tvalid's binary_logloss: 0.0246126\n",
      "[50]\tvalid's binary_logloss: 0.0202605\n",
      "[60]\tvalid's binary_logloss: 0.0180729\n",
      "[70]\tvalid's binary_logloss: 0.0167635\n",
      "[80]\tvalid's binary_logloss: 0.016447\n",
      "[90]\tvalid's binary_logloss: 0.0163518\n",
      "[100]\tvalid's binary_logloss: 0.0163518\n",
      "[110]\tvalid's binary_logloss: 0.0163518\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's binary_logloss: 0.0163518\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.8869565217391304, total=  39.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0888367\n",
      "[20]\tvalid's binary_logloss: 0.0485687\n",
      "[30]\tvalid's binary_logloss: 0.031043\n",
      "[40]\tvalid's binary_logloss: 0.0230204\n",
      "[50]\tvalid's binary_logloss: 0.01889\n",
      "[60]\tvalid's binary_logloss: 0.0162939\n",
      "[70]\tvalid's binary_logloss: 0.0148146\n",
      "[80]\tvalid's binary_logloss: 0.0140761\n",
      "[90]\tvalid's binary_logloss: 0.0140107\n",
      "[100]\tvalid's binary_logloss: 0.0140107\n",
      "[110]\tvalid's binary_logloss: 0.0140107\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's binary_logloss: 0.0140107\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.8869565217391304, total=  39.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0875268\n",
      "[20]\tvalid's binary_logloss: 0.0465538\n",
      "[30]\tvalid's binary_logloss: 0.0297112\n",
      "[40]\tvalid's binary_logloss: 0.021534\n",
      "[50]\tvalid's binary_logloss: 0.0172454\n",
      "[60]\tvalid's binary_logloss: 0.0151091\n",
      "[70]\tvalid's binary_logloss: 0.0138771\n",
      "[80]\tvalid's binary_logloss: 0.0132467\n",
      "[90]\tvalid's binary_logloss: 0.0132513\n",
      "[100]\tvalid's binary_logloss: 0.0132515\n",
      "[110]\tvalid's binary_logloss: 0.0132516\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's binary_logloss: 0.0132467\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9137931034482758, total=  40.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0899668\n",
      "[20]\tvalid's binary_logloss: 0.0478572\n",
      "[30]\tvalid's binary_logloss: 0.0308534\n",
      "[40]\tvalid's binary_logloss: 0.0226537\n",
      "[50]\tvalid's binary_logloss: 0.018144\n",
      "[60]\tvalid's binary_logloss: 0.0159567\n",
      "[70]\tvalid's binary_logloss: 0.0147391\n",
      "[80]\tvalid's binary_logloss: 0.0140867\n",
      "[90]\tvalid's binary_logloss: 0.0140331\n",
      "[100]\tvalid's binary_logloss: 0.014031\n",
      "[110]\tvalid's binary_logloss: 0.01403\n",
      "[120]\tvalid's binary_logloss: 0.0140294\n",
      "[130]\tvalid's binary_logloss: 0.0140292\n",
      "[140]\tvalid's binary_logloss: 0.0140291\n",
      "[150]\tvalid's binary_logloss: 0.0140291\n",
      "[160]\tvalid's binary_logloss: 0.0140291\n",
      "[170]\tvalid's binary_logloss: 0.0140291\n",
      "[180]\tvalid's binary_logloss: 0.0140291\n",
      "[190]\tvalid's binary_logloss: 0.014029\n",
      "[200]\tvalid's binary_logloss: 0.014029\n",
      "[210]\tvalid's binary_logloss: 0.014029\n",
      "[220]\tvalid's binary_logloss: 0.014029\n",
      "[230]\tvalid's binary_logloss: 0.014029\n",
      "[240]\tvalid's binary_logloss: 0.014029\n",
      "[250]\tvalid's binary_logloss: 0.014029\n",
      "[260]\tvalid's binary_logloss: 0.014029\n",
      "[270]\tvalid's binary_logloss: 0.014029\n",
      "Early stopping, best iteration is:\n",
      "[241]\tvalid's binary_logloss: 0.014029\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9137931034482758, total=  43.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.091305\n",
      "[20]\tvalid's binary_logloss: 0.0492385\n",
      "[30]\tvalid's binary_logloss: 0.0320496\n",
      "[40]\tvalid's binary_logloss: 0.0241505\n",
      "[50]\tvalid's binary_logloss: 0.020099\n",
      "[60]\tvalid's binary_logloss: 0.0180573\n",
      "[70]\tvalid's binary_logloss: 0.0166324\n",
      "[80]\tvalid's binary_logloss: 0.0163265\n",
      "[90]\tvalid's binary_logloss: 0.0162928\n",
      "[100]\tvalid's binary_logloss: 0.0162928\n",
      "[110]\tvalid's binary_logloss: 0.0162928\n",
      "[120]\tvalid's binary_logloss: 0.0162928\n",
      "[130]\tvalid's binary_logloss: 0.0162928\n",
      "[140]\tvalid's binary_logloss: 0.0162928\n",
      "[150]\tvalid's binary_logloss: 0.0162928\n",
      "[160]\tvalid's binary_logloss: 0.0162928\n",
      "[170]\tvalid's binary_logloss: 0.0162928\n",
      "[180]\tvalid's binary_logloss: 0.0162928\n",
      "[190]\tvalid's binary_logloss: 0.0162928\n",
      "[200]\tvalid's binary_logloss: 0.0162928\n",
      "[210]\tvalid's binary_logloss: 0.0162928\n",
      "[220]\tvalid's binary_logloss: 0.0162928\n",
      "Early stopping, best iteration is:\n",
      "[199]\tvalid's binary_logloss: 0.0162928\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.8849557522123894, total=  42.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0888166\n",
      "[20]\tvalid's binary_logloss: 0.047153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalid's binary_logloss: 0.0302641\n",
      "[40]\tvalid's binary_logloss: 0.021978\n",
      "[50]\tvalid's binary_logloss: 0.017818\n",
      "[60]\tvalid's binary_logloss: 0.01534\n",
      "[70]\tvalid's binary_logloss: 0.0140562\n",
      "[80]\tvalid's binary_logloss: 0.013508\n",
      "[90]\tvalid's binary_logloss: 0.0134456\n",
      "[100]\tvalid's binary_logloss: 0.0134456\n",
      "[110]\tvalid's binary_logloss: 0.0134456\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid's binary_logloss: 0.0134456\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=1, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.8965517241379309, total=  39.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.089118\n",
      "[20]\tvalid's binary_logloss: 0.0465301\n",
      "[30]\tvalid's binary_logloss: 0.0287903\n",
      "[40]\tvalid's binary_logloss: 0.0206926\n",
      "[50]\tvalid's binary_logloss: 0.0164851\n",
      "[60]\tvalid's binary_logloss: 0.0145001\n",
      "[70]\tvalid's binary_logloss: 0.0130773\n",
      "[80]\tvalid's binary_logloss: 0.0123617\n",
      "[90]\tvalid's binary_logloss: 0.0121527\n",
      "[100]\tvalid's binary_logloss: 0.0118123\n",
      "[110]\tvalid's binary_logloss: 0.011684\n",
      "[120]\tvalid's binary_logloss: 0.0116644\n",
      "[130]\tvalid's binary_logloss: 0.0116468\n",
      "[140]\tvalid's binary_logloss: 0.0116361\n",
      "[150]\tvalid's binary_logloss: 0.0116759\n",
      "Early stopping, best iteration is:\n",
      "[123]\tvalid's binary_logloss: 0.0115996\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.923076923076923, total=  45.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0905412\n",
      "[20]\tvalid's binary_logloss: 0.0476732\n",
      "[30]\tvalid's binary_logloss: 0.0292013\n",
      "[40]\tvalid's binary_logloss: 0.0208274\n",
      "[50]\tvalid's binary_logloss: 0.0167288\n",
      "[60]\tvalid's binary_logloss: 0.0145599\n",
      "[70]\tvalid's binary_logloss: 0.0131728\n",
      "[80]\tvalid's binary_logloss: 0.0124028\n",
      "[90]\tvalid's binary_logloss: 0.0117249\n",
      "[100]\tvalid's binary_logloss: 0.0114134\n",
      "[110]\tvalid's binary_logloss: 0.0112565\n",
      "[120]\tvalid's binary_logloss: 0.0110646\n",
      "[130]\tvalid's binary_logloss: 0.0109653\n",
      "[140]\tvalid's binary_logloss: 0.0108992\n",
      "[150]\tvalid's binary_logloss: 0.010775\n",
      "[160]\tvalid's binary_logloss: 0.0107469\n",
      "[170]\tvalid's binary_logloss: 0.0107565\n",
      "[180]\tvalid's binary_logloss: 0.0107371\n",
      "[190]\tvalid's binary_logloss: 0.0107133\n",
      "[200]\tvalid's binary_logloss: 0.0107016\n",
      "[210]\tvalid's binary_logloss: 0.0106843\n",
      "[220]\tvalid's binary_logloss: 0.0106421\n",
      "[230]\tvalid's binary_logloss: 0.0106245\n",
      "[240]\tvalid's binary_logloss: 0.010621\n",
      "[250]\tvalid's binary_logloss: 0.0106156\n",
      "[260]\tvalid's binary_logloss: 0.0106171\n",
      "[270]\tvalid's binary_logloss: 0.0106097\n",
      "Early stopping, best iteration is:\n",
      "[244]\tvalid's binary_logloss: 0.010581\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9333333333333333, total=  59.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0923414\n",
      "[20]\tvalid's binary_logloss: 0.0486611\n",
      "[30]\tvalid's binary_logloss: 0.0312534\n",
      "[40]\tvalid's binary_logloss: 0.0228972\n",
      "[50]\tvalid's binary_logloss: 0.0188787\n",
      "[60]\tvalid's binary_logloss: 0.0165823\n",
      "[70]\tvalid's binary_logloss: 0.0153075\n",
      "[80]\tvalid's binary_logloss: 0.014749\n",
      "[90]\tvalid's binary_logloss: 0.014442\n",
      "[100]\tvalid's binary_logloss: 0.0142556\n",
      "[110]\tvalid's binary_logloss: 0.0142567\n",
      "[120]\tvalid's binary_logloss: 0.0142405\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid's binary_logloss: 0.0142128\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.8849557522123894, total=  43.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0908009\n",
      "[20]\tvalid's binary_logloss: 0.0476859\n",
      "[30]\tvalid's binary_logloss: 0.0299542\n",
      "[40]\tvalid's binary_logloss: 0.0215883\n",
      "[50]\tvalid's binary_logloss: 0.0172178\n",
      "[60]\tvalid's binary_logloss: 0.0148326\n",
      "[70]\tvalid's binary_logloss: 0.0133646\n",
      "[80]\tvalid's binary_logloss: 0.0124674\n",
      "[90]\tvalid's binary_logloss: 0.0119612\n",
      "[100]\tvalid's binary_logloss: 0.0116996\n",
      "[110]\tvalid's binary_logloss: 0.0114243\n",
      "[120]\tvalid's binary_logloss: 0.0113361\n",
      "[130]\tvalid's binary_logloss: 0.0112555\n",
      "[140]\tvalid's binary_logloss: 0.0111732\n",
      "[150]\tvalid's binary_logloss: 0.0111736\n",
      "[160]\tvalid's binary_logloss: 0.0111805\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid's binary_logloss: 0.0111495\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.8869565217391304, total=  46.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0837134\n",
      "[20]\tvalid's binary_logloss: 0.0420519\n",
      "[30]\tvalid's binary_logloss: 0.0246768\n",
      "[40]\tvalid's binary_logloss: 0.0174155\n",
      "[50]\tvalid's binary_logloss: 0.0137975\n",
      "[60]\tvalid's binary_logloss: 0.0125085\n",
      "[70]\tvalid's binary_logloss: 0.012259\n",
      "[80]\tvalid's binary_logloss: 0.0122807\n",
      "[90]\tvalid's binary_logloss: 0.0122927\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid's binary_logloss: 0.0121991\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  42.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0849737\n",
      "[20]\tvalid's binary_logloss: 0.0430953\n",
      "[30]\tvalid's binary_logloss: 0.0257685\n",
      "[40]\tvalid's binary_logloss: 0.0177347\n",
      "[50]\tvalid's binary_logloss: 0.0141035\n",
      "[60]\tvalid's binary_logloss: 0.012563\n",
      "[70]\tvalid's binary_logloss: 0.0118777\n",
      "[80]\tvalid's binary_logloss: 0.0116199\n",
      "[90]\tvalid's binary_logloss: 0.0112391\n",
      "[100]\tvalid's binary_logloss: 0.0113364\n",
      "[110]\tvalid's binary_logloss: 0.0112599\n",
      "[120]\tvalid's binary_logloss: 0.0113354\n",
      "[130]\tvalid's binary_logloss: 0.0113805\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid's binary_logloss: 0.0111815\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9411764705882353, total=  44.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0879699\n",
      "[20]\tvalid's binary_logloss: 0.0453921\n",
      "[30]\tvalid's binary_logloss: 0.0278318\n",
      "[40]\tvalid's binary_logloss: 0.0201702\n",
      "[50]\tvalid's binary_logloss: 0.0165578\n",
      "[60]\tvalid's binary_logloss: 0.0152823\n",
      "[70]\tvalid's binary_logloss: 0.0151035\n",
      "[80]\tvalid's binary_logloss: 0.0150943\n",
      "[90]\tvalid's binary_logloss: 0.0152781\n",
      "[100]\tvalid's binary_logloss: 0.0153488\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's binary_logloss: 0.0149445\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  42.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0845326\n",
      "[20]\tvalid's binary_logloss: 0.0430909\n",
      "[30]\tvalid's binary_logloss: 0.0263749\n",
      "[40]\tvalid's binary_logloss: 0.0186653\n",
      "[50]\tvalid's binary_logloss: 0.0149744\n",
      "[60]\tvalid's binary_logloss: 0.0131963\n",
      "[70]\tvalid's binary_logloss: 0.012213\n",
      "[80]\tvalid's binary_logloss: 0.0118145\n",
      "[90]\tvalid's binary_logloss: 0.0114918\n",
      "[100]\tvalid's binary_logloss: 0.0115263\n",
      "[110]\tvalid's binary_logloss: 0.0115157\n",
      "[120]\tvalid's binary_logloss: 0.0117257\n",
      "[130]\tvalid's binary_logloss: 0.0117293\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid's binary_logloss: 0.0113975\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  45.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0834715\n",
      "[20]\tvalid's binary_logloss: 0.0417063\n",
      "[30]\tvalid's binary_logloss: 0.0244881\n",
      "[40]\tvalid's binary_logloss: 0.0177994\n",
      "[50]\tvalid's binary_logloss: 0.0141877\n",
      "[60]\tvalid's binary_logloss: 0.0130341\n",
      "[70]\tvalid's binary_logloss: 0.0128096\n",
      "[80]\tvalid's binary_logloss: 0.0129257\n",
      "[90]\tvalid's binary_logloss: 0.0132139\n",
      "[100]\tvalid's binary_logloss: 0.0136487\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid's binary_logloss: 0.0128096\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  46.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0844293\n",
      "[20]\tvalid's binary_logloss: 0.0424448\n",
      "[30]\tvalid's binary_logloss: 0.0254481\n",
      "[40]\tvalid's binary_logloss: 0.0177285\n",
      "[50]\tvalid's binary_logloss: 0.0143115\n",
      "[60]\tvalid's binary_logloss: 0.0127744\n",
      "[70]\tvalid's binary_logloss: 0.0118974\n",
      "[80]\tvalid's binary_logloss: 0.0120883\n",
      "[90]\tvalid's binary_logloss: 0.0120666\n",
      "[100]\tvalid's binary_logloss: 0.0122993\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's binary_logloss: 0.0118817\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.905982905982906, total=  46.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0868168\n",
      "[20]\tvalid's binary_logloss: 0.0452452\n",
      "[30]\tvalid's binary_logloss: 0.0279654\n",
      "[40]\tvalid's binary_logloss: 0.0203459\n",
      "[50]\tvalid's binary_logloss: 0.0169977\n",
      "[60]\tvalid's binary_logloss: 0.0157603\n",
      "[70]\tvalid's binary_logloss: 0.0157491\n",
      "[80]\tvalid's binary_logloss: 0.0162491\n",
      "[90]\tvalid's binary_logloss: 0.0167511\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's binary_logloss: 0.0154272\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  46.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0832241\n",
      "[20]\tvalid's binary_logloss: 0.0428957\n",
      "[30]\tvalid's binary_logloss: 0.0263857\n",
      "[40]\tvalid's binary_logloss: 0.0186772\n",
      "[50]\tvalid's binary_logloss: 0.0147854\n",
      "[60]\tvalid's binary_logloss: 0.013313\n",
      "[70]\tvalid's binary_logloss: 0.0125145\n",
      "[80]\tvalid's binary_logloss: 0.0119281\n",
      "[90]\tvalid's binary_logloss: 0.0120871\n",
      "[100]\tvalid's binary_logloss: 0.0121043\n",
      "[110]\tvalid's binary_logloss: 0.0127316\n",
      "[120]\tvalid's binary_logloss: 0.0130072\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's binary_logloss: 0.0118194\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.8965517241379309, total=  48.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0895017\n",
      "[20]\tvalid's binary_logloss: 0.0468828\n",
      "[30]\tvalid's binary_logloss: 0.0289624\n",
      "[40]\tvalid's binary_logloss: 0.0208599\n",
      "[50]\tvalid's binary_logloss: 0.0167901\n",
      "[60]\tvalid's binary_logloss: 0.0144905\n",
      "[70]\tvalid's binary_logloss: 0.0133122\n",
      "[80]\tvalid's binary_logloss: 0.0125234\n",
      "[90]\tvalid's binary_logloss: 0.0121318\n",
      "[100]\tvalid's binary_logloss: 0.0116658\n",
      "[110]\tvalid's binary_logloss: 0.0115074\n",
      "[120]\tvalid's binary_logloss: 0.0113263\n",
      "[130]\tvalid's binary_logloss: 0.0111896\n",
      "[140]\tvalid's binary_logloss: 0.0111483\n",
      "[150]\tvalid's binary_logloss: 0.0111203\n",
      "[160]\tvalid's binary_logloss: 0.0110874\n",
      "[170]\tvalid's binary_logloss: 0.011101\n",
      "[180]\tvalid's binary_logloss: 0.0110481\n",
      "[190]\tvalid's binary_logloss: 0.0110583\n",
      "[200]\tvalid's binary_logloss: 0.0110485\n",
      "[210]\tvalid's binary_logloss: 0.011064\n",
      "Early stopping, best iteration is:\n",
      "[188]\tvalid's binary_logloss: 0.0109916\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.923076923076923, total=  46.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0909198\n",
      "[20]\tvalid's binary_logloss: 0.0477165\n",
      "[30]\tvalid's binary_logloss: 0.0292872\n",
      "[40]\tvalid's binary_logloss: 0.0210568\n",
      "[50]\tvalid's binary_logloss: 0.0167835\n",
      "[60]\tvalid's binary_logloss: 0.0145695\n",
      "[70]\tvalid's binary_logloss: 0.0132926\n",
      "[80]\tvalid's binary_logloss: 0.0124092\n",
      "[90]\tvalid's binary_logloss: 0.0118215\n",
      "[100]\tvalid's binary_logloss: 0.0115227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110]\tvalid's binary_logloss: 0.0112605\n",
      "[120]\tvalid's binary_logloss: 0.0111153\n",
      "[130]\tvalid's binary_logloss: 0.0109413\n",
      "[140]\tvalid's binary_logloss: 0.01093\n",
      "[150]\tvalid's binary_logloss: 0.0108383\n",
      "[160]\tvalid's binary_logloss: 0.0108004\n",
      "[170]\tvalid's binary_logloss: 0.0107396\n",
      "[180]\tvalid's binary_logloss: 0.0106689\n",
      "[190]\tvalid's binary_logloss: 0.0106213\n",
      "[200]\tvalid's binary_logloss: 0.0106156\n",
      "[210]\tvalid's binary_logloss: 0.0105958\n",
      "[220]\tvalid's binary_logloss: 0.0105621\n",
      "[230]\tvalid's binary_logloss: 0.0105604\n",
      "[240]\tvalid's binary_logloss: 0.0105447\n",
      "[250]\tvalid's binary_logloss: 0.0105127\n",
      "[260]\tvalid's binary_logloss: 0.0105127\n",
      "[270]\tvalid's binary_logloss: 0.0105127\n",
      "Early stopping, best iteration is:\n",
      "[248]\tvalid's binary_logloss: 0.0105099\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  47.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0926221\n",
      "[20]\tvalid's binary_logloss: 0.0490795\n",
      "[30]\tvalid's binary_logloss: 0.0317025\n",
      "[40]\tvalid's binary_logloss: 0.0233903\n",
      "[50]\tvalid's binary_logloss: 0.0193081\n",
      "[60]\tvalid's binary_logloss: 0.0171661\n",
      "[70]\tvalid's binary_logloss: 0.0158644\n",
      "[80]\tvalid's binary_logloss: 0.0152075\n",
      "[90]\tvalid's binary_logloss: 0.014913\n",
      "[100]\tvalid's binary_logloss: 0.0145757\n",
      "[110]\tvalid's binary_logloss: 0.0145939\n",
      "[120]\tvalid's binary_logloss: 0.0144382\n",
      "[130]\tvalid's binary_logloss: 0.0143724\n",
      "[140]\tvalid's binary_logloss: 0.0144224\n",
      "[150]\tvalid's binary_logloss: 0.0144928\n",
      "[160]\tvalid's binary_logloss: 0.0144941\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid's binary_logloss: 0.0143572\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.8849557522123894, total=  44.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0893209\n",
      "[20]\tvalid's binary_logloss: 0.0476667\n",
      "[30]\tvalid's binary_logloss: 0.0299314\n",
      "[40]\tvalid's binary_logloss: 0.0209397\n",
      "[50]\tvalid's binary_logloss: 0.0167675\n",
      "[60]\tvalid's binary_logloss: 0.0147518\n",
      "[70]\tvalid's binary_logloss: 0.0133965\n",
      "[80]\tvalid's binary_logloss: 0.0124777\n",
      "[90]\tvalid's binary_logloss: 0.0118418\n",
      "[100]\tvalid's binary_logloss: 0.0114831\n",
      "[110]\tvalid's binary_logloss: 0.0112183\n",
      "[120]\tvalid's binary_logloss: 0.0110438\n",
      "[130]\tvalid's binary_logloss: 0.0108771\n",
      "[140]\tvalid's binary_logloss: 0.0107864\n",
      "[150]\tvalid's binary_logloss: 0.0106383\n",
      "[160]\tvalid's binary_logloss: 0.0105724\n",
      "[170]\tvalid's binary_logloss: 0.0104811\n",
      "[180]\tvalid's binary_logloss: 0.0104213\n",
      "[190]\tvalid's binary_logloss: 0.0103569\n",
      "[200]\tvalid's binary_logloss: 0.0102864\n",
      "[210]\tvalid's binary_logloss: 0.0102652\n",
      "[220]\tvalid's binary_logloss: 0.0102334\n",
      "[230]\tvalid's binary_logloss: 0.0101968\n",
      "[240]\tvalid's binary_logloss: 0.0102113\n",
      "[250]\tvalid's binary_logloss: 0.0102072\n",
      "[260]\tvalid's binary_logloss: 0.0102146\n",
      "Early stopping, best iteration is:\n",
      "[233]\tvalid's binary_logloss: 0.0101871\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9137931034482758, total=  47.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0842072\n",
      "[20]\tvalid's binary_logloss: 0.042009\n",
      "[30]\tvalid's binary_logloss: 0.0251595\n",
      "[40]\tvalid's binary_logloss: 0.0177125\n",
      "[50]\tvalid's binary_logloss: 0.0143193\n",
      "[60]\tvalid's binary_logloss: 0.0127963\n",
      "[70]\tvalid's binary_logloss: 0.0120826\n",
      "[80]\tvalid's binary_logloss: 0.0118012\n",
      "[90]\tvalid's binary_logloss: 0.0114421\n",
      "[100]\tvalid's binary_logloss: 0.0112528\n",
      "[110]\tvalid's binary_logloss: 0.0112986\n",
      "[120]\tvalid's binary_logloss: 0.011269\n",
      "[130]\tvalid's binary_logloss: 0.0112826\n",
      "Early stopping, best iteration is:\n",
      "[103]\tvalid's binary_logloss: 0.0111499\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  42.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0856073\n",
      "[20]\tvalid's binary_logloss: 0.0439746\n",
      "[30]\tvalid's binary_logloss: 0.0264989\n",
      "[40]\tvalid's binary_logloss: 0.0189474\n",
      "[50]\tvalid's binary_logloss: 0.0152369\n",
      "[60]\tvalid's binary_logloss: 0.0133689\n",
      "[70]\tvalid's binary_logloss: 0.0123993\n",
      "[80]\tvalid's binary_logloss: 0.0119655\n",
      "[90]\tvalid's binary_logloss: 0.0116468\n",
      "[100]\tvalid's binary_logloss: 0.0115266\n",
      "[110]\tvalid's binary_logloss: 0.0114033\n",
      "[120]\tvalid's binary_logloss: 0.0113186\n",
      "[130]\tvalid's binary_logloss: 0.011246\n",
      "[140]\tvalid's binary_logloss: 0.0112514\n",
      "[150]\tvalid's binary_logloss: 0.0112514\n",
      "[160]\tvalid's binary_logloss: 0.0112514\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid's binary_logloss: 0.011246\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9152542372881356, total=  42.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0879067\n",
      "[20]\tvalid's binary_logloss: 0.0448043\n",
      "[30]\tvalid's binary_logloss: 0.0278625\n",
      "[40]\tvalid's binary_logloss: 0.020418\n",
      "[50]\tvalid's binary_logloss: 0.0171633\n",
      "[60]\tvalid's binary_logloss: 0.0156697\n",
      "[70]\tvalid's binary_logloss: 0.0149287\n",
      "[80]\tvalid's binary_logloss: 0.0147985\n",
      "[90]\tvalid's binary_logloss: 0.0147491\n",
      "[100]\tvalid's binary_logloss: 0.014687\n",
      "[110]\tvalid's binary_logloss: 0.0147198\n",
      "[120]\tvalid's binary_logloss: 0.0147442\n",
      "[130]\tvalid's binary_logloss: 0.0148063\n",
      "[140]\tvalid's binary_logloss: 0.0148199\n",
      "Early stopping, best iteration is:\n",
      "[117]\tvalid's binary_logloss: 0.0146661\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9043478260869565, total=  42.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.084638\n",
      "[20]\tvalid's binary_logloss: 0.0431312\n",
      "[30]\tvalid's binary_logloss: 0.0265418\n",
      "[40]\tvalid's binary_logloss: 0.0188266\n",
      "[50]\tvalid's binary_logloss: 0.0152281\n",
      "[60]\tvalid's binary_logloss: 0.0130431\n",
      "[70]\tvalid's binary_logloss: 0.0122244\n",
      "[80]\tvalid's binary_logloss: 0.0114733\n",
      "[90]\tvalid's binary_logloss: 0.0111535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid's binary_logloss: 0.0109658\n",
      "[110]\tvalid's binary_logloss: 0.010855\n",
      "[120]\tvalid's binary_logloss: 0.0107756\n",
      "[130]\tvalid's binary_logloss: 0.0107253\n",
      "[140]\tvalid's binary_logloss: 0.0107253\n",
      "[150]\tvalid's binary_logloss: 0.0107253\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid's binary_logloss: 0.0106984\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  42.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0838315\n",
      "[20]\tvalid's binary_logloss: 0.042557\n",
      "[30]\tvalid's binary_logloss: 0.0249705\n",
      "[40]\tvalid's binary_logloss: 0.0174178\n",
      "[50]\tvalid's binary_logloss: 0.0139446\n",
      "[60]\tvalid's binary_logloss: 0.0125953\n",
      "[70]\tvalid's binary_logloss: 0.0117043\n",
      "[80]\tvalid's binary_logloss: 0.0113518\n",
      "[90]\tvalid's binary_logloss: 0.0111686\n",
      "[100]\tvalid's binary_logloss: 0.0108909\n",
      "[110]\tvalid's binary_logloss: 0.01111\n",
      "[120]\tvalid's binary_logloss: 0.0111299\n",
      "[130]\tvalid's binary_logloss: 0.0111299\n",
      "Early stopping, best iteration is:\n",
      "[101]\tvalid's binary_logloss: 0.0108752\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  42.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0849533\n",
      "[20]\tvalid's binary_logloss: 0.0435513\n",
      "[30]\tvalid's binary_logloss: 0.0260779\n",
      "[40]\tvalid's binary_logloss: 0.0186028\n",
      "[50]\tvalid's binary_logloss: 0.0146363\n",
      "[60]\tvalid's binary_logloss: 0.0129619\n",
      "[70]\tvalid's binary_logloss: 0.0119355\n",
      "[80]\tvalid's binary_logloss: 0.0115934\n",
      "[90]\tvalid's binary_logloss: 0.011314\n",
      "[100]\tvalid's binary_logloss: 0.0111198\n",
      "[110]\tvalid's binary_logloss: 0.0109898\n",
      "[120]\tvalid's binary_logloss: 0.0109662\n",
      "[130]\tvalid's binary_logloss: 0.0109662\n",
      "[140]\tvalid's binary_logloss: 0.0109662\n",
      "Early stopping, best iteration is:\n",
      "[113]\tvalid's binary_logloss: 0.0109625\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  42.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0879459\n",
      "[20]\tvalid's binary_logloss: 0.0456743\n",
      "[30]\tvalid's binary_logloss: 0.0284976\n",
      "[40]\tvalid's binary_logloss: 0.0211891\n",
      "[50]\tvalid's binary_logloss: 0.0177802\n",
      "[60]\tvalid's binary_logloss: 0.0166988\n",
      "[70]\tvalid's binary_logloss: 0.0162643\n",
      "[80]\tvalid's binary_logloss: 0.0160829\n",
      "[90]\tvalid's binary_logloss: 0.0160154\n",
      "[100]\tvalid's binary_logloss: 0.0161271\n",
      "[110]\tvalid's binary_logloss: 0.0162963\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid's binary_logloss: 0.0160104\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.8947368421052632, total=  41.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0840822\n",
      "[20]\tvalid's binary_logloss: 0.0431884\n",
      "[30]\tvalid's binary_logloss: 0.026913\n",
      "[40]\tvalid's binary_logloss: 0.0189989\n",
      "[50]\tvalid's binary_logloss: 0.0149637\n",
      "[60]\tvalid's binary_logloss: 0.0130823\n",
      "[70]\tvalid's binary_logloss: 0.0120535\n",
      "[80]\tvalid's binary_logloss: 0.0114917\n",
      "[90]\tvalid's binary_logloss: 0.0112362\n",
      "[100]\tvalid's binary_logloss: 0.0112079\n",
      "[110]\tvalid's binary_logloss: 0.0110748\n",
      "[120]\tvalid's binary_logloss: 0.0110712\n",
      "[130]\tvalid's binary_logloss: 0.0110712\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid's binary_logloss: 0.011056\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.8965517241379309, total=  41.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0920988\n",
      "[20]\tvalid's binary_logloss: 0.0494416\n",
      "[30]\tvalid's binary_logloss: 0.0320775\n",
      "[40]\tvalid's binary_logloss: 0.0235975\n",
      "[50]\tvalid's binary_logloss: 0.0187356\n",
      "[60]\tvalid's binary_logloss: 0.016012\n",
      "[70]\tvalid's binary_logloss: 0.0143483\n",
      "[80]\tvalid's binary_logloss: 0.0132597\n",
      "[90]\tvalid's binary_logloss: 0.0127943\n",
      "[100]\tvalid's binary_logloss: 0.0125575\n",
      "[110]\tvalid's binary_logloss: 0.0125568\n",
      "[120]\tvalid's binary_logloss: 0.0125568\n",
      "[130]\tvalid's binary_logloss: 0.0125568\n",
      "[140]\tvalid's binary_logloss: 0.0125568\n",
      "[150]\tvalid's binary_logloss: 0.0125568\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid's binary_logloss: 0.0125568\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9137931034482758, total=  40.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0936929\n",
      "[20]\tvalid's binary_logloss: 0.0512704\n",
      "[30]\tvalid's binary_logloss: 0.0329636\n",
      "[40]\tvalid's binary_logloss: 0.0243384\n",
      "[50]\tvalid's binary_logloss: 0.0198861\n",
      "[60]\tvalid's binary_logloss: 0.0170384\n",
      "[70]\tvalid's binary_logloss: 0.0154478\n",
      "[80]\tvalid's binary_logloss: 0.0143562\n",
      "[90]\tvalid's binary_logloss: 0.0137016\n",
      "[100]\tvalid's binary_logloss: 0.0133228\n",
      "[110]\tvalid's binary_logloss: 0.0133198\n",
      "[120]\tvalid's binary_logloss: 0.0133198\n",
      "Early stopping, best iteration is:\n",
      "[99]\tvalid's binary_logloss: 0.0133119\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  40.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0951908\n",
      "[20]\tvalid's binary_logloss: 0.0524025\n",
      "[30]\tvalid's binary_logloss: 0.0353567\n",
      "[40]\tvalid's binary_logloss: 0.0270435\n",
      "[50]\tvalid's binary_logloss: 0.0224333\n",
      "[60]\tvalid's binary_logloss: 0.0196862\n",
      "[70]\tvalid's binary_logloss: 0.017855\n",
      "[80]\tvalid's binary_logloss: 0.0169771\n",
      "[90]\tvalid's binary_logloss: 0.0165469\n",
      "[100]\tvalid's binary_logloss: 0.0164143\n",
      "[110]\tvalid's binary_logloss: 0.0164127\n",
      "[120]\tvalid's binary_logloss: 0.0164127\n",
      "Early stopping, best iteration is:\n",
      "[99]\tvalid's binary_logloss: 0.01641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.8771929824561403, total=  40.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0919924\n",
      "[20]\tvalid's binary_logloss: 0.0517817\n",
      "[30]\tvalid's binary_logloss: 0.0336756\n",
      "[40]\tvalid's binary_logloss: 0.0241727\n",
      "[50]\tvalid's binary_logloss: 0.0193574\n",
      "[60]\tvalid's binary_logloss: 0.0164406\n",
      "[70]\tvalid's binary_logloss: 0.014844\n",
      "[80]\tvalid's binary_logloss: 0.013768\n",
      "[90]\tvalid's binary_logloss: 0.013006\n",
      "[100]\tvalid's binary_logloss: 0.0127354\n",
      "[110]\tvalid's binary_logloss: 0.0127354\n",
      "[120]\tvalid's binary_logloss: 0.0127354\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid's binary_logloss: 0.0127354\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  40.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0886835\n",
      "[20]\tvalid's binary_logloss: 0.0471729\n",
      "[30]\tvalid's binary_logloss: 0.0301582\n",
      "[40]\tvalid's binary_logloss: 0.0219045\n",
      "[50]\tvalid's binary_logloss: 0.0177717\n",
      "[60]\tvalid's binary_logloss: 0.0154669\n",
      "[70]\tvalid's binary_logloss: 0.0141533\n",
      "[80]\tvalid's binary_logloss: 0.0135907\n",
      "[90]\tvalid's binary_logloss: 0.0135003\n",
      "[100]\tvalid's binary_logloss: 0.0135003\n",
      "[110]\tvalid's binary_logloss: 0.0135003\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid's binary_logloss: 0.0135003\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  40.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0887689\n",
      "[20]\tvalid's binary_logloss: 0.0477637\n",
      "[30]\tvalid's binary_logloss: 0.0306707\n",
      "[40]\tvalid's binary_logloss: 0.022432\n",
      "[50]\tvalid's binary_logloss: 0.0181257\n",
      "[60]\tvalid's binary_logloss: 0.0158996\n",
      "[70]\tvalid's binary_logloss: 0.0146343\n",
      "[80]\tvalid's binary_logloss: 0.0139438\n",
      "[90]\tvalid's binary_logloss: 0.013798\n",
      "[100]\tvalid's binary_logloss: 0.013798\n",
      "[110]\tvalid's binary_logloss: 0.013798\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's binary_logloss: 0.013798\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9152542372881356, total=  39.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0921132\n",
      "[20]\tvalid's binary_logloss: 0.0496675\n",
      "[30]\tvalid's binary_logloss: 0.0328503\n",
      "[40]\tvalid's binary_logloss: 0.0246126\n",
      "[50]\tvalid's binary_logloss: 0.0202605\n",
      "[60]\tvalid's binary_logloss: 0.0180729\n",
      "[70]\tvalid's binary_logloss: 0.0167635\n",
      "[80]\tvalid's binary_logloss: 0.016447\n",
      "[90]\tvalid's binary_logloss: 0.0163518\n",
      "[100]\tvalid's binary_logloss: 0.0163518\n",
      "[110]\tvalid's binary_logloss: 0.0163518\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's binary_logloss: 0.0163518\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.8869565217391304, total=  39.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0888367\n",
      "[20]\tvalid's binary_logloss: 0.0485687\n",
      "[30]\tvalid's binary_logloss: 0.031043\n",
      "[40]\tvalid's binary_logloss: 0.0230204\n",
      "[50]\tvalid's binary_logloss: 0.01889\n",
      "[60]\tvalid's binary_logloss: 0.0162939\n",
      "[70]\tvalid's binary_logloss: 0.0148146\n",
      "[80]\tvalid's binary_logloss: 0.0140761\n",
      "[90]\tvalid's binary_logloss: 0.0140107\n",
      "[100]\tvalid's binary_logloss: 0.0140107\n",
      "[110]\tvalid's binary_logloss: 0.0140107\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid's binary_logloss: 0.0140107\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.8869565217391304, total=  40.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0875268\n",
      "[20]\tvalid's binary_logloss: 0.0465538\n",
      "[30]\tvalid's binary_logloss: 0.0297112\n",
      "[40]\tvalid's binary_logloss: 0.021534\n",
      "[50]\tvalid's binary_logloss: 0.0172454\n",
      "[60]\tvalid's binary_logloss: 0.0151091\n",
      "[70]\tvalid's binary_logloss: 0.0138771\n",
      "[80]\tvalid's binary_logloss: 0.0132467\n",
      "[90]\tvalid's binary_logloss: 0.0132513\n",
      "[100]\tvalid's binary_logloss: 0.0132515\n",
      "[110]\tvalid's binary_logloss: 0.0132516\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's binary_logloss: 0.0132467\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9137931034482758, total=  41.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0899668\n",
      "[20]\tvalid's binary_logloss: 0.0478572\n",
      "[30]\tvalid's binary_logloss: 0.0308534\n",
      "[40]\tvalid's binary_logloss: 0.0226537\n",
      "[50]\tvalid's binary_logloss: 0.018144\n",
      "[60]\tvalid's binary_logloss: 0.0159567\n",
      "[70]\tvalid's binary_logloss: 0.0147391\n",
      "[80]\tvalid's binary_logloss: 0.0140867\n",
      "[90]\tvalid's binary_logloss: 0.0140331\n",
      "[100]\tvalid's binary_logloss: 0.014031\n",
      "[110]\tvalid's binary_logloss: 0.01403\n",
      "[120]\tvalid's binary_logloss: 0.0140294\n",
      "[130]\tvalid's binary_logloss: 0.0140292\n",
      "[140]\tvalid's binary_logloss: 0.0140291\n",
      "[150]\tvalid's binary_logloss: 0.0140291\n",
      "[160]\tvalid's binary_logloss: 0.0140291\n",
      "[170]\tvalid's binary_logloss: 0.0140291\n",
      "[180]\tvalid's binary_logloss: 0.0140291\n",
      "[190]\tvalid's binary_logloss: 0.014029\n",
      "[200]\tvalid's binary_logloss: 0.014029\n",
      "[210]\tvalid's binary_logloss: 0.014029\n",
      "[220]\tvalid's binary_logloss: 0.014029\n",
      "[230]\tvalid's binary_logloss: 0.014029\n",
      "[240]\tvalid's binary_logloss: 0.014029\n",
      "[250]\tvalid's binary_logloss: 0.014029\n",
      "[260]\tvalid's binary_logloss: 0.014029\n",
      "[270]\tvalid's binary_logloss: 0.014029\n",
      "Early stopping, best iteration is:\n",
      "[242]\tvalid's binary_logloss: 0.014029\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9137931034482758, total=  43.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.091305\n",
      "[20]\tvalid's binary_logloss: 0.0492385\n",
      "[30]\tvalid's binary_logloss: 0.0320496\n",
      "[40]\tvalid's binary_logloss: 0.0241505\n",
      "[50]\tvalid's binary_logloss: 0.020099\n",
      "[60]\tvalid's binary_logloss: 0.0180573\n",
      "[70]\tvalid's binary_logloss: 0.0166324\n",
      "[80]\tvalid's binary_logloss: 0.0163265\n",
      "[90]\tvalid's binary_logloss: 0.0162928\n",
      "[100]\tvalid's binary_logloss: 0.0162928\n",
      "[110]\tvalid's binary_logloss: 0.0162928\n",
      "[120]\tvalid's binary_logloss: 0.0162928\n",
      "[130]\tvalid's binary_logloss: 0.0162928\n",
      "[140]\tvalid's binary_logloss: 0.0162928\n",
      "[150]\tvalid's binary_logloss: 0.0162928\n",
      "[160]\tvalid's binary_logloss: 0.0162928\n",
      "[170]\tvalid's binary_logloss: 0.0162928\n",
      "[180]\tvalid's binary_logloss: 0.0162928\n",
      "[190]\tvalid's binary_logloss: 0.0162928\n",
      "[200]\tvalid's binary_logloss: 0.0162928\n",
      "[210]\tvalid's binary_logloss: 0.0162928\n",
      "[220]\tvalid's binary_logloss: 0.0162928\n",
      "Early stopping, best iteration is:\n",
      "[199]\tvalid's binary_logloss: 0.0162928\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.8849557522123894, total=  42.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0888166\n",
      "[20]\tvalid's binary_logloss: 0.047153\n",
      "[30]\tvalid's binary_logloss: 0.0302641\n",
      "[40]\tvalid's binary_logloss: 0.021978\n",
      "[50]\tvalid's binary_logloss: 0.017818\n",
      "[60]\tvalid's binary_logloss: 0.01534\n",
      "[70]\tvalid's binary_logloss: 0.0140562\n",
      "[80]\tvalid's binary_logloss: 0.013508\n",
      "[90]\tvalid's binary_logloss: 0.0134456\n",
      "[100]\tvalid's binary_logloss: 0.0134456\n",
      "[110]\tvalid's binary_logloss: 0.0134456\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid's binary_logloss: 0.0134456\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=3, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.8965517241379309, total=  41.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.089118\n",
      "[20]\tvalid's binary_logloss: 0.0465301\n",
      "[30]\tvalid's binary_logloss: 0.0287903\n",
      "[40]\tvalid's binary_logloss: 0.0206926\n",
      "[50]\tvalid's binary_logloss: 0.0164851\n",
      "[60]\tvalid's binary_logloss: 0.0145001\n",
      "[70]\tvalid's binary_logloss: 0.0130773\n",
      "[80]\tvalid's binary_logloss: 0.0123617\n",
      "[90]\tvalid's binary_logloss: 0.0121527\n",
      "[100]\tvalid's binary_logloss: 0.0118123\n",
      "[110]\tvalid's binary_logloss: 0.011684\n",
      "[120]\tvalid's binary_logloss: 0.0116644\n",
      "[130]\tvalid's binary_logloss: 0.0116468\n",
      "[140]\tvalid's binary_logloss: 0.0116361\n",
      "[150]\tvalid's binary_logloss: 0.0116759\n",
      "Early stopping, best iteration is:\n",
      "[123]\tvalid's binary_logloss: 0.0115996\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.923076923076923, total=  45.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0905412\n",
      "[20]\tvalid's binary_logloss: 0.0476732\n",
      "[30]\tvalid's binary_logloss: 0.0292013\n",
      "[40]\tvalid's binary_logloss: 0.0208274\n",
      "[50]\tvalid's binary_logloss: 0.0167288\n",
      "[60]\tvalid's binary_logloss: 0.0145599\n",
      "[70]\tvalid's binary_logloss: 0.0131728\n",
      "[80]\tvalid's binary_logloss: 0.0124028\n",
      "[90]\tvalid's binary_logloss: 0.0117249\n",
      "[100]\tvalid's binary_logloss: 0.0114134\n",
      "[110]\tvalid's binary_logloss: 0.0112565\n",
      "[120]\tvalid's binary_logloss: 0.0110646\n",
      "[130]\tvalid's binary_logloss: 0.0109653\n",
      "[140]\tvalid's binary_logloss: 0.0108992\n",
      "[150]\tvalid's binary_logloss: 0.010775\n",
      "[160]\tvalid's binary_logloss: 0.0107469\n",
      "[170]\tvalid's binary_logloss: 0.0107565\n",
      "[180]\tvalid's binary_logloss: 0.0107371\n",
      "[190]\tvalid's binary_logloss: 0.0107133\n",
      "[200]\tvalid's binary_logloss: 0.0107016\n",
      "[210]\tvalid's binary_logloss: 0.0106843\n",
      "[220]\tvalid's binary_logloss: 0.0106421\n",
      "[230]\tvalid's binary_logloss: 0.0106245\n",
      "[240]\tvalid's binary_logloss: 0.010621\n",
      "[250]\tvalid's binary_logloss: 0.0106156\n",
      "[260]\tvalid's binary_logloss: 0.0106171\n",
      "[270]\tvalid's binary_logloss: 0.0106097\n",
      "Early stopping, best iteration is:\n",
      "[244]\tvalid's binary_logloss: 0.010581\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.9333333333333333, total=  54.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0923414\n",
      "[20]\tvalid's binary_logloss: 0.0486611\n",
      "[30]\tvalid's binary_logloss: 0.0312534\n",
      "[40]\tvalid's binary_logloss: 0.0228972\n",
      "[50]\tvalid's binary_logloss: 0.0188787\n",
      "[60]\tvalid's binary_logloss: 0.0165823\n",
      "[70]\tvalid's binary_logloss: 0.0153075\n",
      "[80]\tvalid's binary_logloss: 0.014749\n",
      "[90]\tvalid's binary_logloss: 0.014442\n",
      "[100]\tvalid's binary_logloss: 0.0142556\n",
      "[110]\tvalid's binary_logloss: 0.0142567\n",
      "[120]\tvalid's binary_logloss: 0.0142405\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid's binary_logloss: 0.0142128\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.8849557522123894, total=  45.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0908009\n",
      "[20]\tvalid's binary_logloss: 0.0476859\n",
      "[30]\tvalid's binary_logloss: 0.0299542\n",
      "[40]\tvalid's binary_logloss: 0.0215883\n",
      "[50]\tvalid's binary_logloss: 0.0172178\n",
      "[60]\tvalid's binary_logloss: 0.0148326\n",
      "[70]\tvalid's binary_logloss: 0.0133646\n",
      "[80]\tvalid's binary_logloss: 0.0124674\n",
      "[90]\tvalid's binary_logloss: 0.0119612\n",
      "[100]\tvalid's binary_logloss: 0.0116996\n",
      "[110]\tvalid's binary_logloss: 0.0114243\n",
      "[120]\tvalid's binary_logloss: 0.0113361\n",
      "[130]\tvalid's binary_logloss: 0.0112555\n",
      "[140]\tvalid's binary_logloss: 0.0111732\n",
      "[150]\tvalid's binary_logloss: 0.0111736\n",
      "[160]\tvalid's binary_logloss: 0.0111805\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid's binary_logloss: 0.0111495\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=1, subsamples=0.8, score=0.8869565217391304, total=  48.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0837134\n",
      "[20]\tvalid's binary_logloss: 0.0420519\n",
      "[30]\tvalid's binary_logloss: 0.0246768\n",
      "[40]\tvalid's binary_logloss: 0.0174155\n",
      "[50]\tvalid's binary_logloss: 0.0137975\n",
      "[60]\tvalid's binary_logloss: 0.0125085\n",
      "[70]\tvalid's binary_logloss: 0.012259\n",
      "[80]\tvalid's binary_logloss: 0.0122807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90]\tvalid's binary_logloss: 0.0122927\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid's binary_logloss: 0.0121991\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  42.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0849737\n",
      "[20]\tvalid's binary_logloss: 0.0430953\n",
      "[30]\tvalid's binary_logloss: 0.0257685\n",
      "[40]\tvalid's binary_logloss: 0.0177347\n",
      "[50]\tvalid's binary_logloss: 0.0141035\n",
      "[60]\tvalid's binary_logloss: 0.012563\n",
      "[70]\tvalid's binary_logloss: 0.0118777\n",
      "[80]\tvalid's binary_logloss: 0.0116199\n",
      "[90]\tvalid's binary_logloss: 0.0112391\n",
      "[100]\tvalid's binary_logloss: 0.0113364\n",
      "[110]\tvalid's binary_logloss: 0.0112599\n",
      "[120]\tvalid's binary_logloss: 0.0113354\n",
      "[130]\tvalid's binary_logloss: 0.0113805\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid's binary_logloss: 0.0111815\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9411764705882353, total=  44.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0879699\n",
      "[20]\tvalid's binary_logloss: 0.0453921\n",
      "[30]\tvalid's binary_logloss: 0.0278318\n",
      "[40]\tvalid's binary_logloss: 0.0201702\n",
      "[50]\tvalid's binary_logloss: 0.0165578\n",
      "[60]\tvalid's binary_logloss: 0.0152823\n",
      "[70]\tvalid's binary_logloss: 0.0151035\n",
      "[80]\tvalid's binary_logloss: 0.0150943\n",
      "[90]\tvalid's binary_logloss: 0.0152781\n",
      "[100]\tvalid's binary_logloss: 0.0153488\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's binary_logloss: 0.0149445\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  42.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0845326\n",
      "[20]\tvalid's binary_logloss: 0.0430909\n",
      "[30]\tvalid's binary_logloss: 0.0263749\n",
      "[40]\tvalid's binary_logloss: 0.0186653\n",
      "[50]\tvalid's binary_logloss: 0.0149744\n",
      "[60]\tvalid's binary_logloss: 0.0131963\n",
      "[70]\tvalid's binary_logloss: 0.012213\n",
      "[80]\tvalid's binary_logloss: 0.0118145\n",
      "[90]\tvalid's binary_logloss: 0.0114918\n",
      "[100]\tvalid's binary_logloss: 0.0115263\n",
      "[110]\tvalid's binary_logloss: 0.0115157\n",
      "[120]\tvalid's binary_logloss: 0.0117257\n",
      "[130]\tvalid's binary_logloss: 0.0117293\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid's binary_logloss: 0.0113975\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  44.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0834715\n",
      "[20]\tvalid's binary_logloss: 0.0417063\n",
      "[30]\tvalid's binary_logloss: 0.0244881\n",
      "[40]\tvalid's binary_logloss: 0.0177994\n",
      "[50]\tvalid's binary_logloss: 0.0141877\n",
      "[60]\tvalid's binary_logloss: 0.0130341\n",
      "[70]\tvalid's binary_logloss: 0.0128096\n",
      "[80]\tvalid's binary_logloss: 0.0129257\n",
      "[90]\tvalid's binary_logloss: 0.0132139\n",
      "[100]\tvalid's binary_logloss: 0.0136487\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid's binary_logloss: 0.0128096\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.923076923076923, total=  46.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0844293\n",
      "[20]\tvalid's binary_logloss: 0.0424448\n",
      "[30]\tvalid's binary_logloss: 0.0254481\n",
      "[40]\tvalid's binary_logloss: 0.0177285\n",
      "[50]\tvalid's binary_logloss: 0.0143115\n",
      "[60]\tvalid's binary_logloss: 0.0127744\n",
      "[70]\tvalid's binary_logloss: 0.0118974\n",
      "[80]\tvalid's binary_logloss: 0.0120883\n",
      "[90]\tvalid's binary_logloss: 0.0120666\n",
      "[100]\tvalid's binary_logloss: 0.0122993\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's binary_logloss: 0.0118817\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.905982905982906, total=  46.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0868168\n",
      "[20]\tvalid's binary_logloss: 0.0452452\n",
      "[30]\tvalid's binary_logloss: 0.0279654\n",
      "[40]\tvalid's binary_logloss: 0.0203459\n",
      "[50]\tvalid's binary_logloss: 0.0169977\n",
      "[60]\tvalid's binary_logloss: 0.0157603\n",
      "[70]\tvalid's binary_logloss: 0.0157491\n",
      "[80]\tvalid's binary_logloss: 0.0162491\n",
      "[90]\tvalid's binary_logloss: 0.0167511\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's binary_logloss: 0.0154272\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.9043478260869565, total=  45.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0832241\n",
      "[20]\tvalid's binary_logloss: 0.0428957\n",
      "[30]\tvalid's binary_logloss: 0.0263857\n",
      "[40]\tvalid's binary_logloss: 0.0186772\n",
      "[50]\tvalid's binary_logloss: 0.0147854\n",
      "[60]\tvalid's binary_logloss: 0.013313\n",
      "[70]\tvalid's binary_logloss: 0.0125145\n",
      "[80]\tvalid's binary_logloss: 0.0119281\n",
      "[90]\tvalid's binary_logloss: 0.0120871\n",
      "[100]\tvalid's binary_logloss: 0.0121043\n",
      "[110]\tvalid's binary_logloss: 0.0127316\n",
      "[120]\tvalid's binary_logloss: 0.0130072\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's binary_logloss: 0.0118194\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0, reg_lambda=0, subsamples=0.8, score=0.8965517241379309, total=  48.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0895017\n",
      "[20]\tvalid's binary_logloss: 0.0468828\n",
      "[30]\tvalid's binary_logloss: 0.0289624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid's binary_logloss: 0.0208599\n",
      "[50]\tvalid's binary_logloss: 0.0167901\n",
      "[60]\tvalid's binary_logloss: 0.0144905\n",
      "[70]\tvalid's binary_logloss: 0.0133122\n",
      "[80]\tvalid's binary_logloss: 0.0125234\n",
      "[90]\tvalid's binary_logloss: 0.0121318\n",
      "[100]\tvalid's binary_logloss: 0.0116658\n",
      "[110]\tvalid's binary_logloss: 0.0115074\n",
      "[120]\tvalid's binary_logloss: 0.0113263\n",
      "[130]\tvalid's binary_logloss: 0.0111896\n",
      "[140]\tvalid's binary_logloss: 0.0111483\n",
      "[150]\tvalid's binary_logloss: 0.0111203\n",
      "[160]\tvalid's binary_logloss: 0.0110874\n",
      "[170]\tvalid's binary_logloss: 0.011101\n",
      "[180]\tvalid's binary_logloss: 0.0110481\n",
      "[190]\tvalid's binary_logloss: 0.0110583\n",
      "[200]\tvalid's binary_logloss: 0.0110485\n",
      "[210]\tvalid's binary_logloss: 0.011064\n",
      "Early stopping, best iteration is:\n",
      "[188]\tvalid's binary_logloss: 0.0109916\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.923076923076923, total=  47.0s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0909198\n",
      "[20]\tvalid's binary_logloss: 0.0477165\n",
      "[30]\tvalid's binary_logloss: 0.0292872\n",
      "[40]\tvalid's binary_logloss: 0.0210568\n",
      "[50]\tvalid's binary_logloss: 0.0167835\n",
      "[60]\tvalid's binary_logloss: 0.0145695\n",
      "[70]\tvalid's binary_logloss: 0.0132926\n",
      "[80]\tvalid's binary_logloss: 0.0124092\n",
      "[90]\tvalid's binary_logloss: 0.0118215\n",
      "[100]\tvalid's binary_logloss: 0.0115227\n",
      "[110]\tvalid's binary_logloss: 0.0112605\n",
      "[120]\tvalid's binary_logloss: 0.0111153\n",
      "[130]\tvalid's binary_logloss: 0.0109413\n",
      "[140]\tvalid's binary_logloss: 0.01093\n",
      "[150]\tvalid's binary_logloss: 0.0108383\n",
      "[160]\tvalid's binary_logloss: 0.0108004\n",
      "[170]\tvalid's binary_logloss: 0.0107396\n",
      "[180]\tvalid's binary_logloss: 0.0106689\n",
      "[190]\tvalid's binary_logloss: 0.0106213\n",
      "[200]\tvalid's binary_logloss: 0.0106156\n",
      "[210]\tvalid's binary_logloss: 0.0105958\n",
      "[220]\tvalid's binary_logloss: 0.0105621\n",
      "[230]\tvalid's binary_logloss: 0.0105604\n",
      "[240]\tvalid's binary_logloss: 0.0105447\n",
      "[250]\tvalid's binary_logloss: 0.0105127\n",
      "[260]\tvalid's binary_logloss: 0.0105127\n",
      "[270]\tvalid's binary_logloss: 0.0105127\n",
      "Early stopping, best iteration is:\n",
      "[248]\tvalid's binary_logloss: 0.0105099\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9322033898305084, total=  47.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0926221\n",
      "[20]\tvalid's binary_logloss: 0.0490795\n",
      "[30]\tvalid's binary_logloss: 0.0317025\n",
      "[40]\tvalid's binary_logloss: 0.0233903\n",
      "[50]\tvalid's binary_logloss: 0.0193081\n",
      "[60]\tvalid's binary_logloss: 0.0171661\n",
      "[70]\tvalid's binary_logloss: 0.0158644\n",
      "[80]\tvalid's binary_logloss: 0.0152075\n",
      "[90]\tvalid's binary_logloss: 0.014913\n",
      "[100]\tvalid's binary_logloss: 0.0145757\n",
      "[110]\tvalid's binary_logloss: 0.0145939\n",
      "[120]\tvalid's binary_logloss: 0.0144382\n",
      "[130]\tvalid's binary_logloss: 0.0143724\n",
      "[140]\tvalid's binary_logloss: 0.0144224\n",
      "[150]\tvalid's binary_logloss: 0.0144928\n",
      "[160]\tvalid's binary_logloss: 0.0144941\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid's binary_logloss: 0.0143572\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.8849557522123894, total=  44.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0893209\n",
      "[20]\tvalid's binary_logloss: 0.0476667\n",
      "[30]\tvalid's binary_logloss: 0.0299314\n",
      "[40]\tvalid's binary_logloss: 0.0209397\n",
      "[50]\tvalid's binary_logloss: 0.0167675\n",
      "[60]\tvalid's binary_logloss: 0.0147518\n",
      "[70]\tvalid's binary_logloss: 0.0133965\n",
      "[80]\tvalid's binary_logloss: 0.0124777\n",
      "[90]\tvalid's binary_logloss: 0.0118418\n",
      "[100]\tvalid's binary_logloss: 0.0114831\n",
      "[110]\tvalid's binary_logloss: 0.0112183\n",
      "[120]\tvalid's binary_logloss: 0.0110438\n",
      "[130]\tvalid's binary_logloss: 0.0108771\n",
      "[140]\tvalid's binary_logloss: 0.0107864\n",
      "[150]\tvalid's binary_logloss: 0.0106383\n",
      "[160]\tvalid's binary_logloss: 0.0105724\n",
      "[170]\tvalid's binary_logloss: 0.0104811\n",
      "[180]\tvalid's binary_logloss: 0.0104213\n",
      "[190]\tvalid's binary_logloss: 0.0103569\n",
      "[200]\tvalid's binary_logloss: 0.0102864\n",
      "[210]\tvalid's binary_logloss: 0.0102652\n",
      "[220]\tvalid's binary_logloss: 0.0102334\n",
      "[230]\tvalid's binary_logloss: 0.0101968\n",
      "[240]\tvalid's binary_logloss: 0.0102113\n",
      "[250]\tvalid's binary_logloss: 0.0102072\n",
      "[260]\tvalid's binary_logloss: 0.0102146\n",
      "Early stopping, best iteration is:\n",
      "[233]\tvalid's binary_logloss: 0.0101871\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=1, subsamples=0.8, score=0.9137931034482758, total=  47.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0842072\n",
      "[20]\tvalid's binary_logloss: 0.042009\n",
      "[30]\tvalid's binary_logloss: 0.0251595\n",
      "[40]\tvalid's binary_logloss: 0.0177125\n",
      "[50]\tvalid's binary_logloss: 0.0143193\n",
      "[60]\tvalid's binary_logloss: 0.0127963\n",
      "[70]\tvalid's binary_logloss: 0.0120826\n",
      "[80]\tvalid's binary_logloss: 0.0118012\n",
      "[90]\tvalid's binary_logloss: 0.0114421\n",
      "[100]\tvalid's binary_logloss: 0.0112528\n",
      "[110]\tvalid's binary_logloss: 0.0112986\n",
      "[120]\tvalid's binary_logloss: 0.011269\n",
      "[130]\tvalid's binary_logloss: 0.0112826\n",
      "Early stopping, best iteration is:\n",
      "[103]\tvalid's binary_logloss: 0.0111499\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  42.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0856073\n",
      "[20]\tvalid's binary_logloss: 0.0439746\n",
      "[30]\tvalid's binary_logloss: 0.0264989\n",
      "[40]\tvalid's binary_logloss: 0.0189474\n",
      "[50]\tvalid's binary_logloss: 0.0152369\n",
      "[60]\tvalid's binary_logloss: 0.0133689\n",
      "[70]\tvalid's binary_logloss: 0.0123993\n",
      "[80]\tvalid's binary_logloss: 0.0119655\n",
      "[90]\tvalid's binary_logloss: 0.0116468\n",
      "[100]\tvalid's binary_logloss: 0.0115266\n",
      "[110]\tvalid's binary_logloss: 0.0114033\n",
      "[120]\tvalid's binary_logloss: 0.0113186\n",
      "[130]\tvalid's binary_logloss: 0.011246\n",
      "[140]\tvalid's binary_logloss: 0.0112514\n",
      "[150]\tvalid's binary_logloss: 0.0112514\n",
      "[160]\tvalid's binary_logloss: 0.0112514\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid's binary_logloss: 0.011246\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9152542372881356, total=  42.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0879067\n",
      "[20]\tvalid's binary_logloss: 0.0448043\n",
      "[30]\tvalid's binary_logloss: 0.0278625\n",
      "[40]\tvalid's binary_logloss: 0.020418\n",
      "[50]\tvalid's binary_logloss: 0.0171633\n",
      "[60]\tvalid's binary_logloss: 0.0156697\n",
      "[70]\tvalid's binary_logloss: 0.0149287\n",
      "[80]\tvalid's binary_logloss: 0.0147985\n",
      "[90]\tvalid's binary_logloss: 0.0147491\n",
      "[100]\tvalid's binary_logloss: 0.014687\n",
      "[110]\tvalid's binary_logloss: 0.0147198\n",
      "[120]\tvalid's binary_logloss: 0.0147442\n",
      "[130]\tvalid's binary_logloss: 0.0148063\n",
      "[140]\tvalid's binary_logloss: 0.0148199\n",
      "Early stopping, best iteration is:\n",
      "[117]\tvalid's binary_logloss: 0.0146661\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9043478260869565, total=  43.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.084638\n",
      "[20]\tvalid's binary_logloss: 0.0431312\n",
      "[30]\tvalid's binary_logloss: 0.0265418\n",
      "[40]\tvalid's binary_logloss: 0.0188266\n",
      "[50]\tvalid's binary_logloss: 0.0152281\n",
      "[60]\tvalid's binary_logloss: 0.0130431\n",
      "[70]\tvalid's binary_logloss: 0.0122244\n",
      "[80]\tvalid's binary_logloss: 0.0114733\n",
      "[90]\tvalid's binary_logloss: 0.0111535\n",
      "[100]\tvalid's binary_logloss: 0.0109658\n",
      "[110]\tvalid's binary_logloss: 0.010855\n",
      "[120]\tvalid's binary_logloss: 0.0107756\n",
      "[130]\tvalid's binary_logloss: 0.0107253\n",
      "[140]\tvalid's binary_logloss: 0.0107253\n",
      "[150]\tvalid's binary_logloss: 0.0107253\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid's binary_logloss: 0.0106984\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  42.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0838315\n",
      "[20]\tvalid's binary_logloss: 0.042557\n",
      "[30]\tvalid's binary_logloss: 0.0249705\n",
      "[40]\tvalid's binary_logloss: 0.0174178\n",
      "[50]\tvalid's binary_logloss: 0.0139446\n",
      "[60]\tvalid's binary_logloss: 0.0125953\n",
      "[70]\tvalid's binary_logloss: 0.0117043\n",
      "[80]\tvalid's binary_logloss: 0.0113518\n",
      "[90]\tvalid's binary_logloss: 0.0111686\n",
      "[100]\tvalid's binary_logloss: 0.0108909\n",
      "[110]\tvalid's binary_logloss: 0.01111\n",
      "[120]\tvalid's binary_logloss: 0.0111299\n",
      "[130]\tvalid's binary_logloss: 0.0111299\n",
      "Early stopping, best iteration is:\n",
      "[101]\tvalid's binary_logloss: 0.0108752\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  42.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0849533\n",
      "[20]\tvalid's binary_logloss: 0.0435513\n",
      "[30]\tvalid's binary_logloss: 0.0260779\n",
      "[40]\tvalid's binary_logloss: 0.0186028\n",
      "[50]\tvalid's binary_logloss: 0.0146363\n",
      "[60]\tvalid's binary_logloss: 0.0129619\n",
      "[70]\tvalid's binary_logloss: 0.0119355\n",
      "[80]\tvalid's binary_logloss: 0.0115934\n",
      "[90]\tvalid's binary_logloss: 0.011314\n",
      "[100]\tvalid's binary_logloss: 0.0111198\n",
      "[110]\tvalid's binary_logloss: 0.0109898\n",
      "[120]\tvalid's binary_logloss: 0.0109662\n",
      "[130]\tvalid's binary_logloss: 0.0109662\n",
      "[140]\tvalid's binary_logloss: 0.0109662\n",
      "Early stopping, best iteration is:\n",
      "[113]\tvalid's binary_logloss: 0.0109625\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.9322033898305084, total=  42.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0879459\n",
      "[20]\tvalid's binary_logloss: 0.0456743\n",
      "[30]\tvalid's binary_logloss: 0.0284976\n",
      "[40]\tvalid's binary_logloss: 0.0211891\n",
      "[50]\tvalid's binary_logloss: 0.0177802\n",
      "[60]\tvalid's binary_logloss: 0.0166988\n",
      "[70]\tvalid's binary_logloss: 0.0162643\n",
      "[80]\tvalid's binary_logloss: 0.0160829\n",
      "[90]\tvalid's binary_logloss: 0.0160154\n",
      "[100]\tvalid's binary_logloss: 0.0161271\n",
      "[110]\tvalid's binary_logloss: 0.0162963\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid's binary_logloss: 0.0160104\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.8947368421052632, total=  41.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0840822\n",
      "[20]\tvalid's binary_logloss: 0.0431884\n",
      "[30]\tvalid's binary_logloss: 0.026913\n",
      "[40]\tvalid's binary_logloss: 0.0189989\n",
      "[50]\tvalid's binary_logloss: 0.0149637\n",
      "[60]\tvalid's binary_logloss: 0.0130823\n",
      "[70]\tvalid's binary_logloss: 0.0120535\n",
      "[80]\tvalid's binary_logloss: 0.0114917\n",
      "[90]\tvalid's binary_logloss: 0.0112362\n",
      "[100]\tvalid's binary_logloss: 0.0112079\n",
      "[110]\tvalid's binary_logloss: 0.0110748\n",
      "[120]\tvalid's binary_logloss: 0.0110712\n",
      "[130]\tvalid's binary_logloss: 0.0110712\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid's binary_logloss: 0.011056\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=0.1, reg_lambda=0, subsamples=0.8, score=0.8965517241379309, total=  42.5s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0920988\n",
      "[20]\tvalid's binary_logloss: 0.0494416\n",
      "[30]\tvalid's binary_logloss: 0.0320775\n",
      "[40]\tvalid's binary_logloss: 0.0235975\n",
      "[50]\tvalid's binary_logloss: 0.0187356\n",
      "[60]\tvalid's binary_logloss: 0.016012\n",
      "[70]\tvalid's binary_logloss: 0.0143483\n",
      "[80]\tvalid's binary_logloss: 0.0132597\n",
      "[90]\tvalid's binary_logloss: 0.0127943\n",
      "[100]\tvalid's binary_logloss: 0.0125575\n",
      "[110]\tvalid's binary_logloss: 0.0125568\n",
      "[120]\tvalid's binary_logloss: 0.0125568\n",
      "[130]\tvalid's binary_logloss: 0.0125568\n",
      "[140]\tvalid's binary_logloss: 0.0125568\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid's binary_logloss: 0.0125568\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9137931034482758, total=  41.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0936929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tvalid's binary_logloss: 0.0512704\n",
      "[30]\tvalid's binary_logloss: 0.0329636\n",
      "[40]\tvalid's binary_logloss: 0.0243384\n",
      "[50]\tvalid's binary_logloss: 0.0198861\n",
      "[60]\tvalid's binary_logloss: 0.0170384\n",
      "[70]\tvalid's binary_logloss: 0.0154478\n",
      "[80]\tvalid's binary_logloss: 0.0143562\n",
      "[90]\tvalid's binary_logloss: 0.0137016\n",
      "[100]\tvalid's binary_logloss: 0.0133228\n",
      "[110]\tvalid's binary_logloss: 0.0133198\n",
      "[120]\tvalid's binary_logloss: 0.0133198\n",
      "Early stopping, best iteration is:\n",
      "[99]\tvalid's binary_logloss: 0.0133119\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  40.3s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0951908\n",
      "[20]\tvalid's binary_logloss: 0.0524025\n",
      "[30]\tvalid's binary_logloss: 0.0353567\n",
      "[40]\tvalid's binary_logloss: 0.0270435\n",
      "[50]\tvalid's binary_logloss: 0.0224333\n",
      "[60]\tvalid's binary_logloss: 0.0196862\n",
      "[70]\tvalid's binary_logloss: 0.017855\n",
      "[80]\tvalid's binary_logloss: 0.0169771\n",
      "[90]\tvalid's binary_logloss: 0.0165469\n",
      "[100]\tvalid's binary_logloss: 0.0164143\n",
      "[110]\tvalid's binary_logloss: 0.0164127\n",
      "[120]\tvalid's binary_logloss: 0.0164127\n",
      "Early stopping, best iteration is:\n",
      "[99]\tvalid's binary_logloss: 0.01641\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.8771929824561403, total=  39.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0919924\n",
      "[20]\tvalid's binary_logloss: 0.0517817\n",
      "[30]\tvalid's binary_logloss: 0.0336756\n",
      "[40]\tvalid's binary_logloss: 0.0241727\n",
      "[50]\tvalid's binary_logloss: 0.0193574\n",
      "[60]\tvalid's binary_logloss: 0.0164406\n",
      "[70]\tvalid's binary_logloss: 0.014844\n",
      "[80]\tvalid's binary_logloss: 0.013768\n",
      "[90]\tvalid's binary_logloss: 0.013006\n",
      "[100]\tvalid's binary_logloss: 0.0127354\n",
      "[110]\tvalid's binary_logloss: 0.0127354\n",
      "[120]\tvalid's binary_logloss: 0.0127354\n",
      "[130]\tvalid's binary_logloss: 0.0127354\n",
      "Early stopping, best iteration is:\n",
      "[102]\tvalid's binary_logloss: 0.0127354\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=1, subsamples=0.8, score=0.9043478260869565, total=  40.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0886835\n",
      "[20]\tvalid's binary_logloss: 0.0471729\n",
      "[30]\tvalid's binary_logloss: 0.0301582\n",
      "[40]\tvalid's binary_logloss: 0.0219045\n",
      "[50]\tvalid's binary_logloss: 0.0177717\n",
      "[60]\tvalid's binary_logloss: 0.0154669\n",
      "[70]\tvalid's binary_logloss: 0.0141533\n",
      "[80]\tvalid's binary_logloss: 0.0135907\n",
      "[90]\tvalid's binary_logloss: 0.0135003\n",
      "[100]\tvalid's binary_logloss: 0.0135003\n",
      "[110]\tvalid's binary_logloss: 0.0135003\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid's binary_logloss: 0.0135003\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9137931034482758, total=  40.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0887689\n",
      "[20]\tvalid's binary_logloss: 0.0477637\n",
      "[30]\tvalid's binary_logloss: 0.0306707\n",
      "[40]\tvalid's binary_logloss: 0.022432\n",
      "[50]\tvalid's binary_logloss: 0.0181257\n",
      "[60]\tvalid's binary_logloss: 0.0158996\n",
      "[70]\tvalid's binary_logloss: 0.0146343\n",
      "[80]\tvalid's binary_logloss: 0.0139438\n",
      "[90]\tvalid's binary_logloss: 0.013798\n",
      "[100]\tvalid's binary_logloss: 0.013798\n",
      "[110]\tvalid's binary_logloss: 0.013798\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's binary_logloss: 0.013798\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.9152542372881356, total=  40.1s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0921132\n",
      "[20]\tvalid's binary_logloss: 0.0496675\n",
      "[30]\tvalid's binary_logloss: 0.0328503\n",
      "[40]\tvalid's binary_logloss: 0.0246126\n",
      "[50]\tvalid's binary_logloss: 0.0202605\n",
      "[60]\tvalid's binary_logloss: 0.0180729\n",
      "[70]\tvalid's binary_logloss: 0.0167635\n",
      "[80]\tvalid's binary_logloss: 0.016447\n",
      "[90]\tvalid's binary_logloss: 0.0163518\n",
      "[100]\tvalid's binary_logloss: 0.0163518\n",
      "[110]\tvalid's binary_logloss: 0.0163518\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's binary_logloss: 0.0163518\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.8869565217391304, total=  39.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0888367\n",
      "[20]\tvalid's binary_logloss: 0.0485687\n",
      "[30]\tvalid's binary_logloss: 0.031043\n",
      "[40]\tvalid's binary_logloss: 0.0230204\n",
      "[50]\tvalid's binary_logloss: 0.01889\n",
      "[60]\tvalid's binary_logloss: 0.0162939\n",
      "[70]\tvalid's binary_logloss: 0.0148146\n",
      "[80]\tvalid's binary_logloss: 0.0140761\n",
      "[90]\tvalid's binary_logloss: 0.0140107\n",
      "[100]\tvalid's binary_logloss: 0.0140107\n",
      "[110]\tvalid's binary_logloss: 0.0140107\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid's binary_logloss: 0.0140107\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0.1, subsamples=0.8, score=0.8869565217391304, total=  39.7s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0875268\n",
      "[20]\tvalid's binary_logloss: 0.0465538\n",
      "[30]\tvalid's binary_logloss: 0.0297112\n",
      "[40]\tvalid's binary_logloss: 0.021534\n",
      "[50]\tvalid's binary_logloss: 0.0172454\n",
      "[60]\tvalid's binary_logloss: 0.0151091\n",
      "[70]\tvalid's binary_logloss: 0.0138771\n",
      "[80]\tvalid's binary_logloss: 0.0132467\n",
      "[90]\tvalid's binary_logloss: 0.0132513\n",
      "[100]\tvalid's binary_logloss: 0.0132515\n",
      "[110]\tvalid's binary_logloss: 0.0132516\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's binary_logloss: 0.0132467\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9137931034482758, total=  39.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0899668\n",
      "[20]\tvalid's binary_logloss: 0.0478572\n",
      "[30]\tvalid's binary_logloss: 0.0308534\n",
      "[40]\tvalid's binary_logloss: 0.0226537\n",
      "[50]\tvalid's binary_logloss: 0.018144\n",
      "[60]\tvalid's binary_logloss: 0.0159567\n",
      "[70]\tvalid's binary_logloss: 0.0147391\n",
      "[80]\tvalid's binary_logloss: 0.0140867\n",
      "[90]\tvalid's binary_logloss: 0.0140331\n",
      "[100]\tvalid's binary_logloss: 0.014031\n",
      "[110]\tvalid's binary_logloss: 0.01403\n",
      "[120]\tvalid's binary_logloss: 0.0140294\n",
      "[130]\tvalid's binary_logloss: 0.0140292\n",
      "[140]\tvalid's binary_logloss: 0.0140291\n",
      "[150]\tvalid's binary_logloss: 0.0140291\n",
      "[160]\tvalid's binary_logloss: 0.0140291\n",
      "[170]\tvalid's binary_logloss: 0.0140291\n",
      "[180]\tvalid's binary_logloss: 0.0140291\n",
      "[190]\tvalid's binary_logloss: 0.014029\n",
      "[200]\tvalid's binary_logloss: 0.014029\n",
      "[210]\tvalid's binary_logloss: 0.014029\n",
      "[220]\tvalid's binary_logloss: 0.014029\n",
      "[230]\tvalid's binary_logloss: 0.014029\n",
      "[240]\tvalid's binary_logloss: 0.014029\n",
      "[250]\tvalid's binary_logloss: 0.014029\n",
      "[260]\tvalid's binary_logloss: 0.014029\n",
      "[270]\tvalid's binary_logloss: 0.014029\n",
      "Early stopping, best iteration is:\n",
      "[240]\tvalid's binary_logloss: 0.014029\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.9137931034482758, total=  42.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.091305\n",
      "[20]\tvalid's binary_logloss: 0.0492385\n",
      "[30]\tvalid's binary_logloss: 0.0320496\n",
      "[40]\tvalid's binary_logloss: 0.0241505\n",
      "[50]\tvalid's binary_logloss: 0.020099\n",
      "[60]\tvalid's binary_logloss: 0.0180573\n",
      "[70]\tvalid's binary_logloss: 0.0166324\n",
      "[80]\tvalid's binary_logloss: 0.0163265\n",
      "[90]\tvalid's binary_logloss: 0.0162928\n",
      "[100]\tvalid's binary_logloss: 0.0162928\n",
      "[110]\tvalid's binary_logloss: 0.0162928\n",
      "[120]\tvalid's binary_logloss: 0.0162928\n",
      "[130]\tvalid's binary_logloss: 0.0162928\n",
      "[140]\tvalid's binary_logloss: 0.0162928\n",
      "[150]\tvalid's binary_logloss: 0.0162928\n",
      "[160]\tvalid's binary_logloss: 0.0162928\n",
      "[170]\tvalid's binary_logloss: 0.0162928\n",
      "[180]\tvalid's binary_logloss: 0.0162928\n",
      "[190]\tvalid's binary_logloss: 0.0162928\n",
      "[200]\tvalid's binary_logloss: 0.0162928\n",
      "[210]\tvalid's binary_logloss: 0.0162928\n",
      "[220]\tvalid's binary_logloss: 0.0162928\n",
      "Early stopping, best iteration is:\n",
      "[199]\tvalid's binary_logloss: 0.0162928\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.8849557522123894, total=  42.9s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.0888166\n",
      "[20]\tvalid's binary_logloss: 0.047153\n",
      "[30]\tvalid's binary_logloss: 0.0302641\n",
      "[40]\tvalid's binary_logloss: 0.021978\n",
      "[50]\tvalid's binary_logloss: 0.017818\n",
      "[60]\tvalid's binary_logloss: 0.01534\n",
      "[70]\tvalid's binary_logloss: 0.0140562\n",
      "[80]\tvalid's binary_logloss: 0.013508\n",
      "[90]\tvalid's binary_logloss: 0.0134456\n",
      "[100]\tvalid's binary_logloss: 0.0134456\n",
      "[110]\tvalid's binary_logloss: 0.0134456\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid's binary_logloss: 0.0134456\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, metric=binary_logloss, min_chlid_wight=5, n_estimators=10000, objective=binary, random_state=0, reg_alpha=1, reg_lambda=0, subsamples=0.8, score=0.8965517241379309, total=  39.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 324 out of 324 | elapsed: 416.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\tvalid's binary_logloss: 0.105314\n",
      "[20]\tvalid's binary_logloss: 0.0631555\n",
      "[30]\tvalid's binary_logloss: 0.0422966\n",
      "[40]\tvalid's binary_logloss: 0.0299846\n",
      "[50]\tvalid's binary_logloss: 0.0227428\n",
      "[60]\tvalid's binary_logloss: 0.0178151\n",
      "[70]\tvalid's binary_logloss: 0.0141128\n",
      "[80]\tvalid's binary_logloss: 0.0113684\n",
      "[90]\tvalid's binary_logloss: 0.00897569\n",
      "[100]\tvalid's binary_logloss: 0.00718457\n",
      "[110]\tvalid's binary_logloss: 0.00587878\n",
      "[120]\tvalid's binary_logloss: 0.00479114\n",
      "[130]\tvalid's binary_logloss: 0.00398992\n",
      "[140]\tvalid's binary_logloss: 0.00333816\n",
      "[150]\tvalid's binary_logloss: 0.00285184\n",
      "[160]\tvalid's binary_logloss: 0.00244191\n",
      "[170]\tvalid's binary_logloss: 0.00212321\n",
      "[180]\tvalid's binary_logloss: 0.00184287\n",
      "[190]\tvalid's binary_logloss: 0.00160712\n",
      "[200]\tvalid's binary_logloss: 0.00143025\n",
      "[210]\tvalid's binary_logloss: 0.00127051\n",
      "[220]\tvalid's binary_logloss: 0.0011393\n",
      "[230]\tvalid's binary_logloss: 0.00103483\n",
      "[240]\tvalid's binary_logloss: 0.000939507\n",
      "[250]\tvalid's binary_logloss: 0.000861202\n",
      "[260]\tvalid's binary_logloss: 0.000793302\n",
      "[270]\tvalid's binary_logloss: 0.000730292\n",
      "[280]\tvalid's binary_logloss: 0.000675893\n",
      "[290]\tvalid's binary_logloss: 0.000630395\n",
      "[300]\tvalid's binary_logloss: 0.000586634\n",
      "[310]\tvalid's binary_logloss: 0.000549207\n",
      "[320]\tvalid's binary_logloss: 0.000514783\n",
      "[330]\tvalid's binary_logloss: 0.000484957\n",
      "[340]\tvalid's binary_logloss: 0.000457965\n",
      "[350]\tvalid's binary_logloss: 0.000432963\n",
      "[360]\tvalid's binary_logloss: 0.000410497\n",
      "[370]\tvalid's binary_logloss: 0.000390528\n",
      "[380]\tvalid's binary_logloss: 0.000371896\n",
      "[390]\tvalid's binary_logloss: 0.000354488\n",
      "[400]\tvalid's binary_logloss: 0.000338904\n",
      "[410]\tvalid's binary_logloss: 0.000324239\n",
      "[420]\tvalid's binary_logloss: 0.000309864\n",
      "[430]\tvalid's binary_logloss: 0.000297978\n",
      "[440]\tvalid's binary_logloss: 0.000286359\n",
      "[450]\tvalid's binary_logloss: 0.000275294\n",
      "[460]\tvalid's binary_logloss: 0.000265072\n",
      "[470]\tvalid's binary_logloss: 0.000255841\n",
      "[480]\tvalid's binary_logloss: 0.000246838\n",
      "[490]\tvalid's binary_logloss: 0.000238352\n",
      "[500]\tvalid's binary_logloss: 0.000230539\n",
      "[510]\tvalid's binary_logloss: 0.000223068\n",
      "[520]\tvalid's binary_logloss: 0.00021595\n",
      "[530]\tvalid's binary_logloss: 0.00020946\n",
      "[540]\tvalid's binary_logloss: 0.000203277\n",
      "[550]\tvalid's binary_logloss: 0.000197586\n",
      "[560]\tvalid's binary_logloss: 0.000192259\n",
      "[570]\tvalid's binary_logloss: 0.00018705\n",
      "[580]\tvalid's binary_logloss: 0.00018201\n",
      "[590]\tvalid's binary_logloss: 0.000177579\n",
      "[600]\tvalid's binary_logloss: 0.000173212\n",
      "[610]\tvalid's binary_logloss: 0.000168953\n",
      "[620]\tvalid's binary_logloss: 0.000164858\n",
      "[630]\tvalid's binary_logloss: 0.000161108\n",
      "[640]\tvalid's binary_logloss: 0.000157184\n",
      "[650]\tvalid's binary_logloss: 0.000153544\n",
      "[660]\tvalid's binary_logloss: 0.000150169\n",
      "[670]\tvalid's binary_logloss: 0.000147026\n",
      "[680]\tvalid's binary_logloss: 0.000143787\n",
      "[690]\tvalid's binary_logloss: 0.000140732\n",
      "[700]\tvalid's binary_logloss: 0.000137914\n",
      "[710]\tvalid's binary_logloss: 0.000135141\n",
      "[720]\tvalid's binary_logloss: 0.000132424\n",
      "[730]\tvalid's binary_logloss: 0.000129846\n",
      "[740]\tvalid's binary_logloss: 0.000127436\n",
      "[750]\tvalid's binary_logloss: 0.000125013\n",
      "[760]\tvalid's binary_logloss: 0.000122612\n",
      "[770]\tvalid's binary_logloss: 0.00012038\n",
      "[780]\tvalid's binary_logloss: 0.00011827\n",
      "[790]\tvalid's binary_logloss: 0.000116105\n",
      "[800]\tvalid's binary_logloss: 0.000114123\n",
      "[810]\tvalid's binary_logloss: 0.000112151\n",
      "[820]\tvalid's binary_logloss: 0.000110307\n",
      "[830]\tvalid's binary_logloss: 0.000108437\n",
      "[840]\tvalid's binary_logloss: 0.000106712\n",
      "[850]\tvalid's binary_logloss: 0.000105036\n",
      "[860]\tvalid's binary_logloss: 0.000103265\n",
      "[870]\tvalid's binary_logloss: 0.000101732\n",
      "[880]\tvalid's binary_logloss: 0.00010023\n",
      "[890]\tvalid's binary_logloss: 9.87142e-05\n",
      "[900]\tvalid's binary_logloss: 9.72348e-05\n",
      "[910]\tvalid's binary_logloss: 9.57795e-05\n",
      "[920]\tvalid's binary_logloss: 9.43732e-05\n",
      "[930]\tvalid's binary_logloss: 9.29776e-05\n",
      "[940]\tvalid's binary_logloss: 9.16807e-05\n",
      "[950]\tvalid's binary_logloss: 9.04227e-05\n",
      "[960]\tvalid's binary_logloss: 8.91763e-05\n",
      "[970]\tvalid's binary_logloss: 8.79727e-05\n",
      "[980]\tvalid's binary_logloss: 8.68215e-05\n",
      "[990]\tvalid's binary_logloss: 8.56052e-05\n",
      "[1000]\tvalid's binary_logloss: 8.44614e-05\n",
      "[1010]\tvalid's binary_logloss: 8.33305e-05\n",
      "[1020]\tvalid's binary_logloss: 8.22513e-05\n",
      "[1030]\tvalid's binary_logloss: 8.12256e-05\n",
      "[1040]\tvalid's binary_logloss: 8.02007e-05\n",
      "[1050]\tvalid's binary_logloss: 7.92264e-05\n",
      "[1060]\tvalid's binary_logloss: 7.82856e-05\n",
      "[1070]\tvalid's binary_logloss: 7.73169e-05\n",
      "[1080]\tvalid's binary_logloss: 7.63631e-05\n",
      "[1090]\tvalid's binary_logloss: 7.54448e-05\n",
      "[1100]\tvalid's binary_logloss: 7.45945e-05\n",
      "[1110]\tvalid's binary_logloss: 7.37178e-05\n",
      "[1120]\tvalid's binary_logloss: 7.28864e-05\n",
      "[1130]\tvalid's binary_logloss: 7.2067e-05\n",
      "[1140]\tvalid's binary_logloss: 7.12235e-05\n",
      "[1150]\tvalid's binary_logloss: 7.04526e-05\n",
      "[1160]\tvalid's binary_logloss: 6.96812e-05\n",
      "[1170]\tvalid's binary_logloss: 6.89046e-05\n",
      "[1180]\tvalid's binary_logloss: 6.8177e-05\n",
      "[1190]\tvalid's binary_logloss: 6.74458e-05\n",
      "[1200]\tvalid's binary_logloss: 6.6771e-05\n",
      "[1210]\tvalid's binary_logloss: 6.60821e-05\n",
      "[1220]\tvalid's binary_logloss: 6.53949e-05\n",
      "[1230]\tvalid's binary_logloss: 6.47381e-05\n",
      "[1240]\tvalid's binary_logloss: 6.40923e-05\n",
      "[1250]\tvalid's binary_logloss: 6.34613e-05\n",
      "[1260]\tvalid's binary_logloss: 6.2813e-05\n",
      "[1270]\tvalid's binary_logloss: 6.2208e-05\n",
      "[1280]\tvalid's binary_logloss: 6.15972e-05\n",
      "[1290]\tvalid's binary_logloss: 6.09698e-05\n",
      "[1300]\tvalid's binary_logloss: 6.04032e-05\n",
      "[1310]\tvalid's binary_logloss: 5.98259e-05\n",
      "[1320]\tvalid's binary_logloss: 5.92773e-05\n",
      "[1330]\tvalid's binary_logloss: 5.87159e-05\n",
      "[1340]\tvalid's binary_logloss: 5.8172e-05\n",
      "[1350]\tvalid's binary_logloss: 5.76515e-05\n",
      "[1360]\tvalid's binary_logloss: 5.71296e-05\n",
      "[1370]\tvalid's binary_logloss: 5.66237e-05\n",
      "[1380]\tvalid's binary_logloss: 5.6103e-05\n",
      "[1390]\tvalid's binary_logloss: 5.56116e-05\n",
      "[1400]\tvalid's binary_logloss: 5.5127e-05\n",
      "[1410]\tvalid's binary_logloss: 5.46427e-05\n",
      "[1420]\tvalid's binary_logloss: 5.4153e-05\n",
      "[1430]\tvalid's binary_logloss: 5.36953e-05\n",
      "[1440]\tvalid's binary_logloss: 5.32358e-05\n",
      "[1450]\tvalid's binary_logloss: 5.27973e-05\n",
      "[1460]\tvalid's binary_logloss: 5.23643e-05\n",
      "[1470]\tvalid's binary_logloss: 5.19383e-05\n",
      "[1480]\tvalid's binary_logloss: 5.14997e-05\n",
      "[1490]\tvalid's binary_logloss: 5.10963e-05\n",
      "[1500]\tvalid's binary_logloss: 5.06656e-05\n",
      "[1510]\tvalid's binary_logloss: 5.02598e-05\n",
      "[1520]\tvalid's binary_logloss: 4.98648e-05\n",
      "[1530]\tvalid's binary_logloss: 4.94849e-05\n",
      "[1540]\tvalid's binary_logloss: 4.90914e-05\n",
      "[1550]\tvalid's binary_logloss: 4.87126e-05\n",
      "[1560]\tvalid's binary_logloss: 4.83532e-05\n",
      "[1570]\tvalid's binary_logloss: 4.7967e-05\n",
      "[1580]\tvalid's binary_logloss: 4.75977e-05\n",
      "[1590]\tvalid's binary_logloss: 4.72314e-05\n",
      "[1600]\tvalid's binary_logloss: 4.68766e-05\n",
      "[1610]\tvalid's binary_logloss: 4.65408e-05\n",
      "[1620]\tvalid's binary_logloss: 4.61974e-05\n",
      "[1630]\tvalid's binary_logloss: 4.5854e-05\n",
      "[1640]\tvalid's binary_logloss: 4.55223e-05\n",
      "[1650]\tvalid's binary_logloss: 4.51995e-05\n",
      "[1660]\tvalid's binary_logloss: 4.48538e-05\n",
      "[1670]\tvalid's binary_logloss: 4.45443e-05\n",
      "[1680]\tvalid's binary_logloss: 4.42278e-05\n",
      "[1690]\tvalid's binary_logloss: 4.39174e-05\n",
      "[1700]\tvalid's binary_logloss: 4.36146e-05\n",
      "[1710]\tvalid's binary_logloss: 4.33154e-05\n",
      "[1720]\tvalid's binary_logloss: 4.30059e-05\n",
      "[1730]\tvalid's binary_logloss: 4.27051e-05\n",
      "[1740]\tvalid's binary_logloss: 4.24149e-05\n",
      "[1750]\tvalid's binary_logloss: 4.21389e-05\n",
      "[1760]\tvalid's binary_logloss: 4.18648e-05\n",
      "[1770]\tvalid's binary_logloss: 4.15759e-05\n",
      "[1780]\tvalid's binary_logloss: 4.13102e-05\n",
      "[1790]\tvalid's binary_logloss: 4.10379e-05\n",
      "[1800]\tvalid's binary_logloss: 4.0761e-05\n",
      "[1810]\tvalid's binary_logloss: 4.05062e-05\n",
      "[1820]\tvalid's binary_logloss: 4.02555e-05\n",
      "[1830]\tvalid's binary_logloss: 3.99952e-05\n",
      "[1840]\tvalid's binary_logloss: 3.97481e-05\n",
      "[1850]\tvalid's binary_logloss: 3.94938e-05\n",
      "[1860]\tvalid's binary_logloss: 3.92401e-05\n",
      "[1870]\tvalid's binary_logloss: 3.89971e-05\n",
      "[1880]\tvalid's binary_logloss: 3.8768e-05\n",
      "[1890]\tvalid's binary_logloss: 3.85304e-05\n",
      "[1900]\tvalid's binary_logloss: 3.82821e-05\n",
      "[1910]\tvalid's binary_logloss: 3.8059e-05\n",
      "[1920]\tvalid's binary_logloss: 3.78268e-05\n",
      "[1930]\tvalid's binary_logloss: 3.7593e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1940]\tvalid's binary_logloss: 3.73686e-05\n",
      "[1950]\tvalid's binary_logloss: 3.71621e-05\n",
      "[1960]\tvalid's binary_logloss: 3.69379e-05\n",
      "[1970]\tvalid's binary_logloss: 3.67233e-05\n",
      "[1980]\tvalid's binary_logloss: 3.65079e-05\n",
      "[1990]\tvalid's binary_logloss: 3.62929e-05\n",
      "[2000]\tvalid's binary_logloss: 3.60868e-05\n",
      "[2010]\tvalid's binary_logloss: 3.58709e-05\n",
      "[2020]\tvalid's binary_logloss: 3.56663e-05\n",
      "[2030]\tvalid's binary_logloss: 3.54658e-05\n",
      "[2040]\tvalid's binary_logloss: 3.52596e-05\n",
      "[2050]\tvalid's binary_logloss: 3.50626e-05\n",
      "[2060]\tvalid's binary_logloss: 3.48616e-05\n",
      "[2070]\tvalid's binary_logloss: 3.46748e-05\n",
      "[2080]\tvalid's binary_logloss: 3.44776e-05\n",
      "[2090]\tvalid's binary_logloss: 3.42948e-05\n",
      "[2100]\tvalid's binary_logloss: 3.41048e-05\n",
      "[2110]\tvalid's binary_logloss: 3.39214e-05\n",
      "[2120]\tvalid's binary_logloss: 3.37316e-05\n",
      "[2130]\tvalid's binary_logloss: 3.35462e-05\n",
      "[2140]\tvalid's binary_logloss: 3.33623e-05\n",
      "[2150]\tvalid's binary_logloss: 3.3182e-05\n",
      "[2160]\tvalid's binary_logloss: 3.30114e-05\n",
      "[2170]\tvalid's binary_logloss: 3.28446e-05\n",
      "[2180]\tvalid's binary_logloss: 3.26724e-05\n",
      "[2190]\tvalid's binary_logloss: 3.25031e-05\n",
      "[2200]\tvalid's binary_logloss: 3.23385e-05\n",
      "[2210]\tvalid's binary_logloss: 3.21655e-05\n",
      "[2220]\tvalid's binary_logloss: 3.20042e-05\n",
      "[2230]\tvalid's binary_logloss: 3.18371e-05\n",
      "[2240]\tvalid's binary_logloss: 3.16734e-05\n",
      "[2250]\tvalid's binary_logloss: 3.15042e-05\n",
      "[2260]\tvalid's binary_logloss: 3.13469e-05\n",
      "[2270]\tvalid's binary_logloss: 3.11969e-05\n",
      "[2280]\tvalid's binary_logloss: 3.10441e-05\n",
      "[2290]\tvalid's binary_logloss: 3.08937e-05\n",
      "[2300]\tvalid's binary_logloss: 3.07356e-05\n",
      "[2310]\tvalid's binary_logloss: 3.0591e-05\n",
      "[2320]\tvalid's binary_logloss: 3.04394e-05\n",
      "[2330]\tvalid's binary_logloss: 3.0288e-05\n",
      "[2340]\tvalid's binary_logloss: 3.01418e-05\n",
      "[2350]\tvalid's binary_logloss: 2.99933e-05\n",
      "[2360]\tvalid's binary_logloss: 2.9847e-05\n",
      "[2370]\tvalid's binary_logloss: 2.9712e-05\n",
      "[2380]\tvalid's binary_logloss: 2.95708e-05\n",
      "[2390]\tvalid's binary_logloss: 2.94305e-05\n",
      "[2400]\tvalid's binary_logloss: 2.92942e-05\n",
      "[2410]\tvalid's binary_logloss: 2.91539e-05\n",
      "[2420]\tvalid's binary_logloss: 2.90189e-05\n",
      "[2430]\tvalid's binary_logloss: 2.88859e-05\n",
      "[2440]\tvalid's binary_logloss: 2.87597e-05\n",
      "[2450]\tvalid's binary_logloss: 2.86204e-05\n",
      "[2460]\tvalid's binary_logloss: 2.84837e-05\n",
      "[2470]\tvalid's binary_logloss: 2.83561e-05\n",
      "[2480]\tvalid's binary_logloss: 2.82331e-05\n",
      "[2490]\tvalid's binary_logloss: 2.80989e-05\n",
      "[2500]\tvalid's binary_logloss: 2.79736e-05\n",
      "[2510]\tvalid's binary_logloss: 2.78458e-05\n",
      "[2520]\tvalid's binary_logloss: 2.77222e-05\n",
      "[2530]\tvalid's binary_logloss: 2.75993e-05\n",
      "[2540]\tvalid's binary_logloss: 2.74779e-05\n",
      "[2550]\tvalid's binary_logloss: 2.73579e-05\n",
      "[2560]\tvalid's binary_logloss: 2.72413e-05\n",
      "[2570]\tvalid's binary_logloss: 2.71199e-05\n",
      "[2580]\tvalid's binary_logloss: 2.69997e-05\n",
      "[2590]\tvalid's binary_logloss: 2.68864e-05\n",
      "[2600]\tvalid's binary_logloss: 2.67659e-05\n",
      "[2610]\tvalid's binary_logloss: 2.66512e-05\n",
      "[2620]\tvalid's binary_logloss: 2.65396e-05\n",
      "[2630]\tvalid's binary_logloss: 2.6425e-05\n",
      "[2640]\tvalid's binary_logloss: 2.63162e-05\n",
      "[2650]\tvalid's binary_logloss: 2.62034e-05\n",
      "[2660]\tvalid's binary_logloss: 2.60945e-05\n",
      "[2670]\tvalid's binary_logloss: 2.59837e-05\n",
      "[2680]\tvalid's binary_logloss: 2.58754e-05\n",
      "[2690]\tvalid's binary_logloss: 2.57695e-05\n",
      "[2700]\tvalid's binary_logloss: 2.56664e-05\n",
      "[2710]\tvalid's binary_logloss: 2.55621e-05\n",
      "[2720]\tvalid's binary_logloss: 2.54578e-05\n",
      "[2730]\tvalid's binary_logloss: 2.53543e-05\n",
      "[2740]\tvalid's binary_logloss: 2.5252e-05\n",
      "[2750]\tvalid's binary_logloss: 2.51524e-05\n",
      "[2760]\tvalid's binary_logloss: 2.50466e-05\n",
      "[2770]\tvalid's binary_logloss: 2.49476e-05\n",
      "[2780]\tvalid's binary_logloss: 2.48431e-05\n",
      "[2790]\tvalid's binary_logloss: 2.47475e-05\n",
      "[2800]\tvalid's binary_logloss: 2.46429e-05\n",
      "[2810]\tvalid's binary_logloss: 2.4546e-05\n",
      "[2820]\tvalid's binary_logloss: 2.44506e-05\n",
      "[2830]\tvalid's binary_logloss: 2.43538e-05\n",
      "[2840]\tvalid's binary_logloss: 2.42646e-05\n",
      "[2850]\tvalid's binary_logloss: 2.41675e-05\n",
      "[2860]\tvalid's binary_logloss: 2.40737e-05\n",
      "[2870]\tvalid's binary_logloss: 2.398e-05\n",
      "[2880]\tvalid's binary_logloss: 2.38865e-05\n",
      "[2890]\tvalid's binary_logloss: 2.37949e-05\n",
      "[2900]\tvalid's binary_logloss: 2.37054e-05\n",
      "[2910]\tvalid's binary_logloss: 2.36181e-05\n",
      "[2920]\tvalid's binary_logloss: 2.35306e-05\n",
      "[2930]\tvalid's binary_logloss: 2.34419e-05\n",
      "[2940]\tvalid's binary_logloss: 2.33535e-05\n",
      "[2950]\tvalid's binary_logloss: 2.32616e-05\n",
      "[2960]\tvalid's binary_logloss: 2.31748e-05\n",
      "[2970]\tvalid's binary_logloss: 2.3089e-05\n",
      "[2980]\tvalid's binary_logloss: 2.3006e-05\n",
      "[2990]\tvalid's binary_logloss: 2.29175e-05\n",
      "[3000]\tvalid's binary_logloss: 2.28356e-05\n",
      "[3010]\tvalid's binary_logloss: 2.27508e-05\n",
      "[3020]\tvalid's binary_logloss: 2.2663e-05\n",
      "[3030]\tvalid's binary_logloss: 2.2581e-05\n",
      "[3040]\tvalid's binary_logloss: 2.24995e-05\n",
      "[3050]\tvalid's binary_logloss: 2.24169e-05\n",
      "[3060]\tvalid's binary_logloss: 2.23368e-05\n",
      "[3070]\tvalid's binary_logloss: 2.22573e-05\n",
      "[3080]\tvalid's binary_logloss: 2.2176e-05\n",
      "[3090]\tvalid's binary_logloss: 2.20955e-05\n",
      "[3100]\tvalid's binary_logloss: 2.20155e-05\n",
      "[3110]\tvalid's binary_logloss: 2.19401e-05\n",
      "[3120]\tvalid's binary_logloss: 2.18618e-05\n",
      "[3130]\tvalid's binary_logloss: 2.17831e-05\n",
      "[3140]\tvalid's binary_logloss: 2.17102e-05\n",
      "[3150]\tvalid's binary_logloss: 2.16338e-05\n",
      "[3160]\tvalid's binary_logloss: 2.15571e-05\n",
      "[3170]\tvalid's binary_logloss: 2.14818e-05\n",
      "[3180]\tvalid's binary_logloss: 2.1409e-05\n",
      "[3190]\tvalid's binary_logloss: 2.13346e-05\n",
      "[3200]\tvalid's binary_logloss: 2.12647e-05\n",
      "[3210]\tvalid's binary_logloss: 2.11933e-05\n",
      "[3220]\tvalid's binary_logloss: 2.11227e-05\n",
      "[3230]\tvalid's binary_logloss: 2.10521e-05\n",
      "[3240]\tvalid's binary_logloss: 2.0982e-05\n",
      "[3250]\tvalid's binary_logloss: 2.09086e-05\n",
      "[3260]\tvalid's binary_logloss: 2.08413e-05\n",
      "[3270]\tvalid's binary_logloss: 2.07727e-05\n",
      "[3280]\tvalid's binary_logloss: 2.06994e-05\n",
      "[3290]\tvalid's binary_logloss: 2.06332e-05\n",
      "[3300]\tvalid's binary_logloss: 2.05619e-05\n",
      "[3310]\tvalid's binary_logloss: 2.04928e-05\n",
      "[3320]\tvalid's binary_logloss: 2.04247e-05\n",
      "[3330]\tvalid's binary_logloss: 2.03597e-05\n",
      "[3340]\tvalid's binary_logloss: 2.02912e-05\n",
      "[3350]\tvalid's binary_logloss: 2.02244e-05\n",
      "[3360]\tvalid's binary_logloss: 2.01618e-05\n",
      "[3370]\tvalid's binary_logloss: 2.00982e-05\n",
      "[3380]\tvalid's binary_logloss: 2.00335e-05\n",
      "[3390]\tvalid's binary_logloss: 1.99678e-05\n",
      "[3400]\tvalid's binary_logloss: 1.99061e-05\n",
      "[3410]\tvalid's binary_logloss: 1.98415e-05\n",
      "[3420]\tvalid's binary_logloss: 1.97813e-05\n",
      "[3430]\tvalid's binary_logloss: 1.97194e-05\n",
      "[3440]\tvalid's binary_logloss: 1.96546e-05\n",
      "[3450]\tvalid's binary_logloss: 1.95912e-05\n",
      "[3460]\tvalid's binary_logloss: 1.95312e-05\n",
      "[3470]\tvalid's binary_logloss: 1.94682e-05\n",
      "[3480]\tvalid's binary_logloss: 1.94102e-05\n",
      "[3490]\tvalid's binary_logloss: 1.93502e-05\n",
      "[3500]\tvalid's binary_logloss: 1.929e-05\n",
      "[3510]\tvalid's binary_logloss: 1.9229e-05\n",
      "[3520]\tvalid's binary_logloss: 1.91703e-05\n",
      "[3530]\tvalid's binary_logloss: 1.9114e-05\n",
      "[3540]\tvalid's binary_logloss: 1.90557e-05\n",
      "[3550]\tvalid's binary_logloss: 1.8996e-05\n",
      "[3560]\tvalid's binary_logloss: 1.8939e-05\n",
      "[3570]\tvalid's binary_logloss: 1.88821e-05\n",
      "[3580]\tvalid's binary_logloss: 1.88237e-05\n",
      "[3590]\tvalid's binary_logloss: 1.8767e-05\n",
      "[3600]\tvalid's binary_logloss: 1.87129e-05\n",
      "[3610]\tvalid's binary_logloss: 1.86532e-05\n",
      "[3620]\tvalid's binary_logloss: 1.8596e-05\n",
      "[3630]\tvalid's binary_logloss: 1.85408e-05\n",
      "[3640]\tvalid's binary_logloss: 1.84876e-05\n",
      "[3650]\tvalid's binary_logloss: 1.84313e-05\n",
      "[3660]\tvalid's binary_logloss: 1.83755e-05\n",
      "[3670]\tvalid's binary_logloss: 1.83191e-05\n",
      "[3680]\tvalid's binary_logloss: 1.82662e-05\n",
      "[3690]\tvalid's binary_logloss: 1.82107e-05\n",
      "[3700]\tvalid's binary_logloss: 1.81584e-05\n",
      "[3710]\tvalid's binary_logloss: 1.81057e-05\n",
      "[3720]\tvalid's binary_logloss: 1.80535e-05\n",
      "[3730]\tvalid's binary_logloss: 1.80017e-05\n",
      "[3740]\tvalid's binary_logloss: 1.79491e-05\n",
      "[3750]\tvalid's binary_logloss: 1.78981e-05\n",
      "[3760]\tvalid's binary_logloss: 1.7846e-05\n",
      "[3770]\tvalid's binary_logloss: 1.77952e-05\n",
      "[3780]\tvalid's binary_logloss: 1.77457e-05\n",
      "[3790]\tvalid's binary_logloss: 1.76934e-05\n",
      "[3800]\tvalid's binary_logloss: 1.76426e-05\n",
      "[3810]\tvalid's binary_logloss: 1.75922e-05\n",
      "[3820]\tvalid's binary_logloss: 1.75448e-05\n",
      "[3830]\tvalid's binary_logloss: 1.74916e-05\n",
      "[3840]\tvalid's binary_logloss: 1.7441e-05\n",
      "[3850]\tvalid's binary_logloss: 1.73923e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3860]\tvalid's binary_logloss: 1.73416e-05\n",
      "[3870]\tvalid's binary_logloss: 1.72949e-05\n",
      "[3880]\tvalid's binary_logloss: 1.72472e-05\n",
      "[3890]\tvalid's binary_logloss: 1.72001e-05\n",
      "[3900]\tvalid's binary_logloss: 1.71518e-05\n",
      "[3910]\tvalid's binary_logloss: 1.7106e-05\n",
      "[3920]\tvalid's binary_logloss: 1.70616e-05\n",
      "[3930]\tvalid's binary_logloss: 1.70135e-05\n",
      "[3940]\tvalid's binary_logloss: 1.69691e-05\n",
      "[3950]\tvalid's binary_logloss: 1.69234e-05\n",
      "[3960]\tvalid's binary_logloss: 1.68779e-05\n",
      "[3970]\tvalid's binary_logloss: 1.68336e-05\n",
      "[3980]\tvalid's binary_logloss: 1.67874e-05\n",
      "[3990]\tvalid's binary_logloss: 1.67436e-05\n",
      "[4000]\tvalid's binary_logloss: 1.66976e-05\n",
      "[4010]\tvalid's binary_logloss: 1.66517e-05\n",
      "[4020]\tvalid's binary_logloss: 1.66087e-05\n",
      "[4030]\tvalid's binary_logloss: 1.65656e-05\n",
      "[4040]\tvalid's binary_logloss: 1.65206e-05\n",
      "[4050]\tvalid's binary_logloss: 1.64767e-05\n",
      "[4060]\tvalid's binary_logloss: 1.64322e-05\n",
      "[4070]\tvalid's binary_logloss: 1.63891e-05\n",
      "[4080]\tvalid's binary_logloss: 1.63455e-05\n",
      "[4090]\tvalid's binary_logloss: 1.63014e-05\n",
      "[4100]\tvalid's binary_logloss: 1.62578e-05\n",
      "[4110]\tvalid's binary_logloss: 1.62157e-05\n",
      "[4120]\tvalid's binary_logloss: 1.61753e-05\n",
      "[4130]\tvalid's binary_logloss: 1.61317e-05\n",
      "[4140]\tvalid's binary_logloss: 1.60898e-05\n",
      "[4150]\tvalid's binary_logloss: 1.605e-05\n",
      "[4160]\tvalid's binary_logloss: 1.60088e-05\n",
      "[4170]\tvalid's binary_logloss: 1.59651e-05\n",
      "[4180]\tvalid's binary_logloss: 1.59248e-05\n",
      "[4190]\tvalid's binary_logloss: 1.58841e-05\n",
      "[4200]\tvalid's binary_logloss: 1.58418e-05\n",
      "[4210]\tvalid's binary_logloss: 1.58033e-05\n",
      "[4220]\tvalid's binary_logloss: 1.57631e-05\n",
      "[4230]\tvalid's binary_logloss: 1.57234e-05\n",
      "[4240]\tvalid's binary_logloss: 1.56818e-05\n",
      "[4250]\tvalid's binary_logloss: 1.56429e-05\n",
      "[4260]\tvalid's binary_logloss: 1.56031e-05\n",
      "[4270]\tvalid's binary_logloss: 1.55651e-05\n",
      "[4280]\tvalid's binary_logloss: 1.55261e-05\n",
      "[4290]\tvalid's binary_logloss: 1.54854e-05\n",
      "[4300]\tvalid's binary_logloss: 1.54468e-05\n",
      "[4310]\tvalid's binary_logloss: 1.54096e-05\n",
      "[4320]\tvalid's binary_logloss: 1.53729e-05\n",
      "[4330]\tvalid's binary_logloss: 1.53363e-05\n",
      "[4340]\tvalid's binary_logloss: 1.53e-05\n",
      "[4350]\tvalid's binary_logloss: 1.52624e-05\n",
      "[4360]\tvalid's binary_logloss: 1.52254e-05\n",
      "[4370]\tvalid's binary_logloss: 1.51894e-05\n",
      "[4380]\tvalid's binary_logloss: 1.51538e-05\n",
      "[4390]\tvalid's binary_logloss: 1.51163e-05\n",
      "[4400]\tvalid's binary_logloss: 1.50791e-05\n",
      "[4410]\tvalid's binary_logloss: 1.50436e-05\n",
      "[4420]\tvalid's binary_logloss: 1.50089e-05\n",
      "[4430]\tvalid's binary_logloss: 1.49696e-05\n",
      "[4440]\tvalid's binary_logloss: 1.49341e-05\n",
      "[4450]\tvalid's binary_logloss: 1.49001e-05\n",
      "[4460]\tvalid's binary_logloss: 1.48628e-05\n",
      "[4470]\tvalid's binary_logloss: 1.48273e-05\n",
      "[4480]\tvalid's binary_logloss: 1.47933e-05\n",
      "[4490]\tvalid's binary_logloss: 1.47568e-05\n",
      "[4500]\tvalid's binary_logloss: 1.47237e-05\n",
      "[4510]\tvalid's binary_logloss: 1.46882e-05\n",
      "[4520]\tvalid's binary_logloss: 1.46543e-05\n",
      "[4530]\tvalid's binary_logloss: 1.46213e-05\n",
      "[4540]\tvalid's binary_logloss: 1.45867e-05\n",
      "[4550]\tvalid's binary_logloss: 1.45541e-05\n",
      "[4560]\tvalid's binary_logloss: 1.452e-05\n",
      "[4570]\tvalid's binary_logloss: 1.44873e-05\n",
      "[4580]\tvalid's binary_logloss: 1.44538e-05\n",
      "[4590]\tvalid's binary_logloss: 1.44212e-05\n",
      "[4600]\tvalid's binary_logloss: 1.43863e-05\n",
      "[4610]\tvalid's binary_logloss: 1.4355e-05\n",
      "[4620]\tvalid's binary_logloss: 1.43219e-05\n",
      "[4630]\tvalid's binary_logloss: 1.42879e-05\n",
      "[4640]\tvalid's binary_logloss: 1.42549e-05\n",
      "[4650]\tvalid's binary_logloss: 1.42225e-05\n",
      "[4660]\tvalid's binary_logloss: 1.41912e-05\n",
      "[4670]\tvalid's binary_logloss: 1.41582e-05\n",
      "[4680]\tvalid's binary_logloss: 1.41271e-05\n",
      "[4690]\tvalid's binary_logloss: 1.40957e-05\n",
      "[4700]\tvalid's binary_logloss: 1.40627e-05\n",
      "[4710]\tvalid's binary_logloss: 1.40311e-05\n",
      "[4720]\tvalid's binary_logloss: 1.40013e-05\n",
      "[4730]\tvalid's binary_logloss: 1.39708e-05\n",
      "[4740]\tvalid's binary_logloss: 1.39383e-05\n",
      "[4750]\tvalid's binary_logloss: 1.39073e-05\n",
      "[4760]\tvalid's binary_logloss: 1.38764e-05\n",
      "[4770]\tvalid's binary_logloss: 1.38466e-05\n",
      "[4780]\tvalid's binary_logloss: 1.38155e-05\n",
      "[4790]\tvalid's binary_logloss: 1.37853e-05\n",
      "[4800]\tvalid's binary_logloss: 1.37546e-05\n",
      "[4810]\tvalid's binary_logloss: 1.3725e-05\n",
      "[4820]\tvalid's binary_logloss: 1.36946e-05\n",
      "[4830]\tvalid's binary_logloss: 1.3665e-05\n",
      "[4840]\tvalid's binary_logloss: 1.36353e-05\n",
      "[4850]\tvalid's binary_logloss: 1.36061e-05\n",
      "[4860]\tvalid's binary_logloss: 1.35758e-05\n",
      "[4870]\tvalid's binary_logloss: 1.35466e-05\n",
      "[4880]\tvalid's binary_logloss: 1.3518e-05\n",
      "[4890]\tvalid's binary_logloss: 1.34892e-05\n",
      "[4900]\tvalid's binary_logloss: 1.34595e-05\n",
      "[4910]\tvalid's binary_logloss: 1.34307e-05\n",
      "[4920]\tvalid's binary_logloss: 1.34025e-05\n",
      "[4930]\tvalid's binary_logloss: 1.33734e-05\n",
      "[4940]\tvalid's binary_logloss: 1.33459e-05\n",
      "[4950]\tvalid's binary_logloss: 1.33155e-05\n",
      "[4960]\tvalid's binary_logloss: 1.32859e-05\n",
      "[4970]\tvalid's binary_logloss: 1.32598e-05\n",
      "[4980]\tvalid's binary_logloss: 1.32318e-05\n",
      "[4990]\tvalid's binary_logloss: 1.32041e-05\n",
      "[5000]\tvalid's binary_logloss: 1.31769e-05\n",
      "[5010]\tvalid's binary_logloss: 1.31493e-05\n",
      "[5020]\tvalid's binary_logloss: 1.3122e-05\n",
      "[5030]\tvalid's binary_logloss: 1.30937e-05\n",
      "[5040]\tvalid's binary_logloss: 1.30673e-05\n",
      "[5050]\tvalid's binary_logloss: 1.30399e-05\n",
      "[5060]\tvalid's binary_logloss: 1.30148e-05\n",
      "[5070]\tvalid's binary_logloss: 1.29886e-05\n",
      "[5080]\tvalid's binary_logloss: 1.29614e-05\n",
      "[5090]\tvalid's binary_logloss: 1.2934e-05\n",
      "[5100]\tvalid's binary_logloss: 1.29078e-05\n",
      "[5110]\tvalid's binary_logloss: 1.2881e-05\n",
      "[5120]\tvalid's binary_logloss: 1.28542e-05\n",
      "[5130]\tvalid's binary_logloss: 1.28295e-05\n",
      "[5140]\tvalid's binary_logloss: 1.28037e-05\n",
      "[5150]\tvalid's binary_logloss: 1.27781e-05\n",
      "[5160]\tvalid's binary_logloss: 1.27508e-05\n",
      "[5170]\tvalid's binary_logloss: 1.27248e-05\n",
      "[5180]\tvalid's binary_logloss: 1.26986e-05\n",
      "[5190]\tvalid's binary_logloss: 1.26728e-05\n",
      "[5200]\tvalid's binary_logloss: 1.26477e-05\n",
      "[5210]\tvalid's binary_logloss: 1.26222e-05\n",
      "[5220]\tvalid's binary_logloss: 1.25977e-05\n",
      "[5230]\tvalid's binary_logloss: 1.2571e-05\n",
      "[5240]\tvalid's binary_logloss: 1.25468e-05\n",
      "[5250]\tvalid's binary_logloss: 1.25201e-05\n",
      "[5260]\tvalid's binary_logloss: 1.24962e-05\n",
      "[5270]\tvalid's binary_logloss: 1.2471e-05\n",
      "[5280]\tvalid's binary_logloss: 1.24463e-05\n",
      "[5290]\tvalid's binary_logloss: 1.24217e-05\n",
      "[5300]\tvalid's binary_logloss: 1.23963e-05\n",
      "[5310]\tvalid's binary_logloss: 1.23723e-05\n",
      "[5320]\tvalid's binary_logloss: 1.23492e-05\n",
      "[5330]\tvalid's binary_logloss: 1.23251e-05\n",
      "[5340]\tvalid's binary_logloss: 1.23012e-05\n",
      "[5350]\tvalid's binary_logloss: 1.2277e-05\n",
      "[5360]\tvalid's binary_logloss: 1.22521e-05\n",
      "[5370]\tvalid's binary_logloss: 1.22285e-05\n",
      "[5380]\tvalid's binary_logloss: 1.22054e-05\n",
      "[5390]\tvalid's binary_logloss: 1.21818e-05\n",
      "[5400]\tvalid's binary_logloss: 1.21584e-05\n",
      "[5410]\tvalid's binary_logloss: 1.21346e-05\n",
      "[5420]\tvalid's binary_logloss: 1.21106e-05\n",
      "[5430]\tvalid's binary_logloss: 1.20861e-05\n",
      "[5440]\tvalid's binary_logloss: 1.20636e-05\n",
      "[5450]\tvalid's binary_logloss: 1.20406e-05\n",
      "[5460]\tvalid's binary_logloss: 1.20176e-05\n",
      "[5470]\tvalid's binary_logloss: 1.19949e-05\n",
      "[5480]\tvalid's binary_logloss: 1.19731e-05\n",
      "[5490]\tvalid's binary_logloss: 1.195e-05\n",
      "[5500]\tvalid's binary_logloss: 1.19263e-05\n",
      "[5510]\tvalid's binary_logloss: 1.19048e-05\n",
      "[5520]\tvalid's binary_logloss: 1.18813e-05\n",
      "[5530]\tvalid's binary_logloss: 1.18595e-05\n",
      "[5540]\tvalid's binary_logloss: 1.18386e-05\n",
      "[5550]\tvalid's binary_logloss: 1.1816e-05\n",
      "[5560]\tvalid's binary_logloss: 1.17953e-05\n",
      "[5570]\tvalid's binary_logloss: 1.17714e-05\n",
      "[5580]\tvalid's binary_logloss: 1.17495e-05\n",
      "[5590]\tvalid's binary_logloss: 1.17284e-05\n",
      "[5600]\tvalid's binary_logloss: 1.17068e-05\n",
      "[5610]\tvalid's binary_logloss: 1.16845e-05\n",
      "[5620]\tvalid's binary_logloss: 1.16634e-05\n",
      "[5630]\tvalid's binary_logloss: 1.16423e-05\n",
      "[5640]\tvalid's binary_logloss: 1.16216e-05\n",
      "[5650]\tvalid's binary_logloss: 1.15993e-05\n",
      "[5660]\tvalid's binary_logloss: 1.15778e-05\n",
      "[5670]\tvalid's binary_logloss: 1.15568e-05\n",
      "[5680]\tvalid's binary_logloss: 1.15349e-05\n",
      "[5690]\tvalid's binary_logloss: 1.15143e-05\n",
      "[5700]\tvalid's binary_logloss: 1.14931e-05\n",
      "[5710]\tvalid's binary_logloss: 1.14727e-05\n",
      "[5720]\tvalid's binary_logloss: 1.14511e-05\n",
      "[5730]\tvalid's binary_logloss: 1.14311e-05\n",
      "[5740]\tvalid's binary_logloss: 1.14093e-05\n",
      "[5750]\tvalid's binary_logloss: 1.13887e-05\n",
      "[5760]\tvalid's binary_logloss: 1.13686e-05\n",
      "[5770]\tvalid's binary_logloss: 1.13484e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5780]\tvalid's binary_logloss: 1.13271e-05\n",
      "[5790]\tvalid's binary_logloss: 1.13074e-05\n",
      "[5800]\tvalid's binary_logloss: 1.12874e-05\n",
      "[5810]\tvalid's binary_logloss: 1.12666e-05\n",
      "[5820]\tvalid's binary_logloss: 1.12483e-05\n",
      "[5830]\tvalid's binary_logloss: 1.12273e-05\n",
      "[5840]\tvalid's binary_logloss: 1.1207e-05\n",
      "[5850]\tvalid's binary_logloss: 1.11862e-05\n",
      "[5860]\tvalid's binary_logloss: 1.11669e-05\n",
      "[5870]\tvalid's binary_logloss: 1.11477e-05\n",
      "[5880]\tvalid's binary_logloss: 1.11292e-05\n",
      "[5890]\tvalid's binary_logloss: 1.11101e-05\n",
      "[5900]\tvalid's binary_logloss: 1.109e-05\n",
      "[5910]\tvalid's binary_logloss: 1.10703e-05\n",
      "[5920]\tvalid's binary_logloss: 1.10519e-05\n",
      "[5930]\tvalid's binary_logloss: 1.10331e-05\n",
      "[5940]\tvalid's binary_logloss: 1.10128e-05\n",
      "[5950]\tvalid's binary_logloss: 1.09936e-05\n",
      "[5960]\tvalid's binary_logloss: 1.09749e-05\n",
      "[5970]\tvalid's binary_logloss: 1.09554e-05\n",
      "[5980]\tvalid's binary_logloss: 1.09369e-05\n",
      "[5990]\tvalid's binary_logloss: 1.09184e-05\n",
      "[6000]\tvalid's binary_logloss: 1.08981e-05\n",
      "[6010]\tvalid's binary_logloss: 1.08797e-05\n",
      "[6020]\tvalid's binary_logloss: 1.0861e-05\n",
      "[6030]\tvalid's binary_logloss: 1.08432e-05\n",
      "[6040]\tvalid's binary_logloss: 1.08247e-05\n",
      "[6050]\tvalid's binary_logloss: 1.08067e-05\n",
      "[6060]\tvalid's binary_logloss: 1.07882e-05\n",
      "[6070]\tvalid's binary_logloss: 1.07698e-05\n",
      "[6080]\tvalid's binary_logloss: 1.07513e-05\n",
      "[6090]\tvalid's binary_logloss: 1.07322e-05\n",
      "[6100]\tvalid's binary_logloss: 1.07135e-05\n",
      "[6110]\tvalid's binary_logloss: 1.06958e-05\n",
      "[6120]\tvalid's binary_logloss: 1.06781e-05\n",
      "[6130]\tvalid's binary_logloss: 1.06593e-05\n",
      "[6140]\tvalid's binary_logloss: 1.0642e-05\n",
      "[6150]\tvalid's binary_logloss: 1.06236e-05\n",
      "[6160]\tvalid's binary_logloss: 1.0607e-05\n",
      "[6170]\tvalid's binary_logloss: 1.05881e-05\n",
      "[6180]\tvalid's binary_logloss: 1.05709e-05\n",
      "[6190]\tvalid's binary_logloss: 1.05534e-05\n",
      "[6200]\tvalid's binary_logloss: 1.05356e-05\n",
      "[6210]\tvalid's binary_logloss: 1.05187e-05\n",
      "[6220]\tvalid's binary_logloss: 1.05021e-05\n",
      "[6230]\tvalid's binary_logloss: 1.04837e-05\n",
      "[6240]\tvalid's binary_logloss: 1.04671e-05\n",
      "[6250]\tvalid's binary_logloss: 1.04502e-05\n",
      "[6260]\tvalid's binary_logloss: 1.04328e-05\n",
      "[6270]\tvalid's binary_logloss: 1.04149e-05\n",
      "[6280]\tvalid's binary_logloss: 1.0397e-05\n",
      "[6290]\tvalid's binary_logloss: 1.038e-05\n",
      "[6300]\tvalid's binary_logloss: 1.03628e-05\n",
      "[6310]\tvalid's binary_logloss: 1.0346e-05\n",
      "[6320]\tvalid's binary_logloss: 1.033e-05\n",
      "[6330]\tvalid's binary_logloss: 1.0313e-05\n",
      "[6340]\tvalid's binary_logloss: 1.02974e-05\n",
      "[6350]\tvalid's binary_logloss: 1.02817e-05\n",
      "[6360]\tvalid's binary_logloss: 1.02643e-05\n",
      "[6370]\tvalid's binary_logloss: 1.02486e-05\n",
      "[6380]\tvalid's binary_logloss: 1.02315e-05\n",
      "[6390]\tvalid's binary_logloss: 1.02148e-05\n",
      "[6400]\tvalid's binary_logloss: 1.01982e-05\n",
      "[6410]\tvalid's binary_logloss: 1.01812e-05\n",
      "[6420]\tvalid's binary_logloss: 1.0165e-05\n",
      "[6430]\tvalid's binary_logloss: 1.01488e-05\n",
      "[6440]\tvalid's binary_logloss: 1.01327e-05\n",
      "[6450]\tvalid's binary_logloss: 1.01167e-05\n",
      "[6460]\tvalid's binary_logloss: 1.01012e-05\n",
      "[6470]\tvalid's binary_logloss: 1.00848e-05\n",
      "[6480]\tvalid's binary_logloss: 1.00692e-05\n",
      "[6490]\tvalid's binary_logloss: 1.00533e-05\n",
      "[6500]\tvalid's binary_logloss: 1.00375e-05\n",
      "[6510]\tvalid's binary_logloss: 1.00212e-05\n",
      "[6520]\tvalid's binary_logloss: 1.00049e-05\n",
      "[6530]\tvalid's binary_logloss: 9.98995e-06\n",
      "[6540]\tvalid's binary_logloss: 9.97415e-06\n",
      "[6550]\tvalid's binary_logloss: 9.95798e-06\n",
      "[6560]\tvalid's binary_logloss: 9.94255e-06\n",
      "[6570]\tvalid's binary_logloss: 9.92702e-06\n",
      "[6580]\tvalid's binary_logloss: 9.91111e-06\n",
      "[6590]\tvalid's binary_logloss: 9.89627e-06\n",
      "[6600]\tvalid's binary_logloss: 9.88147e-06\n",
      "[6610]\tvalid's binary_logloss: 9.86516e-06\n",
      "[6620]\tvalid's binary_logloss: 9.85027e-06\n",
      "[6630]\tvalid's binary_logloss: 9.83486e-06\n",
      "[6640]\tvalid's binary_logloss: 9.81947e-06\n",
      "[6650]\tvalid's binary_logloss: 9.80477e-06\n",
      "[6660]\tvalid's binary_logloss: 9.78987e-06\n",
      "[6670]\tvalid's binary_logloss: 9.77489e-06\n",
      "[6680]\tvalid's binary_logloss: 9.76027e-06\n",
      "[6690]\tvalid's binary_logloss: 9.74558e-06\n",
      "[6700]\tvalid's binary_logloss: 9.72991e-06\n",
      "[6710]\tvalid's binary_logloss: 9.71548e-06\n",
      "[6720]\tvalid's binary_logloss: 9.70054e-06\n",
      "[6730]\tvalid's binary_logloss: 9.68618e-06\n",
      "[6740]\tvalid's binary_logloss: 9.67167e-06\n",
      "[6750]\tvalid's binary_logloss: 9.6573e-06\n",
      "[6760]\tvalid's binary_logloss: 9.64274e-06\n",
      "[6770]\tvalid's binary_logloss: 9.6285e-06\n",
      "[6780]\tvalid's binary_logloss: 9.61317e-06\n",
      "[6790]\tvalid's binary_logloss: 9.59856e-06\n",
      "[6800]\tvalid's binary_logloss: 9.58423e-06\n",
      "[6810]\tvalid's binary_logloss: 9.56971e-06\n",
      "[6820]\tvalid's binary_logloss: 9.55585e-06\n",
      "[6830]\tvalid's binary_logloss: 9.54087e-06\n",
      "[6840]\tvalid's binary_logloss: 9.52652e-06\n",
      "[6850]\tvalid's binary_logloss: 9.51294e-06\n",
      "[6860]\tvalid's binary_logloss: 9.49895e-06\n",
      "[6870]\tvalid's binary_logloss: 9.48504e-06\n",
      "[6880]\tvalid's binary_logloss: 9.47124e-06\n",
      "[6890]\tvalid's binary_logloss: 9.45647e-06\n",
      "[6900]\tvalid's binary_logloss: 9.4422e-06\n",
      "[6910]\tvalid's binary_logloss: 9.42894e-06\n",
      "[6920]\tvalid's binary_logloss: 9.416e-06\n",
      "[6930]\tvalid's binary_logloss: 9.40169e-06\n",
      "[6940]\tvalid's binary_logloss: 9.38737e-06\n",
      "[6950]\tvalid's binary_logloss: 9.37365e-06\n",
      "[6960]\tvalid's binary_logloss: 9.36051e-06\n",
      "[6970]\tvalid's binary_logloss: 9.34632e-06\n",
      "[6980]\tvalid's binary_logloss: 9.33267e-06\n",
      "[6990]\tvalid's binary_logloss: 9.31968e-06\n",
      "[7000]\tvalid's binary_logloss: 9.30567e-06\n",
      "[7010]\tvalid's binary_logloss: 9.29233e-06\n",
      "[7020]\tvalid's binary_logloss: 9.27884e-06\n",
      "[7030]\tvalid's binary_logloss: 9.26497e-06\n",
      "[7040]\tvalid's binary_logloss: 9.25123e-06\n",
      "[7050]\tvalid's binary_logloss: 9.23741e-06\n",
      "[7060]\tvalid's binary_logloss: 9.22447e-06\n",
      "[7070]\tvalid's binary_logloss: 9.2112e-06\n",
      "[7080]\tvalid's binary_logloss: 9.19841e-06\n",
      "[7090]\tvalid's binary_logloss: 9.186e-06\n",
      "[7100]\tvalid's binary_logloss: 9.17299e-06\n",
      "[7110]\tvalid's binary_logloss: 9.1597e-06\n",
      "[7120]\tvalid's binary_logloss: 9.14633e-06\n",
      "[7130]\tvalid's binary_logloss: 9.13342e-06\n",
      "[7140]\tvalid's binary_logloss: 9.12059e-06\n",
      "[7150]\tvalid's binary_logloss: 9.10701e-06\n",
      "[7160]\tvalid's binary_logloss: 9.09399e-06\n",
      "[7170]\tvalid's binary_logloss: 9.08083e-06\n",
      "[7180]\tvalid's binary_logloss: 9.06859e-06\n",
      "[7190]\tvalid's binary_logloss: 9.05516e-06\n",
      "[7200]\tvalid's binary_logloss: 9.0422e-06\n",
      "[7210]\tvalid's binary_logloss: 9.0289e-06\n",
      "[7220]\tvalid's binary_logloss: 9.01596e-06\n",
      "[7230]\tvalid's binary_logloss: 9.00403e-06\n",
      "[7240]\tvalid's binary_logloss: 8.99109e-06\n",
      "[7250]\tvalid's binary_logloss: 8.97862e-06\n",
      "[7260]\tvalid's binary_logloss: 8.96662e-06\n",
      "[7270]\tvalid's binary_logloss: 8.95384e-06\n",
      "[7280]\tvalid's binary_logloss: 8.94233e-06\n",
      "[7290]\tvalid's binary_logloss: 8.92942e-06\n",
      "[7300]\tvalid's binary_logloss: 8.91708e-06\n",
      "[7310]\tvalid's binary_logloss: 8.9046e-06\n",
      "[7320]\tvalid's binary_logloss: 8.8921e-06\n",
      "[7330]\tvalid's binary_logloss: 8.8796e-06\n",
      "[7340]\tvalid's binary_logloss: 8.86722e-06\n",
      "[7350]\tvalid's binary_logloss: 8.85499e-06\n",
      "[7360]\tvalid's binary_logloss: 8.84298e-06\n",
      "[7370]\tvalid's binary_logloss: 8.83128e-06\n",
      "[7380]\tvalid's binary_logloss: 8.81933e-06\n",
      "[7390]\tvalid's binary_logloss: 8.80653e-06\n",
      "[7400]\tvalid's binary_logloss: 8.79467e-06\n",
      "[7410]\tvalid's binary_logloss: 8.78269e-06\n",
      "[7420]\tvalid's binary_logloss: 8.77138e-06\n",
      "[7430]\tvalid's binary_logloss: 8.75978e-06\n",
      "[7440]\tvalid's binary_logloss: 8.74793e-06\n",
      "[7450]\tvalid's binary_logloss: 8.73559e-06\n",
      "[7460]\tvalid's binary_logloss: 8.72406e-06\n",
      "[7470]\tvalid's binary_logloss: 8.71235e-06\n",
      "[7480]\tvalid's binary_logloss: 8.70096e-06\n",
      "[7490]\tvalid's binary_logloss: 8.68983e-06\n",
      "[7500]\tvalid's binary_logloss: 8.67874e-06\n",
      "[7510]\tvalid's binary_logloss: 8.66747e-06\n",
      "[7520]\tvalid's binary_logloss: 8.65506e-06\n",
      "[7530]\tvalid's binary_logloss: 8.64297e-06\n",
      "[7540]\tvalid's binary_logloss: 8.63186e-06\n",
      "[7550]\tvalid's binary_logloss: 8.62038e-06\n",
      "[7560]\tvalid's binary_logloss: 8.60893e-06\n",
      "[7570]\tvalid's binary_logloss: 8.59666e-06\n",
      "[7580]\tvalid's binary_logloss: 8.58573e-06\n",
      "[7590]\tvalid's binary_logloss: 8.57412e-06\n",
      "[7600]\tvalid's binary_logloss: 8.56276e-06\n",
      "[7610]\tvalid's binary_logloss: 8.5508e-06\n",
      "[7620]\tvalid's binary_logloss: 8.54003e-06\n",
      "[7630]\tvalid's binary_logloss: 8.52885e-06\n",
      "[7640]\tvalid's binary_logloss: 8.51829e-06\n",
      "[7650]\tvalid's binary_logloss: 8.50762e-06\n",
      "[7660]\tvalid's binary_logloss: 8.49643e-06\n",
      "[7670]\tvalid's binary_logloss: 8.48536e-06\n",
      "[7680]\tvalid's binary_logloss: 8.47425e-06\n",
      "[7690]\tvalid's binary_logloss: 8.4635e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7700]\tvalid's binary_logloss: 8.45231e-06\n",
      "[7710]\tvalid's binary_logloss: 8.44106e-06\n",
      "[7720]\tvalid's binary_logloss: 8.42972e-06\n",
      "[7730]\tvalid's binary_logloss: 8.41812e-06\n",
      "[7740]\tvalid's binary_logloss: 8.40768e-06\n",
      "[7750]\tvalid's binary_logloss: 8.3971e-06\n",
      "[7760]\tvalid's binary_logloss: 8.3863e-06\n",
      "[7770]\tvalid's binary_logloss: 8.37521e-06\n",
      "[7780]\tvalid's binary_logloss: 8.36428e-06\n",
      "[7790]\tvalid's binary_logloss: 8.35339e-06\n",
      "[7800]\tvalid's binary_logloss: 8.34305e-06\n",
      "[7810]\tvalid's binary_logloss: 8.33261e-06\n",
      "[7820]\tvalid's binary_logloss: 8.32171e-06\n",
      "[7830]\tvalid's binary_logloss: 8.31064e-06\n",
      "[7840]\tvalid's binary_logloss: 8.30048e-06\n",
      "[7850]\tvalid's binary_logloss: 8.29011e-06\n",
      "[7860]\tvalid's binary_logloss: 8.27929e-06\n",
      "[7870]\tvalid's binary_logloss: 8.26876e-06\n",
      "[7880]\tvalid's binary_logloss: 8.25782e-06\n",
      "[7890]\tvalid's binary_logloss: 8.2473e-06\n",
      "[7900]\tvalid's binary_logloss: 8.23691e-06\n",
      "[7910]\tvalid's binary_logloss: 8.22705e-06\n",
      "[7920]\tvalid's binary_logloss: 8.21614e-06\n",
      "[7930]\tvalid's binary_logloss: 8.20572e-06\n",
      "[7940]\tvalid's binary_logloss: 8.19537e-06\n",
      "[7950]\tvalid's binary_logloss: 8.18511e-06\n",
      "[7960]\tvalid's binary_logloss: 8.17509e-06\n",
      "[7970]\tvalid's binary_logloss: 8.16478e-06\n",
      "[7980]\tvalid's binary_logloss: 8.15479e-06\n",
      "[7990]\tvalid's binary_logloss: 8.14441e-06\n",
      "[8000]\tvalid's binary_logloss: 8.13443e-06\n",
      "[8010]\tvalid's binary_logloss: 8.12452e-06\n",
      "[8020]\tvalid's binary_logloss: 8.11442e-06\n",
      "[8030]\tvalid's binary_logloss: 8.10418e-06\n",
      "[8040]\tvalid's binary_logloss: 8.09404e-06\n",
      "[8050]\tvalid's binary_logloss: 8.08461e-06\n",
      "[8060]\tvalid's binary_logloss: 8.07459e-06\n",
      "[8070]\tvalid's binary_logloss: 8.06494e-06\n",
      "[8080]\tvalid's binary_logloss: 8.05563e-06\n",
      "[8090]\tvalid's binary_logloss: 8.04535e-06\n",
      "[8100]\tvalid's binary_logloss: 8.03566e-06\n",
      "[8110]\tvalid's binary_logloss: 8.02626e-06\n",
      "[8120]\tvalid's binary_logloss: 8.01628e-06\n",
      "[8130]\tvalid's binary_logloss: 8.00643e-06\n",
      "[8140]\tvalid's binary_logloss: 7.99656e-06\n",
      "[8150]\tvalid's binary_logloss: 7.98683e-06\n",
      "[8160]\tvalid's binary_logloss: 7.97688e-06\n",
      "[8170]\tvalid's binary_logloss: 7.9672e-06\n",
      "[8180]\tvalid's binary_logloss: 7.9578e-06\n",
      "[8190]\tvalid's binary_logloss: 7.94749e-06\n",
      "[8200]\tvalid's binary_logloss: 7.93798e-06\n",
      "[8210]\tvalid's binary_logloss: 7.92802e-06\n",
      "[8220]\tvalid's binary_logloss: 7.91901e-06\n",
      "[8230]\tvalid's binary_logloss: 7.90969e-06\n",
      "[8240]\tvalid's binary_logloss: 7.89974e-06\n",
      "[8250]\tvalid's binary_logloss: 7.89047e-06\n",
      "[8260]\tvalid's binary_logloss: 7.88096e-06\n",
      "[8270]\tvalid's binary_logloss: 7.8712e-06\n",
      "[8280]\tvalid's binary_logloss: 7.86207e-06\n",
      "[8290]\tvalid's binary_logloss: 7.8526e-06\n",
      "[8300]\tvalid's binary_logloss: 7.84322e-06\n",
      "[8310]\tvalid's binary_logloss: 7.83361e-06\n",
      "[8320]\tvalid's binary_logloss: 7.82455e-06\n",
      "[8330]\tvalid's binary_logloss: 7.81566e-06\n",
      "[8340]\tvalid's binary_logloss: 7.80625e-06\n",
      "[8350]\tvalid's binary_logloss: 7.79663e-06\n",
      "[8360]\tvalid's binary_logloss: 7.78724e-06\n",
      "[8370]\tvalid's binary_logloss: 7.77823e-06\n",
      "[8380]\tvalid's binary_logloss: 7.76921e-06\n",
      "[8390]\tvalid's binary_logloss: 7.75986e-06\n",
      "[8400]\tvalid's binary_logloss: 7.75081e-06\n",
      "[8410]\tvalid's binary_logloss: 7.74115e-06\n",
      "[8420]\tvalid's binary_logloss: 7.73208e-06\n",
      "[8430]\tvalid's binary_logloss: 7.72335e-06\n",
      "[8440]\tvalid's binary_logloss: 7.7145e-06\n",
      "[8450]\tvalid's binary_logloss: 7.70526e-06\n",
      "[8460]\tvalid's binary_logloss: 7.6962e-06\n",
      "[8470]\tvalid's binary_logloss: 7.68752e-06\n",
      "[8480]\tvalid's binary_logloss: 7.67856e-06\n",
      "[8490]\tvalid's binary_logloss: 7.66929e-06\n",
      "[8500]\tvalid's binary_logloss: 7.66084e-06\n",
      "[8510]\tvalid's binary_logloss: 7.65146e-06\n",
      "[8520]\tvalid's binary_logloss: 7.64249e-06\n",
      "[8530]\tvalid's binary_logloss: 7.63381e-06\n",
      "[8540]\tvalid's binary_logloss: 7.62483e-06\n",
      "[8550]\tvalid's binary_logloss: 7.61597e-06\n",
      "[8560]\tvalid's binary_logloss: 7.60737e-06\n",
      "[8570]\tvalid's binary_logloss: 7.59848e-06\n",
      "[8580]\tvalid's binary_logloss: 7.5901e-06\n",
      "[8590]\tvalid's binary_logloss: 7.58134e-06\n",
      "[8600]\tvalid's binary_logloss: 7.57261e-06\n",
      "[8610]\tvalid's binary_logloss: 7.56403e-06\n",
      "[8620]\tvalid's binary_logloss: 7.5551e-06\n",
      "[8630]\tvalid's binary_logloss: 7.54663e-06\n",
      "[8640]\tvalid's binary_logloss: 7.53781e-06\n",
      "[8650]\tvalid's binary_logloss: 7.52927e-06\n",
      "[8660]\tvalid's binary_logloss: 7.52009e-06\n",
      "[8670]\tvalid's binary_logloss: 7.51182e-06\n",
      "[8680]\tvalid's binary_logloss: 7.50341e-06\n",
      "[8690]\tvalid's binary_logloss: 7.49518e-06\n",
      "[8700]\tvalid's binary_logloss: 7.48682e-06\n",
      "[8710]\tvalid's binary_logloss: 7.4777e-06\n",
      "[8720]\tvalid's binary_logloss: 7.46948e-06\n",
      "[8730]\tvalid's binary_logloss: 7.46077e-06\n",
      "[8740]\tvalid's binary_logloss: 7.45208e-06\n",
      "[8750]\tvalid's binary_logloss: 7.44401e-06\n",
      "[8760]\tvalid's binary_logloss: 7.43576e-06\n",
      "[8770]\tvalid's binary_logloss: 7.42715e-06\n",
      "[8780]\tvalid's binary_logloss: 7.4186e-06\n",
      "[8790]\tvalid's binary_logloss: 7.41026e-06\n",
      "[8800]\tvalid's binary_logloss: 7.40214e-06\n",
      "[8810]\tvalid's binary_logloss: 7.39369e-06\n",
      "[8820]\tvalid's binary_logloss: 7.3854e-06\n",
      "[8830]\tvalid's binary_logloss: 7.37679e-06\n",
      "[8840]\tvalid's binary_logloss: 7.3692e-06\n",
      "[8850]\tvalid's binary_logloss: 7.36067e-06\n",
      "[8860]\tvalid's binary_logloss: 7.35211e-06\n",
      "[8870]\tvalid's binary_logloss: 7.34401e-06\n",
      "[8880]\tvalid's binary_logloss: 7.33608e-06\n",
      "[8890]\tvalid's binary_logloss: 7.32757e-06\n",
      "[8900]\tvalid's binary_logloss: 7.31936e-06\n",
      "[8910]\tvalid's binary_logloss: 7.31144e-06\n",
      "[8920]\tvalid's binary_logloss: 7.30373e-06\n",
      "[8930]\tvalid's binary_logloss: 7.29557e-06\n",
      "[8940]\tvalid's binary_logloss: 7.28789e-06\n",
      "[8950]\tvalid's binary_logloss: 7.28019e-06\n",
      "[8960]\tvalid's binary_logloss: 7.27249e-06\n",
      "[8970]\tvalid's binary_logloss: 7.26448e-06\n",
      "[8980]\tvalid's binary_logloss: 7.25606e-06\n",
      "[8990]\tvalid's binary_logloss: 7.24775e-06\n",
      "[9000]\tvalid's binary_logloss: 7.23973e-06\n",
      "[9010]\tvalid's binary_logloss: 7.23156e-06\n",
      "[9020]\tvalid's binary_logloss: 7.22339e-06\n",
      "[9030]\tvalid's binary_logloss: 7.21539e-06\n",
      "[9040]\tvalid's binary_logloss: 7.20727e-06\n",
      "[9050]\tvalid's binary_logloss: 7.19994e-06\n",
      "[9060]\tvalid's binary_logloss: 7.19153e-06\n",
      "[9070]\tvalid's binary_logloss: 7.18372e-06\n",
      "[9080]\tvalid's binary_logloss: 7.17648e-06\n",
      "[9090]\tvalid's binary_logloss: 7.16853e-06\n",
      "[9100]\tvalid's binary_logloss: 7.1603e-06\n",
      "[9110]\tvalid's binary_logloss: 7.15268e-06\n",
      "[9120]\tvalid's binary_logloss: 7.14502e-06\n",
      "[9130]\tvalid's binary_logloss: 7.13717e-06\n",
      "[9140]\tvalid's binary_logloss: 7.12942e-06\n",
      "[9150]\tvalid's binary_logloss: 7.12192e-06\n",
      "[9160]\tvalid's binary_logloss: 7.11433e-06\n",
      "[9170]\tvalid's binary_logloss: 7.10687e-06\n",
      "[9180]\tvalid's binary_logloss: 7.09942e-06\n",
      "[9190]\tvalid's binary_logloss: 7.09186e-06\n",
      "[9200]\tvalid's binary_logloss: 7.08447e-06\n",
      "[9210]\tvalid's binary_logloss: 7.07654e-06\n",
      "[9220]\tvalid's binary_logloss: 7.06879e-06\n",
      "[9230]\tvalid's binary_logloss: 7.06097e-06\n",
      "[9240]\tvalid's binary_logloss: 7.05362e-06\n",
      "[9250]\tvalid's binary_logloss: 7.0457e-06\n",
      "[9260]\tvalid's binary_logloss: 7.03838e-06\n",
      "[9270]\tvalid's binary_logloss: 7.03097e-06\n",
      "[9280]\tvalid's binary_logloss: 7.02315e-06\n",
      "[9290]\tvalid's binary_logloss: 7.01606e-06\n",
      "[9300]\tvalid's binary_logloss: 7.00893e-06\n",
      "[9310]\tvalid's binary_logloss: 7.00118e-06\n",
      "[9320]\tvalid's binary_logloss: 6.99377e-06\n",
      "[9330]\tvalid's binary_logloss: 6.98641e-06\n",
      "[9340]\tvalid's binary_logloss: 6.97881e-06\n",
      "[9350]\tvalid's binary_logloss: 6.97156e-06\n",
      "[9360]\tvalid's binary_logloss: 6.96422e-06\n",
      "[9370]\tvalid's binary_logloss: 6.95647e-06\n",
      "[9380]\tvalid's binary_logloss: 6.94956e-06\n",
      "[9390]\tvalid's binary_logloss: 6.94202e-06\n",
      "[9400]\tvalid's binary_logloss: 6.93485e-06\n",
      "[9410]\tvalid's binary_logloss: 6.92737e-06\n",
      "[9420]\tvalid's binary_logloss: 6.92009e-06\n",
      "[9430]\tvalid's binary_logloss: 6.91256e-06\n",
      "[9440]\tvalid's binary_logloss: 6.90568e-06\n",
      "[9450]\tvalid's binary_logloss: 6.89871e-06\n",
      "[9460]\tvalid's binary_logloss: 6.89126e-06\n",
      "[9470]\tvalid's binary_logloss: 6.88429e-06\n",
      "[9480]\tvalid's binary_logloss: 6.87695e-06\n",
      "[9490]\tvalid's binary_logloss: 6.86953e-06\n",
      "[9500]\tvalid's binary_logloss: 6.86243e-06\n",
      "[9510]\tvalid's binary_logloss: 6.85546e-06\n",
      "[9520]\tvalid's binary_logloss: 6.84816e-06\n",
      "[9530]\tvalid's binary_logloss: 6.84137e-06\n",
      "[9540]\tvalid's binary_logloss: 6.83407e-06\n",
      "[9550]\tvalid's binary_logloss: 6.82726e-06\n",
      "[9560]\tvalid's binary_logloss: 6.81996e-06\n",
      "[9570]\tvalid's binary_logloss: 6.81303e-06\n",
      "[9580]\tvalid's binary_logloss: 6.80627e-06\n",
      "[9590]\tvalid's binary_logloss: 6.79977e-06\n",
      "[9600]\tvalid's binary_logloss: 6.79201e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9610]\tvalid's binary_logloss: 6.78519e-06\n",
      "[9620]\tvalid's binary_logloss: 6.77799e-06\n",
      "[9630]\tvalid's binary_logloss: 6.77136e-06\n",
      "[9640]\tvalid's binary_logloss: 6.7642e-06\n",
      "[9650]\tvalid's binary_logloss: 6.75736e-06\n",
      "[9660]\tvalid's binary_logloss: 6.75073e-06\n",
      "[9670]\tvalid's binary_logloss: 6.74367e-06\n",
      "[9680]\tvalid's binary_logloss: 6.73664e-06\n",
      "[9690]\tvalid's binary_logloss: 6.72988e-06\n",
      "[9700]\tvalid's binary_logloss: 6.72315e-06\n",
      "[9710]\tvalid's binary_logloss: 6.71645e-06\n",
      "[9720]\tvalid's binary_logloss: 6.71005e-06\n",
      "[9730]\tvalid's binary_logloss: 6.70329e-06\n",
      "[9740]\tvalid's binary_logloss: 6.69643e-06\n",
      "[9750]\tvalid's binary_logloss: 6.68987e-06\n",
      "[9760]\tvalid's binary_logloss: 6.68333e-06\n",
      "[9770]\tvalid's binary_logloss: 6.67674e-06\n",
      "[9780]\tvalid's binary_logloss: 6.66996e-06\n",
      "[9790]\tvalid's binary_logloss: 6.66311e-06\n",
      "[9800]\tvalid's binary_logloss: 6.65629e-06\n",
      "[9810]\tvalid's binary_logloss: 6.64963e-06\n",
      "[9820]\tvalid's binary_logloss: 6.6433e-06\n",
      "[9830]\tvalid's binary_logloss: 6.63654e-06\n",
      "[9840]\tvalid's binary_logloss: 6.6302e-06\n",
      "[9850]\tvalid's binary_logloss: 6.62338e-06\n",
      "[9860]\tvalid's binary_logloss: 6.61695e-06\n",
      "[9870]\tvalid's binary_logloss: 6.61027e-06\n",
      "[9880]\tvalid's binary_logloss: 6.60403e-06\n",
      "[9890]\tvalid's binary_logloss: 6.59741e-06\n",
      "[9900]\tvalid's binary_logloss: 6.59087e-06\n",
      "[9910]\tvalid's binary_logloss: 6.5842e-06\n",
      "[9920]\tvalid's binary_logloss: 6.57789e-06\n",
      "[9930]\tvalid's binary_logloss: 6.57148e-06\n",
      "[9940]\tvalid's binary_logloss: 6.56518e-06\n",
      "[9950]\tvalid's binary_logloss: 6.55889e-06\n",
      "[9960]\tvalid's binary_logloss: 6.55249e-06\n",
      "[9970]\tvalid's binary_logloss: 6.54635e-06\n",
      "[9980]\tvalid's binary_logloss: 6.54011e-06\n",
      "[9990]\tvalid's binary_logloss: 6.5337e-06\n",
      "[10000]\tvalid's binary_logloss: 6.527e-06\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\tvalid's binary_logloss: 6.527e-06\n",
      "\n",
      "**************************************************\n",
      "Best score reached: 0.9274827690840522 with params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'metric': 'binary_logloss', 'min_chlid_wight': 1, 'n_estimators': 10000, 'objective': 'binary', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'subsamples': 0.8} \n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/mlisovyi/lightgbm-hyperparameter-optimisation-lb-0-761\n",
    "\n",
    "fit_params={\n",
    "    \"early_stopping_rounds\":30,\n",
    "    \"eval_set\" : [(X_train_valid, y_train_valid)],\n",
    "    'eval_names': ['valid'],\n",
    "    'verbose': 10\n",
    "    }\n",
    "\n",
    "param_grid ={\n",
    "    'boosting_type':['gbdt'], \n",
    "    'objective':['binary'],\n",
    "    \"metric\":[\"binary_logloss\"], \n",
    "    'n_estimators':[10000],\n",
    "    'random_state':[0],\n",
    "    'learning_rate':[0.1],\n",
    "    'max_depth':[3]#, 5, 7], #3~9\n",
    "    \"min_chlid_wight\":[1]#, 3, 5], #1~5\n",
    "    'gamma':[0], #0~0.4\n",
    "    'colsample_bytree':[0.8],\n",
    "    'subsamples':[0.8],\n",
    "    'reg_alpha':[0]#, 0.1, 1],\n",
    "    'reg_lambda':[1]#, 0.1, 0]\n",
    "    }\n",
    "\n",
    "clf = lgb.LGBMClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "#     cv=2,\n",
    "    cv=StratifiedKFold(n_splits=4, random_state=0, shuffle=True),\n",
    "    scoring='f1',\n",
    "    refit=True,\n",
    "    verbose=3\n",
    "    )\n",
    "\n",
    "grid_search.fit(X_train_valid, y_train_valid, **fit_params)\n",
    "\n",
    "print(\"\\n\"+\"*\"*50)\n",
    "print('Best score reached: {} with params: {} '.format(grid_search.best_score_, grid_search.best_params_))\n",
    "print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'colsample_bytree': 0.8,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'metric': 'binary_logloss',\n",
       " 'min_chlid_wight': 1,\n",
       " 'n_estimators': 10000,\n",
       " 'objective': 'binary',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'subsamples': 0.8}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'colsample_bytree': 0.8,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.01,\n",
       " 'max_depth': 3,\n",
       " 'metric': 'binary_logloss',\n",
       " 'min_chlid_wight': 1,\n",
       " 'n_estimators': 10000,\n",
       " 'objective': 'binary',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'subsamples': 0.8}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"learning_rate\"] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.3, random_state=0, stratify=y_train_valid)\n",
    "X_train.columns = range(X_train.shape[1])\n",
    "X_valid.columns = range(X_valid.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lgb.Dataset(X_train, label=y_train)\n",
    "valid = lgb.Dataset(X_valid, label=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[10]\ttraining's binary_logloss: 0.22652\tvalid_1's binary_logloss: 0.234078\n",
      "[20]\ttraining's binary_logloss: 0.199553\tvalid_1's binary_logloss: 0.212354\n",
      "[30]\ttraining's binary_logloss: 0.179654\tvalid_1's binary_logloss: 0.196726\n",
      "[40]\ttraining's binary_logloss: 0.164278\tvalid_1's binary_logloss: 0.184398\n",
      "[50]\ttraining's binary_logloss: 0.150852\tvalid_1's binary_logloss: 0.173079\n",
      "[60]\ttraining's binary_logloss: 0.139365\tvalid_1's binary_logloss: 0.163266\n",
      "[70]\ttraining's binary_logloss: 0.129398\tvalid_1's binary_logloss: 0.15429\n",
      "[80]\ttraining's binary_logloss: 0.120639\tvalid_1's binary_logloss: 0.146112\n",
      "[90]\ttraining's binary_logloss: 0.112782\tvalid_1's binary_logloss: 0.138592\n",
      "[100]\ttraining's binary_logloss: 0.105506\tvalid_1's binary_logloss: 0.131747\n",
      "[110]\ttraining's binary_logloss: 0.0989966\tvalid_1's binary_logloss: 0.125553\n",
      "[120]\ttraining's binary_logloss: 0.093138\tvalid_1's binary_logloss: 0.119965\n",
      "[130]\ttraining's binary_logloss: 0.0875601\tvalid_1's binary_logloss: 0.114711\n",
      "[140]\ttraining's binary_logloss: 0.0825873\tvalid_1's binary_logloss: 0.11005\n",
      "[150]\ttraining's binary_logloss: 0.0778578\tvalid_1's binary_logloss: 0.105614\n",
      "[160]\ttraining's binary_logloss: 0.0736776\tvalid_1's binary_logloss: 0.101606\n",
      "[170]\ttraining's binary_logloss: 0.0697621\tvalid_1's binary_logloss: 0.0979989\n",
      "[180]\ttraining's binary_logloss: 0.0661846\tvalid_1's binary_logloss: 0.0944853\n",
      "[190]\ttraining's binary_logloss: 0.0628545\tvalid_1's binary_logloss: 0.0914032\n",
      "[200]\ttraining's binary_logloss: 0.0597242\tvalid_1's binary_logloss: 0.0884191\n",
      "[210]\ttraining's binary_logloss: 0.0568943\tvalid_1's binary_logloss: 0.0858452\n",
      "[220]\ttraining's binary_logloss: 0.0542724\tvalid_1's binary_logloss: 0.0834748\n",
      "[230]\ttraining's binary_logloss: 0.0517772\tvalid_1's binary_logloss: 0.0813189\n",
      "[240]\ttraining's binary_logloss: 0.0494972\tvalid_1's binary_logloss: 0.0792029\n",
      "[250]\ttraining's binary_logloss: 0.0472254\tvalid_1's binary_logloss: 0.0771813\n",
      "[260]\ttraining's binary_logloss: 0.0451806\tvalid_1's binary_logloss: 0.0754099\n",
      "[270]\ttraining's binary_logloss: 0.0432213\tvalid_1's binary_logloss: 0.0735149\n",
      "[280]\ttraining's binary_logloss: 0.0414258\tvalid_1's binary_logloss: 0.0719164\n",
      "[290]\ttraining's binary_logloss: 0.0397661\tvalid_1's binary_logloss: 0.0703343\n",
      "[300]\ttraining's binary_logloss: 0.0381882\tvalid_1's binary_logloss: 0.0689183\n",
      "[310]\ttraining's binary_logloss: 0.0366435\tvalid_1's binary_logloss: 0.0674717\n",
      "[320]\ttraining's binary_logloss: 0.0352623\tvalid_1's binary_logloss: 0.0662744\n",
      "[330]\ttraining's binary_logloss: 0.0339698\tvalid_1's binary_logloss: 0.0651584\n",
      "[340]\ttraining's binary_logloss: 0.0327533\tvalid_1's binary_logloss: 0.0641772\n",
      "[350]\ttraining's binary_logloss: 0.0315369\tvalid_1's binary_logloss: 0.0631563\n",
      "[360]\ttraining's binary_logloss: 0.0304251\tvalid_1's binary_logloss: 0.0621871\n",
      "[370]\ttraining's binary_logloss: 0.0293557\tvalid_1's binary_logloss: 0.0613202\n",
      "[380]\ttraining's binary_logloss: 0.0283484\tvalid_1's binary_logloss: 0.0605635\n",
      "[390]\ttraining's binary_logloss: 0.0273767\tvalid_1's binary_logloss: 0.0597427\n",
      "[400]\ttraining's binary_logloss: 0.0264716\tvalid_1's binary_logloss: 0.0588336\n",
      "[410]\ttraining's binary_logloss: 0.0255961\tvalid_1's binary_logloss: 0.0580422\n",
      "[420]\ttraining's binary_logloss: 0.0247737\tvalid_1's binary_logloss: 0.0573979\n",
      "[430]\ttraining's binary_logloss: 0.0240095\tvalid_1's binary_logloss: 0.0568798\n",
      "[440]\ttraining's binary_logloss: 0.0232289\tvalid_1's binary_logloss: 0.0562587\n",
      "[450]\ttraining's binary_logloss: 0.0225256\tvalid_1's binary_logloss: 0.0557999\n",
      "[460]\ttraining's binary_logloss: 0.0218267\tvalid_1's binary_logloss: 0.0551568\n",
      "[470]\ttraining's binary_logloss: 0.0211423\tvalid_1's binary_logloss: 0.0545362\n",
      "[480]\ttraining's binary_logloss: 0.0204636\tvalid_1's binary_logloss: 0.0539955\n",
      "[490]\ttraining's binary_logloss: 0.0198286\tvalid_1's binary_logloss: 0.0534253\n",
      "[500]\ttraining's binary_logloss: 0.0192277\tvalid_1's binary_logloss: 0.0529059\n",
      "[510]\ttraining's binary_logloss: 0.018678\tvalid_1's binary_logloss: 0.0524548\n",
      "[520]\ttraining's binary_logloss: 0.0181347\tvalid_1's binary_logloss: 0.0518958\n",
      "[530]\ttraining's binary_logloss: 0.0175981\tvalid_1's binary_logloss: 0.0515284\n",
      "[540]\ttraining's binary_logloss: 0.0170741\tvalid_1's binary_logloss: 0.0511571\n",
      "[550]\ttraining's binary_logloss: 0.0165846\tvalid_1's binary_logloss: 0.050732\n",
      "[560]\ttraining's binary_logloss: 0.0161268\tvalid_1's binary_logloss: 0.0503534\n",
      "[570]\ttraining's binary_logloss: 0.0156627\tvalid_1's binary_logloss: 0.0499941\n",
      "[580]\ttraining's binary_logloss: 0.0152605\tvalid_1's binary_logloss: 0.0495979\n",
      "[590]\ttraining's binary_logloss: 0.014824\tvalid_1's binary_logloss: 0.0492912\n",
      "[600]\ttraining's binary_logloss: 0.0144267\tvalid_1's binary_logloss: 0.04893\n",
      "[610]\ttraining's binary_logloss: 0.0140449\tvalid_1's binary_logloss: 0.0485327\n",
      "[620]\ttraining's binary_logloss: 0.0136912\tvalid_1's binary_logloss: 0.0481858\n",
      "[630]\ttraining's binary_logloss: 0.0133359\tvalid_1's binary_logloss: 0.0480332\n",
      "[640]\ttraining's binary_logloss: 0.0129641\tvalid_1's binary_logloss: 0.0477873\n",
      "[650]\ttraining's binary_logloss: 0.0126176\tvalid_1's binary_logloss: 0.0475071\n",
      "[660]\ttraining's binary_logloss: 0.0123043\tvalid_1's binary_logloss: 0.0472183\n",
      "[670]\ttraining's binary_logloss: 0.0119933\tvalid_1's binary_logloss: 0.0469787\n",
      "[680]\ttraining's binary_logloss: 0.0116597\tvalid_1's binary_logloss: 0.0467434\n",
      "[690]\ttraining's binary_logloss: 0.0113384\tvalid_1's binary_logloss: 0.0465075\n",
      "[700]\ttraining's binary_logloss: 0.0110418\tvalid_1's binary_logloss: 0.0463003\n",
      "[710]\ttraining's binary_logloss: 0.0107533\tvalid_1's binary_logloss: 0.046128\n",
      "[720]\ttraining's binary_logloss: 0.0104877\tvalid_1's binary_logloss: 0.0459131\n",
      "[730]\ttraining's binary_logloss: 0.0102449\tvalid_1's binary_logloss: 0.045565\n",
      "[740]\ttraining's binary_logloss: 0.00998659\tvalid_1's binary_logloss: 0.0453942\n",
      "[750]\ttraining's binary_logloss: 0.00973127\tvalid_1's binary_logloss: 0.0452436\n",
      "[760]\ttraining's binary_logloss: 0.0094931\tvalid_1's binary_logloss: 0.0450328\n",
      "[770]\ttraining's binary_logloss: 0.00926628\tvalid_1's binary_logloss: 0.0448811\n",
      "[780]\ttraining's binary_logloss: 0.00905488\tvalid_1's binary_logloss: 0.0447237\n",
      "[790]\ttraining's binary_logloss: 0.00882687\tvalid_1's binary_logloss: 0.0445656\n",
      "[800]\ttraining's binary_logloss: 0.00861112\tvalid_1's binary_logloss: 0.0444686\n",
      "[810]\ttraining's binary_logloss: 0.0083966\tvalid_1's binary_logloss: 0.0443431\n",
      "[820]\ttraining's binary_logloss: 0.00819799\tvalid_1's binary_logloss: 0.044243\n",
      "[830]\ttraining's binary_logloss: 0.0080046\tvalid_1's binary_logloss: 0.0441616\n",
      "[840]\ttraining's binary_logloss: 0.00781627\tvalid_1's binary_logloss: 0.0440324\n",
      "[850]\ttraining's binary_logloss: 0.00763451\tvalid_1's binary_logloss: 0.0439388\n",
      "[860]\ttraining's binary_logloss: 0.00744409\tvalid_1's binary_logloss: 0.0438168\n",
      "[870]\ttraining's binary_logloss: 0.0072733\tvalid_1's binary_logloss: 0.043702\n",
      "[880]\ttraining's binary_logloss: 0.00710068\tvalid_1's binary_logloss: 0.0437053\n",
      "[890]\ttraining's binary_logloss: 0.00694494\tvalid_1's binary_logloss: 0.0436541\n",
      "[900]\ttraining's binary_logloss: 0.00677947\tvalid_1's binary_logloss: 0.0434906\n",
      "[910]\ttraining's binary_logloss: 0.00662624\tvalid_1's binary_logloss: 0.0434576\n",
      "[920]\ttraining's binary_logloss: 0.00648165\tvalid_1's binary_logloss: 0.0433571\n",
      "[930]\ttraining's binary_logloss: 0.00635078\tvalid_1's binary_logloss: 0.0432581\n",
      "[940]\ttraining's binary_logloss: 0.00621613\tvalid_1's binary_logloss: 0.0431292\n",
      "[950]\ttraining's binary_logloss: 0.00608698\tvalid_1's binary_logloss: 0.0430718\n",
      "[960]\ttraining's binary_logloss: 0.00596853\tvalid_1's binary_logloss: 0.04298\n",
      "[970]\ttraining's binary_logloss: 0.00584145\tvalid_1's binary_logloss: 0.0430585\n",
      "[980]\ttraining's binary_logloss: 0.00571728\tvalid_1's binary_logloss: 0.0429707\n",
      "[990]\ttraining's binary_logloss: 0.00560808\tvalid_1's binary_logloss: 0.042899\n",
      "[1000]\ttraining's binary_logloss: 0.00548562\tvalid_1's binary_logloss: 0.0427815\n",
      "[1010]\ttraining's binary_logloss: 0.00537242\tvalid_1's binary_logloss: 0.0427251\n",
      "[1020]\ttraining's binary_logloss: 0.00526284\tvalid_1's binary_logloss: 0.0426135\n",
      "[1030]\ttraining's binary_logloss: 0.00514726\tvalid_1's binary_logloss: 0.0425729\n",
      "[1040]\ttraining's binary_logloss: 0.00503866\tvalid_1's binary_logloss: 0.0424891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1050]\ttraining's binary_logloss: 0.00493468\tvalid_1's binary_logloss: 0.0424616\n",
      "[1060]\ttraining's binary_logloss: 0.00483883\tvalid_1's binary_logloss: 0.0424234\n",
      "[1070]\ttraining's binary_logloss: 0.00474168\tvalid_1's binary_logloss: 0.0423182\n",
      "[1080]\ttraining's binary_logloss: 0.00464998\tvalid_1's binary_logloss: 0.042256\n",
      "[1090]\ttraining's binary_logloss: 0.00455937\tvalid_1's binary_logloss: 0.0422088\n",
      "[1100]\ttraining's binary_logloss: 0.0044647\tvalid_1's binary_logloss: 0.0420855\n",
      "[1110]\ttraining's binary_logloss: 0.0043789\tvalid_1's binary_logloss: 0.0420239\n",
      "[1120]\ttraining's binary_logloss: 0.00429219\tvalid_1's binary_logloss: 0.041965\n",
      "[1130]\ttraining's binary_logloss: 0.00420782\tvalid_1's binary_logloss: 0.0418813\n",
      "[1140]\ttraining's binary_logloss: 0.00412552\tvalid_1's binary_logloss: 0.0418454\n",
      "[1150]\ttraining's binary_logloss: 0.00404461\tvalid_1's binary_logloss: 0.0418025\n",
      "[1160]\ttraining's binary_logloss: 0.00396941\tvalid_1's binary_logloss: 0.0417058\n",
      "[1170]\ttraining's binary_logloss: 0.00389315\tvalid_1's binary_logloss: 0.0416012\n",
      "[1180]\ttraining's binary_logloss: 0.00382117\tvalid_1's binary_logloss: 0.0415138\n",
      "[1190]\ttraining's binary_logloss: 0.00375136\tvalid_1's binary_logloss: 0.0415119\n",
      "[1200]\ttraining's binary_logloss: 0.0036815\tvalid_1's binary_logloss: 0.0414681\n",
      "[1210]\ttraining's binary_logloss: 0.00361205\tvalid_1's binary_logloss: 0.0414391\n",
      "[1220]\ttraining's binary_logloss: 0.00354298\tvalid_1's binary_logloss: 0.041348\n",
      "[1230]\ttraining's binary_logloss: 0.00347866\tvalid_1's binary_logloss: 0.0413511\n",
      "[1240]\ttraining's binary_logloss: 0.00341075\tvalid_1's binary_logloss: 0.0413072\n",
      "[1250]\ttraining's binary_logloss: 0.00335069\tvalid_1's binary_logloss: 0.0412167\n",
      "[1260]\ttraining's binary_logloss: 0.00329087\tvalid_1's binary_logloss: 0.0411506\n",
      "[1270]\ttraining's binary_logloss: 0.00322994\tvalid_1's binary_logloss: 0.0411745\n",
      "[1280]\ttraining's binary_logloss: 0.00317326\tvalid_1's binary_logloss: 0.0410946\n",
      "[1290]\ttraining's binary_logloss: 0.00311476\tvalid_1's binary_logloss: 0.0410338\n",
      "[1300]\ttraining's binary_logloss: 0.00306053\tvalid_1's binary_logloss: 0.0410165\n",
      "[1310]\ttraining's binary_logloss: 0.00300789\tvalid_1's binary_logloss: 0.0409708\n",
      "[1320]\ttraining's binary_logloss: 0.00295779\tvalid_1's binary_logloss: 0.0409694\n",
      "[1330]\ttraining's binary_logloss: 0.00291118\tvalid_1's binary_logloss: 0.0409342\n",
      "[1340]\ttraining's binary_logloss: 0.00286131\tvalid_1's binary_logloss: 0.0409331\n",
      "[1350]\ttraining's binary_logloss: 0.00281503\tvalid_1's binary_logloss: 0.0409074\n",
      "[1360]\ttraining's binary_logloss: 0.00276759\tvalid_1's binary_logloss: 0.040886\n",
      "[1370]\ttraining's binary_logloss: 0.00272415\tvalid_1's binary_logloss: 0.0408497\n",
      "[1380]\ttraining's binary_logloss: 0.00268136\tvalid_1's binary_logloss: 0.0407872\n",
      "[1390]\ttraining's binary_logloss: 0.00263577\tvalid_1's binary_logloss: 0.0407549\n",
      "[1400]\ttraining's binary_logloss: 0.0025929\tvalid_1's binary_logloss: 0.0407579\n",
      "[1410]\ttraining's binary_logloss: 0.00255043\tvalid_1's binary_logloss: 0.0407044\n",
      "[1420]\ttraining's binary_logloss: 0.00251025\tvalid_1's binary_logloss: 0.0407035\n",
      "[1430]\ttraining's binary_logloss: 0.00247019\tvalid_1's binary_logloss: 0.0407129\n",
      "[1440]\ttraining's binary_logloss: 0.00243203\tvalid_1's binary_logloss: 0.0407028\n",
      "Early stopping, best iteration is:\n",
      "[1416]\ttraining's binary_logloss: 0.0025263\tvalid_1's binary_logloss: 0.040679\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(params=params,\n",
    "                  train_set=train,\n",
    "                  valid_sets=[train, valid],\n",
    "                  num_boost_round=1000,\n",
    "                  early_stopping_rounds=30,\n",
    "                  verbose_eval=10,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_valid_pred = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 376
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_valid_pred, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 251,
       "width": 376
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_valid_pred, bins=100, range=(0, 0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(y_valid_pred, threshold):\n",
    "    print(\"-\"*30)\n",
    "    print(\"threshold : \" + str(threshold))\n",
    "    print('acc : ' + str(accuracy_score(y_valid==0, (y_valid_pred)<threshold)))\n",
    "    print('recall : ' + str(recall_score(y_valid==0, (y_valid_pred)<threshold)))\n",
    "    print('precision : ' + str(precision_score(y_valid==0, (y_valid_pred)<threshold)))\n",
    "    print('f1 : ' + str(f1_score(y_valid==0, (y_valid_pred)<threshold)))\n",
    "    cm = confusion_matrix(y_valid==0, (y_valid_pred)<threshold)\n",
    "    print(cm)\n",
    "    return f1_score(y_valid==0, (y_valid_pred)<threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "threshold : 0.0\n",
      "acc : 0.07645875251509054\n",
      "recall : 0.0\n",
      "precision : 0.0\n",
      "f1 : 0.0\n",
      "[[ 76   0]\n",
      " [918   0]]\n",
      "------------------------------\n",
      "threshold : 0.01\n",
      "acc : 0.9517102615694165\n",
      "recall : 0.94880174291939\n",
      "precision : 0.9988532110091743\n",
      "f1 : 0.9731843575418995\n",
      "[[ 75   1]\n",
      " [ 47 871]]\n",
      "------------------------------\n",
      "threshold : 0.02\n",
      "acc : 0.9728370221327968\n",
      "recall : 0.9738562091503268\n",
      "precision : 0.9966555183946488\n",
      "f1 : 0.9851239669421488\n",
      "[[ 73   3]\n",
      " [ 24 894]]\n",
      "------------------------------\n",
      "threshold : 0.03\n",
      "acc : 0.9788732394366197\n",
      "recall : 0.9814814814814815\n",
      "precision : 0.9955801104972376\n",
      "f1 : 0.988480526604498\n",
      "[[ 72   4]\n",
      " [ 17 901]]\n",
      "------------------------------\n",
      "threshold : 0.04\n",
      "acc : 0.9828973843058351\n",
      "recall : 0.985838779956427\n",
      "precision : 0.9955995599559956\n",
      "f1 : 0.9906951286261632\n",
      "[[ 72   4]\n",
      " [ 13 905]]\n",
      "------------------------------\n",
      "threshold : 0.05\n",
      "acc : 0.9849094567404426\n",
      "recall : 0.9901960784313726\n",
      "precision : 0.9934426229508196\n",
      "f1 : 0.9918166939443536\n",
      "[[ 70   6]\n",
      " [  9 909]]\n",
      "------------------------------\n",
      "threshold : 0.06\n",
      "acc : 0.9879275653923542\n",
      "recall : 0.9934640522875817\n",
      "precision : 0.9934640522875817\n",
      "f1 : 0.9934640522875817\n",
      "[[ 70   6]\n",
      " [  6 912]]\n",
      "------------------------------\n",
      "threshold : 0.07\n",
      "acc : 0.9879275653923542\n",
      "recall : 0.9934640522875817\n",
      "precision : 0.9934640522875817\n",
      "f1 : 0.9934640522875817\n",
      "[[ 70   6]\n",
      " [  6 912]]\n",
      "------------------------------\n",
      "threshold : 0.08\n",
      "acc : 0.9879275653923542\n",
      "recall : 0.9934640522875817\n",
      "precision : 0.9934640522875817\n",
      "f1 : 0.9934640522875817\n",
      "[[ 70   6]\n",
      " [  6 912]]\n",
      "------------------------------\n",
      "threshold : 0.09\n",
      "acc : 0.9889336016096579\n",
      "recall : 0.9945533769063181\n",
      "precision : 0.9934711643090316\n",
      "f1 : 0.9940119760479041\n",
      "[[ 70   6]\n",
      " [  5 913]]\n",
      "------------------------------\n",
      "threshold : 0.1\n",
      "acc : 0.9899396378269618\n",
      "recall : 0.9956427015250545\n",
      "precision : 0.9934782608695653\n",
      "f1 : 0.9945593035908598\n",
      "[[ 70   6]\n",
      " [  4 914]]\n",
      "------------------------------\n",
      "threshold : 0.11\n",
      "acc : 0.9919517102615694\n",
      "recall : 0.9978213507625272\n",
      "precision : 0.9934924078091106\n",
      "f1 : 0.9956521739130435\n",
      "[[ 70   6]\n",
      " [  2 916]]\n",
      "------------------------------\n",
      "threshold : 0.12\n",
      "acc : 0.9919517102615694\n",
      "recall : 0.9978213507625272\n",
      "precision : 0.9934924078091106\n",
      "f1 : 0.9956521739130435\n",
      "[[ 70   6]\n",
      " [  2 916]]\n",
      "------------------------------\n",
      "threshold : 0.13\n",
      "acc : 0.9919517102615694\n",
      "recall : 0.9978213507625272\n",
      "precision : 0.9934924078091106\n",
      "f1 : 0.9956521739130435\n",
      "[[ 70   6]\n",
      " [  2 916]]\n",
      "------------------------------\n",
      "threshold : 0.14\n",
      "acc : 0.9919517102615694\n",
      "recall : 0.9978213507625272\n",
      "precision : 0.9934924078091106\n",
      "f1 : 0.9956521739130435\n",
      "[[ 70   6]\n",
      " [  2 916]]\n",
      "------------------------------\n",
      "threshold : 0.15\n",
      "acc : 0.9919517102615694\n",
      "recall : 0.9978213507625272\n",
      "precision : 0.9934924078091106\n",
      "f1 : 0.9956521739130435\n",
      "[[ 70   6]\n",
      " [  2 916]]\n",
      "------------------------------\n",
      "threshold : 0.16\n",
      "acc : 0.9919517102615694\n",
      "recall : 0.9978213507625272\n",
      "precision : 0.9934924078091106\n",
      "f1 : 0.9956521739130435\n",
      "[[ 70   6]\n",
      " [  2 916]]\n",
      "------------------------------\n",
      "threshold : 0.17\n",
      "acc : 0.9929577464788732\n",
      "recall : 0.9989106753812637\n",
      "precision : 0.9934994582881906\n",
      "f1 : 0.9961977186311788\n",
      "[[ 70   6]\n",
      " [  1 917]]\n",
      "------------------------------\n",
      "threshold : 0.18\n",
      "acc : 0.9919517102615694\n",
      "recall : 0.9989106753812637\n",
      "precision : 0.9924242424242424\n",
      "f1 : 0.9956568946796961\n",
      "[[ 69   7]\n",
      " [  1 917]]\n",
      "------------------------------\n",
      "threshold : 0.19\n",
      "acc : 0.9919517102615694\n",
      "recall : 0.9989106753812637\n",
      "precision : 0.9924242424242424\n",
      "f1 : 0.9956568946796961\n",
      "[[ 69   7]\n",
      " [  1 917]]\n",
      "\n",
      "******************************\n",
      "best threshold : 0.17\n",
      "best f1_score : 0.9961977186311788\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "f1_best = 0.0\n",
    "threshold_best = 0.0\n",
    "for threshold in range(0, 20):\n",
    "    f1 = evaluation(y_valid_pred, threshold/100)\n",
    "    if f1 > f1_best:\n",
    "        f1_best = f1\n",
    "        threshold_best = threshold/100\n",
    "print(\"\\n\"+\"*\"*30)\n",
    "print(\"best threshold : \" + str(threshold_best))\n",
    "print(\"best f1_score : \" + str(f1_best))\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.feature_importance(importance_type='gain')\n",
    "importance = pd.DataFrame(importance, index=col_name, columns=['importance']).sort_values('importance', ascending=False)\n",
    "importance = importance.iloc[0:100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x2160 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 1677,
       "width": 701
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 30))\n",
    "sns.barplot(x = importance['importance'], y = importance.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.feature_importance(importance_type='split')\n",
    "importance = pd.DataFrame(importance, index=col_name, columns=['importance']).sort_values('importance', ascending=False)\n",
    "importance = importance.iloc[0:100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x2160 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 1677,
       "width": 711
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 30))\n",
    "sns.barplot(x = importance['importance'], y = importance.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.46955770e-04, 3.00420902e-04, 1.23561402e-04, ...,\n",
       "       7.17622397e-04, 6.27824298e-05, 4.13138257e-04])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1420"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 383
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_test, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 376
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_test, bins=100, range=(0, 0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold_best : 0.17\n",
      "akutagawa : 100\n",
      "not akutagawa : 1320\n"
     ]
    }
   ],
   "source": [
    "print(\"threshold_best : \" + str(threshold_best))\n",
    "print(\"akutagawa : \" + str(sum((y_test > threshold_best)*1)))\n",
    "print(\"not akutagawa : \" + str(sum((y_test <= threshold_best)*1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writing_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4626</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4629</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4640</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4650</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4653</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4661</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4663</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4670</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4679</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4680</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4686</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4688</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4694</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4706</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4716</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4719</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4725</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4726</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4727</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            author\n",
       "writing_id        \n",
       "4                0\n",
       "5                0\n",
       "6                1\n",
       "10               1\n",
       "11               1\n",
       "13               1\n",
       "18               0\n",
       "21               0\n",
       "26               1\n",
       "29               0\n",
       "36               0\n",
       "37               1\n",
       "38               0\n",
       "40               1\n",
       "42               1\n",
       "44               0\n",
       "48               0\n",
       "49               1\n",
       "52               0\n",
       "57               1\n",
       "58               0\n",
       "63               0\n",
       "65               0\n",
       "67               1\n",
       "69               1\n",
       "72               1\n",
       "78               0\n",
       "79               1\n",
       "82               1\n",
       "84               1\n",
       "...            ...\n",
       "4626             1\n",
       "4629             0\n",
       "4630             1\n",
       "4631             0\n",
       "4634             0\n",
       "4640             0\n",
       "4646             0\n",
       "4650             0\n",
       "4653             0\n",
       "4661             0\n",
       "4663             0\n",
       "4667             1\n",
       "4670             1\n",
       "4673             1\n",
       "4678             1\n",
       "4679             0\n",
       "4680             1\n",
       "4682             0\n",
       "4683             0\n",
       "4686             0\n",
       "4688             0\n",
       "4694             1\n",
       "4704             1\n",
       "4706             1\n",
       "4714             1\n",
       "4716             0\n",
       "4719             0\n",
       "4725             0\n",
       "4726             1\n",
       "4727             0\n",
       "\n",
       "[1420 rows x 1 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_sample = pd.read_csv(\"../input/sample_submission.csv\", index_col=0)\n",
    "submit_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writing_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4626</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4629</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4640</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4650</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4653</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4661</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4663</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4670</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4679</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4680</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4686</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4688</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4694</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4706</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4716</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4719</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4725</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4726</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4727</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            author\n",
       "writing_id        \n",
       "4                0\n",
       "5                0\n",
       "6                0\n",
       "10               0\n",
       "11               0\n",
       "13               0\n",
       "18               0\n",
       "21               0\n",
       "26               0\n",
       "29               0\n",
       "36               0\n",
       "37               0\n",
       "38               0\n",
       "40               0\n",
       "42               0\n",
       "44               0\n",
       "48               0\n",
       "49               0\n",
       "52               0\n",
       "57               1\n",
       "58               0\n",
       "63               0\n",
       "65               0\n",
       "67               0\n",
       "69               0\n",
       "72               0\n",
       "78               0\n",
       "79               0\n",
       "82               0\n",
       "84               0\n",
       "...            ...\n",
       "4626             0\n",
       "4629             0\n",
       "4630             1\n",
       "4631             0\n",
       "4634             0\n",
       "4640             0\n",
       "4646             0\n",
       "4650             0\n",
       "4653             0\n",
       "4661             0\n",
       "4663             1\n",
       "4667             0\n",
       "4670             0\n",
       "4673             0\n",
       "4678             0\n",
       "4679             0\n",
       "4680             1\n",
       "4682             0\n",
       "4683             0\n",
       "4686             0\n",
       "4688             0\n",
       "4694             0\n",
       "4704             0\n",
       "4706             0\n",
       "4714             0\n",
       "4716             0\n",
       "4719             0\n",
       "4725             0\n",
       "4726             0\n",
       "4727             0\n",
       "\n",
       "[1420 rows x 1 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_sample[\"author\"] = (y_test > threshold_best)*1\n",
    "submit_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9295774647887324"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_sample[\"author\"].value_counts()[0]/(submit_sample[\"author\"].value_counts()[0]+submit_sample[\"author\"].value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9240724762726489"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[\"target\"].value_counts()[0]/(y_train[\"target\"].value_counts()[0]+y_train[\"target\"].value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_sample.to_csv(\"../input/submission_0112_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
